<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>18 Performance Testing</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <style>
        :root {
            --primary: #6366f1;
            --primary-dark: #4f46e5;
            --secondary: #8b5cf6;
            --bg-main: #0f172a;
            --bg-card: #1e293b;
            --bg-code: #0f172a;
            --text-primary: #f1f5f9;
            --text-secondary: #94a3b8;
            --border: #334155;
            --accent: #06b6d4;
            --success: #10b981;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body { 
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.7; 
            color: var(--text-primary); 
            background: var(--bg-main);
        }
        
        .container { 
            max-width: 1200px; 
            margin: 0 auto; 
            padding: 20px; 
        }
        
        .home-btn { 
            position: fixed; 
            top: 20px; 
            right: 20px; 
            background: var(--accent);
            color: white; 
            width: 50px;
            height: 50px;
            border-radius: 50%;
            text-decoration: none; 
            z-index: 1000;
            box-shadow: 0 4px 20px rgba(6, 182, 212, 0.4);
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            font-size: 24px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .home-btn:hover { 
            background: #0891b2;
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(6, 182, 212, 0.5);
        }
        
        .breadcrumb { 
            background: var(--bg-card);
            padding: 12px 0; 
            margin-bottom: 24px; 
            font-size: 14px;
            border-radius: 8px;
            border: 1px solid var(--border);
        }
        
        .breadcrumb a { 
            color: var(--accent);
            text-decoration: none;
            transition: color 0.2s;
        }
        
        .breadcrumb a:hover { 
            color: var(--primary);
            text-decoration: underline;
        }
        
        .content { 
            background: var(--bg-card);
            padding: 3rem; 
            border-radius: 16px; 
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
            border: 1px solid var(--border);
        }
        
        .file-list { 
            display: grid; 
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr)); 
            gap: 1.5rem; 
            margin: 2rem 0; 
        }
        
        .file-item { 
            padding: 1.5rem; 
            border: 1px solid var(--border);
            border-radius: 12px; 
            background: var(--bg-main);
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            overflow: hidden;
        }
        
        .file-item::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 3px;
            background: linear-gradient(90deg, var(--primary), var(--secondary));
            opacity: 0;
            transition: opacity 0.3s;
        }
        
        .file-item:hover { 
            transform: translateY(-4px); 
            box-shadow: 0 8px 30px rgba(99, 102, 241, 0.3);
            border-color: var(--primary);
        }
        
        .file-item:hover::before {
            opacity: 1;
        }
        
        .file-item a { 
            color: var(--text-primary);
            text-decoration: none; 
            font-weight: 600; 
            display: block;
            font-size: 1.1rem;
        }
        
        .file-item a:hover { 
            color: var(--primary);
        }
        
        .file-type { 
            font-size: 13px; 
            color: var(--text-secondary);
            margin-top: 8px; 
            font-weight: 500;
        }
        
        /* Code Blocks */
        pre { 
            background: var(--bg-code);
            padding: 1.5rem; 
            border-radius: 12px; 
            overflow-x: auto;
            border: 1px solid var(--border);
            margin: 1.5rem 0;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3);
            position: relative;
        }
        
        pre code { 
            background: none;
            padding: 0;
            font-family: 'Fira Code', 'Consolas', monospace;
            font-size: 0.9em;
            line-height: 1.6;
        }
        
        /* Copy Button */
        .copy-btn {
            position: absolute;
            top: 8px;
            right: 8px;
            background: var(--primary);
            color: white;
            border: none;
            padding: 6px 12px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 12px;
            font-weight: 600;
            transition: all 0.2s;
            opacity: 0.7;
            z-index: 10;
        }
        
        .copy-btn:hover {
            opacity: 1;
            background: var(--primary-dark);
            transform: translateY(-1px);
        }
        
        .copy-btn.copied {
            background: var(--success);
        }
        
        pre:hover .copy-btn {
            opacity: 1;
        }
        
        /* Inline Code */
        code { 
            background: var(--bg-code);
            color: #8b5cf6;
            padding: 3px 8px; 
            border-radius: 6px; 
            font-size: 1.1em;
            font-family: 'Fira Code', 'Consolas', monospace;
            border: 1px solid var(--border);
        }
        
        /* Headings */
        h1, h2, h3, h4, h5, h6 { 
            color: var(--text-primary);
            margin: 2rem 0 1rem 0;
            font-weight: 700;
            letter-spacing: -0.5px;
        }
        
        h1 { 
            font-size: 2.5rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            border-bottom: 3px solid var(--primary);
            padding-bottom: 12px;
            margin-bottom: 1.5rem;
        }
        
        h2 { 
            font-size: 2rem;
            color: var(--primary);
            border-bottom: 2px solid var(--border);
            padding-bottom: 8px;
        }
        
        h3 { 
            font-size: 1.5rem;
            color: var(--accent);
        }
        
        /* Links */
        a { 
            color: var(--accent);
            transition: color 0.2s;
        }
        
        a:hover { 
            color: var(--primary);
        }
        
        /* Paragraphs */
        p {
            margin: 1rem 0;
            color: var(--text-secondary);
        }
        
        /* Lists */
        ul, ol {
            margin: 1rem 0;
            padding-left: 2rem;
            color: var(--text-secondary);
        }
        
        li {
            margin: 0.5rem 0;
        }
        
        /* Tables */
        table { 
            border-collapse: collapse;
            width: 100%;
            margin: 1.5rem 0;
            background: var(--bg-main);
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
        }
        
        th, td { 
            border: 1px solid var(--border);
            padding: 12px 16px;
            text-align: left;
        }
        
        th { 
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            font-weight: 600;
        }
        
        tr:hover {
            background: var(--bg-card);
        }
        
        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--primary);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            background: var(--bg-main);
            border-radius: 0 8px 8px 0;
            color: var(--text-secondary);
            font-style: italic;
        }
        
        /* Horizontal Rule */
        hr {
            border: none;
            border-top: 2px solid var(--border);
            margin: 2rem 0;
        }
        
        .footer { 
            text-align: center; 
            padding: 2rem; 
            color: var(--text-secondary);
            border-top: 1px solid var(--border);
            margin-top: 3rem; 
            font-size: 14px;
        }
        
        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 12px;
            height: 12px;
        }
        
        ::-webkit-scrollbar-track {
            background: var(--bg-main);
        }
        
        ::-webkit-scrollbar-thumb {
            background: var(--border);
            border-radius: 6px;
        }
        
        ::-webkit-scrollbar-thumb:hover {
            background: var(--primary);
        }
        
        @media (max-width: 768px) {
            .container { padding: 10px; }
            .file-list { grid-template-columns: 1fr; }
            .content { padding: 1.5rem; }
            h1 { font-size: 2rem; }
            h2 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>
    <a href="../index.html" class="home-btn">üè†</a>
    <div class="container">
        <div class="breadcrumb">
            <div style="padding: 0 20px;">
                <a href="../index.html">üè† Home</a> <span style="color: #64748b;">/</span> <a href="index.html">06 Mastery and Beyond</a>
            </div>
        </div>
        <div class="content">
            <h1 id="chapter-18-performance-testing">Chapter 18: Performance Testing</h1>
<h2 id="when-performance-testing-matters">When Performance Testing Matters</h2>
<h2 id="performance-the-invisible-feature">Performance: The Invisible Feature</h2>
<p>Performance is a critical feature of any application. A function that returns the correct result but takes ten seconds to do so might be functionally correct but practically useless. Similarly, a test suite that takes hours to run becomes a bottleneck for development, slowing down feedback loops and discouraging frequent testing.</p>
<p>In the context of pytest, performance testing splits into two distinct domains:</p>
<ol>
<li>
<p><strong>Application Performance Testing</strong>: Measuring the speed and resource consumption (CPU, memory) of your actual application code. The goal is to identify bottlenecks, validate optimizations, and prevent performance regressions. For example, "Does this database query run in under 50ms?" or "Does this data processing function leak memory?"</p>
</li>
<li>
<p><strong>Test Suite Performance Testing</strong>: Measuring the execution time of your tests themselves. The goal here is to keep your CI/CD pipeline fast and responsive. A slow test suite is a drag on productivity. The focus is on identifying and optimizing slow tests, not necessarily the application code they are testing.</p>
</li>
</ol>
<p>This chapter will equip you with the tools and techniques to tackle both domains. We'll start by benchmarking application code to ensure it's fast and efficient, and then we'll turn our attention to keeping the test suite itself lean and quick.</p>
<p>While pytest is not a dedicated load-testing framework like Locust or JMeter (which are designed to simulate thousands of concurrent users), it is an exceptional tool for <strong>micro-benchmarking</strong>‚Äîprecisely measuring the performance of individual functions or components under controlled conditions. This is invaluable for catching performance regressions before they ever reach production.</p>
<h2 id="measuring-test-execution-time">Measuring Test Execution Time</h2>
<h2 id="the-simplest-tool-durations">The Simplest Tool: <code>--durations</code></h2>
<p>Before diving into specialized tools, let's look at a built-in pytest feature that provides a coarse-grained view of test performance: the <code>--durations</code> option.</p>
<p>This option reports the <code>N</code> slowest test items. It's a fantastic starting point for identifying major bottlenecks in your test suite.</p>
<h3 id="phase-1-establish-the-reference-implementation">Phase 1: Establish the Reference Implementation</h3>
<p>Let's create a simple function we want to analyze. We'll write two versions: a naive, slow implementation and a more optimized one. Our task is to find common elements between two lists.</p>
<p>This will be our <strong>anchor example</strong> for the chapter. We will use it to demonstrate the limitations of simple timing and the power of statistical benchmarking.</p>
<p>First, let's create our utility module.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># utils/list_operations.py</span>

<span class="k">def</span><span class="w"> </span><span class="nf">find_common_elements_naive</span><span class="p">(</span><span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds common elements using nested loops.</span>
<span class="sd">    This has a time complexity of O(n*m).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">common</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">item_a</span> <span class="ow">in</span> <span class="n">list_a</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">item_b</span> <span class="ow">in</span> <span class="n">list_b</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">item_a</span> <span class="o">==</span> <span class="n">item_b</span><span class="p">:</span>
                <span class="n">common</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item_a</span><span class="p">)</span>
                <span class="k">break</span>  <span class="c1"># Move to next item in list_a once a match is found</span>
    <span class="k">return</span> <span class="n">common</span>

<span class="k">def</span><span class="w"> </span><span class="nf">find_common_elements_optimized</span><span class="p">(</span><span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Finds common elements using set intersection.</span>
<span class="sd">    This has a time complexity of O(n+m).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">set_a</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">list_a</span><span class="p">)</span>
    <span class="n">set_b</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">list_b</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">set_a</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">set_b</span><span class="p">))</span>
</code></pre></div>

<p>Now, let's write a simple test for the naive version. We'll add a <code>time.sleep()</code> to simulate a slow operation and ensure it shows up in our report.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_performance.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils.list_operations</span><span class="w"> </span><span class="kn">import</span> <span class="n">find_common_elements_naive</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">sample_lists</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Provides two lists for testing.&quot;&quot;&quot;</span>
    <span class="n">list_a</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
    <span class="n">list_b</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">150</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_find_common_elements_correctness</span><span class="p">(</span><span class="n">sample_lists</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A standard functional test to ensure correctness.&quot;&quot;&quot;</span>
    <span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span> <span class="o">=</span> <span class="n">sample_lists</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">find_common_elements_naive</span><span class="p">(</span><span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span><span class="p">)</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_slow_operation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A test that is intentionally slow.&quot;&quot;&quot;</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_fast_operation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A test that is very fast.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">2</span>
</code></pre></div>

<h3 id="iteration-1-identifying-slow-tests">Iteration 1: Identifying Slow Tests</h3>
<p>Our goal is to find the slowest parts of our test suite. We suspect <code>test_slow_operation</code> is a problem, but in a large suite, we wouldn't know where to look.</p>
<p>Let's run pytest with <code>--durations=3</code> to see the top 3 slowest items. The duration includes the test function execution time plus its setup and teardown phases.</p>
<div class="codehilite"><pre><span></span><code>pytest<span class="w"> </span>--durations<span class="o">=</span><span class="m">3</span>
</code></pre></div>

<h3 id="diagnostic-analysis-reading-the-output">Diagnostic Analysis: Reading the Output</h3>
<p><strong>The complete output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
...
collected<span class="w"> </span><span class="m">3</span><span class="w"> </span>items

tests/test_performance.py<span class="w"> </span>...<span class="w">                                            </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">===========================</span><span class="w"> </span>slowest<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="nv">durations</span><span class="w"> </span><span class="o">===========================</span>
<span class="m">0</span>.50s<span class="w"> </span>call<span class="w">     </span>tests/test_performance.py::test_slow_operation
<span class="m">0</span>.00s<span class="w"> </span>call<span class="w">     </span>tests/test_performance.py::test_find_common_elements_correctness
<span class="m">0</span>.00s<span class="w"> </span>setup<span class="w">    </span>tests/test_performance.py::test_find_common_elements_correctness
<span class="m">0</span>.00s<span class="w"> </span>call<span class="w">     </span>tests/test_performance.py::test_fast_operation
<span class="m">0</span>.00s<span class="w"> </span>teardown<span class="w"> </span>tests/test_performance.py::test_find_common_elements_correctness
<span class="m">0</span>.00s<span class="w"> </span>setup<span class="w">    </span>tests/test_performance.py::test_slow_operation
...
<span class="o">==============================</span><span class="w"> </span><span class="m">3</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.51s<span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p><strong>Let's parse this section by section</strong>:</p>
<ol>
<li>
<p><strong>The summary table</strong>: <code>slowest 3 test durations</code></p>
<ul>
<li>What this tells us: It lists the slowest items, breaking them down into <code>setup</code>, <code>call</code> (the test body itself), and <code>teardown</code>. This is crucial for diagnosing whether the slowness is in the test logic or the fixture setup.</li>
</ul>
</li>
<li>
<p><strong>The top entry</strong>: <code>0.50s call tests/test_performance.py::test_slow_operation</code></p>
<ul>
<li>What this tells us: Unsurprisingly, the <code>call</code> phase of <code>test_slow_operation</code> took about half a second, which directly corresponds to our <code>time.sleep(0.5)</code>.</li>
</ul>
</li>
<li>
<p><strong>The other entries</strong>: The other tests and setup phases are extremely fast, registering as <code>0.00s</code>.</p>
</li>
</ol>
<p><strong>Root cause identified</strong>: The <code>test_slow_operation</code> is, by far, the slowest test.
<strong>Why the current approach is limited</strong>: While <code>--durations</code> is excellent for finding slow <em>tests</em>, it's a poor tool for benchmarking <em>code</em>. The measurements are noisy, include pytest overhead, and are based on a single run. If we ran this command again, the exact millisecond values would fluctuate. This makes it unreliable for comparing two fast functions.
<strong>What we need</strong>: A tool that can run a piece of code many times in isolation, perform statistical analysis, and give us a reliable, repeatable measurement. This is called micro-benchmarking.</p>
<h2 id="pytest-benchmark-for-reliable-benchmarks">pytest-benchmark for Reliable Benchmarks</h2>
<p>To get reliable performance measurements, we need a specialized tool. The most popular and powerful option in the pytest ecosystem is <code>pytest-benchmark</code>.</p>
<p>First, install it:</p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>pytest-benchmark
</code></pre></div>

<p><code>pytest-benchmark</code> provides a <code>benchmark</code> fixture that handles the complexity of running your code multiple times, measuring execution time accurately, and calculating statistics like mean, median, standard deviation, and more.</p>
<h3 id="iteration-2-introducing-pytest-benchmark">Iteration 2: Introducing <code>pytest-benchmark</code></h3>
<p>Let's try to benchmark our <code>find_common_elements_naive</code> function. Using <code>--durations</code> was ineffective because the function is too fast to be measured reliably in a single run. <code>pytest-benchmark</code> solves this.</p>
<p>Here's how you use the <code>benchmark</code> fixture: you call it like a function, passing the callable you want to benchmark as the first argument, followed by its arguments.</p>
<p><strong>Before: The old functional test</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_performance.py (excerpt)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_find_common_elements_correctness</span><span class="p">(</span><span class="n">sample_lists</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A standard functional test to ensure correctness.&quot;&quot;&quot;</span>
    <span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span> <span class="o">=</span> <span class="n">sample_lists</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">find_common_elements_naive</span><span class="p">(</span><span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span><span class="p">)</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span>
</code></pre></div>

<p><strong>After: The new benchmark test</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_performance.py (add this test)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils.list_operations</span><span class="w"> </span><span class="kn">import</span> <span class="n">find_common_elements_naive</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_find_common_elements_naive_performance</span><span class="p">(</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">sample_lists</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmarks the naive implementation.&quot;&quot;&quot;</span>
    <span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span> <span class="o">=</span> <span class="n">sample_lists</span>
    <span class="c1"># The benchmark fixture takes the function to run, and its args/kwargs</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">find_common_elements_naive</span><span class="p">,</span> <span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span><span class="p">)</span>

    <span class="c1"># You can still add assertions to ensure correctness!</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span>
</code></pre></div>

<p>Now, let's run pytest. <code>pytest-benchmark</code> is automatically active when installed.</p>
<div class="codehilite"><pre><span></span><code>pytest<span class="w"> </span>tests/test_performance.py::test_find_common_elements_naive_performance
</code></pre></div>

<p><strong>The complete output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
...
plugins:<span class="w"> </span>benchmark-4.0.0,<span class="w"> </span>...
...
collected<span class="w"> </span><span class="m">1</span><span class="w"> </span>item

tests/test_performance.py::test_find_common_elements_naive_performance<span class="w"> </span>
--------------------------------<span class="w"> </span>benchmark:<span class="w"> </span><span class="m">1</span><span class="w"> </span>tests<span class="w"> </span>--------------------------------
Name<span class="w"> </span><span class="o">(</span><span class="nb">time</span><span class="w"> </span><span class="k">in</span><span class="w"> </span>ms<span class="o">)</span><span class="w">                                        </span>Min<span class="w">      </span>Max<span class="w">     </span>Mean<span class="w">   </span>StdDev<span class="w">  </span>Median<span class="w">     </span>IQR<span class="w">  </span>Outliers<span class="w">     </span>OPS<span class="w">  </span>Rounds<span class="w">  </span>Iterations
--------------------------------------------------------------------------------------------------------------------------------------------
test_find_common_elements_naive_performance<span class="w">         </span><span class="m">1</span>.0315<span class="w">   </span><span class="m">1</span>.5329<span class="w">   </span><span class="m">1</span>.0861<span class="w">   </span><span class="m">0</span>.0981<span class="w">  </span><span class="m">1</span>.0524<span class="w">   </span><span class="m">0</span>.0497<span class="w">     </span><span class="m">15</span><span class="p">;</span><span class="m">10</span><span class="w">   </span><span class="m">920</span>.758<span class="w">     </span><span class="m">172</span><span class="w">           </span><span class="m">1</span>
--------------------------------------------------------------------------------------------------------------------------------------------
PASSED<span class="w">                                                                   </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">==============================</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">2</span>.05s<span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<h3 id="diagnostic-analysis-reading-the-benchmark-report">Diagnostic Analysis: Reading the Benchmark Report</h3>
<p>This table is packed with statistical data, which is why it's so much more reliable than a single timing run.</p>
<ol>
<li><strong>Name</strong>: The name of the test function.</li>
<li><strong>Min, Max, Mean, Median</strong>: These are the core statistics of the execution time over many runs (called "rounds"). The <code>Mean</code> is the average time, but <code>Median</code> is often more useful as it's less sensitive to outliers.</li>
<li><strong>StdDev</strong>: The standard deviation, which measures how much the timings varied. A low <code>StdDev</code> indicates a stable, reliable measurement.</li>
<li><strong>OPS (Operations Per Second)</strong>: This is a very useful metric (<code>1 / Mean</code>), telling you how many times the function can run in one second.</li>
<li><strong>Rounds</strong>: The number of times the benchmark measurement was taken.</li>
<li><strong>Iterations</strong>: The number of times the code was run within each round. <code>pytest-benchmark</code> automatically adjusts this number to get a reasonable total execution time.</li>
</ol>
<p><strong>Root cause identified</strong>: We now have a statistically sound measurement of our function's performance.
<strong>What we need</strong>: A way to compare this naive implementation against our optimized one.</p>
<h3 id="iteration-3-comparing-implementations-with-parametrization">Iteration 3: Comparing Implementations with Parametrization</h3>
<p><code>pytest-benchmark</code> shines when comparing different approaches. We can use <code>pytest.mark.parametrize</code> to feed both our naive and optimized functions into the same benchmark test. This ensures they are benchmarked under identical conditions.</p>
<p><strong>Before: A single benchmark test</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_performance.py (excerpt)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils.list_operations</span><span class="w"> </span><span class="kn">import</span> <span class="n">find_common_elements_naive</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_find_common_elements_naive_performance</span><span class="p">(</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">sample_lists</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmarks the naive implementation.&quot;&quot;&quot;</span>
    <span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span> <span class="o">=</span> <span class="n">sample_lists</span>
    <span class="n">benchmark</span><span class="p">(</span><span class="n">find_common_elements_naive</span><span class="p">,</span> <span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span><span class="p">)</span>
</code></pre></div>

<p><strong>After: A parametrized comparison test</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_performance.py (replace previous test with this)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils.list_operations</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">find_common_elements_naive</span><span class="p">,</span>
    <span class="n">find_common_elements_optimized</span>
<span class="p">)</span>

<span class="c1"># A list of functions to test</span>
<span class="n">ALGORITHMS</span> <span class="o">=</span> <span class="p">[</span><span class="n">find_common_elements_naive</span><span class="p">,</span> <span class="n">find_common_elements_optimized</span><span class="p">]</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s2">&quot;algorithm&quot;</span><span class="p">,</span> <span class="n">ALGORITHMS</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_find_common_elements_performance</span><span class="p">(</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">sample_lists</span><span class="p">,</span> <span class="n">algorithm</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Benchmarks and compares different implementations.&quot;&quot;&quot;</span>
    <span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span> <span class="o">=</span> <span class="n">sample_lists</span>

    <span class="c1"># The benchmark fixture gets the parametrized algorithm</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">list_a</span><span class="p">,</span> <span class="n">list_b</span><span class="p">)</span>

    <span class="c1"># We can still assert correctness</span>
    <span class="n">expected</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span>
</code></pre></div>

<p>Let's run this parametrized test.</p>
<div class="codehilite"><pre><span></span><code>pytest<span class="w"> </span>tests/test_performance.py::test_find_common_elements_performance
</code></pre></div>

<p>The output now includes a grouped and sorted comparison table, making the performance difference immediately obvious.</p>
<p><strong>The complete output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
...
collected<span class="w"> </span><span class="m">2</span><span class="w"> </span>items

tests/test_performance.py<span class="w"> </span>..<span class="w">                                             </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
--------------------------------<span class="w"> </span>benchmark:<span class="w"> </span><span class="m">2</span><span class="w"> </span>tests<span class="w"> </span>--------------------------------
Name<span class="w"> </span><span class="o">(</span><span class="nb">time</span><span class="w"> </span><span class="k">in</span><span class="w"> </span>us<span class="o">)</span><span class="w">                                           </span>Min<span class="w">      </span>Max<span class="w">     </span>Mean<span class="w">   </span>StdDev<span class="w">   </span>Median<span class="w">      </span>IQR<span class="w">  </span>Outliers<span class="w">      </span>OPS<span class="w">  </span>Rounds<span class="w">  </span>Iterations
-------------------------------------------------------------------------------------------------------------------------------------------------
test_find_common_elements_performance<span class="o">[</span>optimized<span class="o">]</span><span class="w">         </span><span class="m">6</span>.8460<span class="w">   </span><span class="m">9</span>.9410<span class="w">   </span><span class="m">7</span>.1359<span class="w">   </span><span class="m">0</span>.4904<span class="w">   </span><span class="m">7</span>.0090<span class="w">   </span><span class="m">0</span>.2295<span class="w">   </span><span class="m">453</span><span class="p">;</span><span class="m">136</span><span class="w">  </span><span class="m">140</span>,139.7525<span class="w">    </span><span class="m">1000</span><span class="w">           </span><span class="m">1</span>
test_find_common_elements_performance<span class="o">[</span>naive<span class="o">]</span><span class="w">         </span><span class="m">1</span>,029.1000<span class="w"> </span><span class="m">1</span>,170.2000<span class="w"> </span><span class="m">1</span>,048.9839<span class="w"> </span><span class="m">25</span>.1278<span class="w"> </span><span class="m">1</span>,041.8000<span class="w">  </span><span class="m">20</span>.4250<span class="w">     </span><span class="m">23</span><span class="p">;</span><span class="m">5</span><span class="w">    </span><span class="m">953</span>.3031<span class="w">     </span><span class="m">176</span><span class="w">           </span><span class="m">1</span>
-------------------------------------------------------------------------------------------------------------------------------------------------

<span class="o">==============================</span><span class="w"> </span><span class="m">2</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">3</span>.12s<span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p>The table is automatically sorted from fastest to slowest. The results are stark: the optimized version runs in about 7 <strong>microseconds</strong> (<code>us</code>), while the naive version takes over 1000 microseconds (1 <strong>millisecond</strong>). The optimized version is over 100 times faster! This is the kind of insight that simple timing with <code>--durations</code> could never provide.</p>
<h2 id="memory-profiling-in-tests">Memory Profiling in Tests</h2>
<h2 id="memory-the-other-side-of-performance">Memory: The Other Side of Performance</h2>
<p>Performance isn't just about speed; it's also about memory consumption. A function that's lightning-fast but consumes gigabytes of RAM can be just as problematic as a slow one. To profile memory usage, we can use the <code>pytest-memray</code> plugin, which integrates the powerful <code>memray</code> memory profiler into pytest.</p>
<p>First, install the necessary packages:</p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>pytest-memray<span class="w"> </span>memray
</code></pre></div>

<h3 id="phase-1-establish-the-reference-implementation_1">Phase 1: Establish the Reference Implementation</h3>
<p>Let's create a function that is deliberately memory-intensive. It will build a large data structure in memory.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># utils/memory_operations.py</span>

<span class="k">def</span><span class="w"> </span><span class="nf">create_large_object</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a list containing many dictionaries.</span>
<span class="sd">    This is designed to consume a significant amount of memory.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Each dictionary is a small object, but we create millions of them.</span>
    <span class="k">return</span> <span class="p">[{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="s2">&quot;x&quot;</span> <span class="o">*</span> <span class="mi">10</span><span class="p">}</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">)]</span>
</code></pre></div>

<p>And a simple test for it:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_memory.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils.memory_operations</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_large_object</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_create_large_object</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests the memory-intensive function.&quot;&quot;&quot;</span>
    <span class="c1"># Create 1 million objects</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">create_large_object</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1_000_000</span>
    <span class="k">assert</span> <span class="n">data</span><span class="p">[</span><span class="mi">999</span><span class="p">][</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">999</span>
</code></pre></div>

<p>This test passes, but it tells us nothing about how much memory was used.</p>
<h3 id="iteration-1-gaining-visibility-with-memray">Iteration 1: Gaining Visibility with <code>--memray</code></h3>
<p>The "failure" here is a lack of visibility. We have no idea if our function is efficient or a memory hog. Let's run pytest with the <code>--memray</code> flag to enable memory profiling.</p>
<div class="codehilite"><pre><span></span><code>pytest<span class="w"> </span>--memray<span class="w"> </span>tests/test_memory.py
</code></pre></div>

<p>This command runs the tests as usual, but <code>memray</code> tracks every memory allocation in the background. After the test run, it generates a report.</p>
<p><strong>The complete output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
...
plugins:<span class="w"> </span>memray-1.5.0,<span class="w"> </span>...
...
collected<span class="w"> </span><span class="m">1</span><span class="w"> </span>item

tests/test_memory.py<span class="w"> </span>.<span class="w">                                                   </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
<span class="o">===============================</span><span class="w"> </span>slowest<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="nv">durations</span><span class="w"> </span><span class="o">===============================</span>
<span class="m">0</span>.34s<span class="w"> </span>call<span class="w">     </span>tests/test_memory.py::test_create_large_object
<span class="o">(</span><span class="m">1</span><span class="w"> </span>durations<span class="w"> </span>hidden<span class="o">)</span>
<span class="o">==============================</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.48s<span class="w"> </span><span class="o">===============================</span>
Wrote<span class="w"> </span>memray<span class="w"> </span>report<span class="w"> </span>to:<span class="w"> </span>memray-tests.test_memory.py::test_create_large_object.bin
You<span class="w"> </span>can<span class="w"> </span>now<span class="w"> </span>generate<span class="w"> </span>a<span class="w"> </span>report<span class="w"> </span>from<span class="w"> </span>the<span class="w"> </span>stored<span class="w"> </span>allocation<span class="w"> </span>records.
Some<span class="w"> </span>available<span class="w"> </span>reporters<span class="w"> </span>are:
-<span class="w"> </span>memray<span class="w"> </span>flamegraph<span class="w"> </span>memray-tests.test_memory.py::test_create_large_object.bin
-<span class="w"> </span>memray<span class="w"> </span>table<span class="w"> </span>memray-tests.test_memory.py::test_create_large_object.bin
-<span class="w"> </span>memray<span class="w"> </span>summary<span class="w"> </span>memray-tests.test_memory.py::test_create_large_object.bin
</code></pre></div>

<p>The key line is <code>Wrote memray report to: ...</code>. <code>memray</code> has saved a detailed snapshot of all memory allocations to a <code>.bin</code> file. We can now analyze this file. A flame graph is often the most intuitive visualization.</p>
<div class="codehilite"><pre><span></span><code>memray<span class="w"> </span>flamegraph<span class="w"> </span>memray-tests.test_memory.py::test_create_large_object.bin
</code></pre></div>

<p>This command will generate an HTML file and open it in your browser. The flame graph is a powerful visualization where:
-   The width of a bar represents the percentage of memory allocated by that function and its children.
-   The y-axis represents the call stack.</p>
<p>You would see a large bar corresponding to the list comprehension inside <code>create_large_object</code>, confirming that it is the source of the vast majority of memory allocations.</p>
<h3 id="iteration-2-enforcing-memory-limits">Iteration 2: Enforcing Memory Limits</h3>
<p>Visibility is good, but automated prevention of regressions is better. <code>pytest-memray</code> allows you to set memory limits for your tests. If a test exceeds this limit, it fails. This is perfect for CI.</p>
<p>We can use the <code>@pytest.mark.limit_memory</code> marker. Let's set a limit of 100 MiB and see our test fail.</p>
<p><strong>Before: No memory limit</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_memory.py (excerpt)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils.memory_operations</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_large_object</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_create_large_object</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">create_large_object</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1_000_000</span>
</code></pre></div>

<p><strong>After: With a memory limit marker</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_memory.py (modified)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils.memory_operations</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_large_object</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">limit_memory</span><span class="p">(</span><span class="s2">&quot;100 MB&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_create_large_object_with_limit</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests the memory-intensive function with a limit.&quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">create_large_object</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1_000_000</span>
    <span class="k">assert</span> <span class="n">data</span><span class="p">[</span><span class="mi">999</span><span class="p">][</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">999</span>
</code></pre></div>

<p>Now, run the test again. It doesn't need the <code>--memray</code> flag; the marker is sufficient.</p>
<div class="codehilite"><pre><span></span><code>pytest<span class="w"> </span>tests/test_memory.py
</code></pre></div>

<h3 id="diagnostic-analysis-reading-the-failure">Diagnostic Analysis: Reading the Failure</h3>
<p><strong>The complete output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
...
collected<span class="w"> </span><span class="m">1</span><span class="w"> </span>item

tests/test_memory.py<span class="w"> </span>F<span class="w">                                                   </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">===================================</span><span class="w"> </span><span class="nv">FAILURES</span><span class="w"> </span><span class="o">===================================</span>
___________________<span class="w"> </span>test_create_large_object_with_limit<span class="w"> </span>____________________

<span class="w">    </span>@pytest.mark.limit_memory<span class="o">(</span><span class="s2">&quot;100 MB&quot;</span><span class="o">)</span>
<span class="w">    </span>def<span class="w"> </span>test_create_large_object_with_limit<span class="o">()</span>:
<span class="w">        </span><span class="s2">&quot;&quot;&quot;Tests the memory-intensive function with a limit.&quot;&quot;&quot;</span>
&gt;<span class="w">       </span><span class="nv">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>create_large_object<span class="o">(</span>1_000_000<span class="o">)</span>

tests/test_memory.py:6:<span class="w"> </span>
_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>
utils/memory_operations.py:7:<span class="w"> </span><span class="k">in</span><span class="w"> </span>create_large_object
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">[{</span><span class="s2">&quot;id&quot;</span>:<span class="w"> </span>i,<span class="w"> </span><span class="s2">&quot;data&quot;</span>:<span class="w"> </span><span class="s2">&quot;x&quot;</span><span class="w"> </span>*<span class="w"> </span><span class="m">10</span><span class="o">}</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>i<span class="w"> </span><span class="k">in</span><span class="w"> </span>range<span class="o">(</span>size<span class="o">)]</span>
_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>

HighWatermarkExceededError:<span class="w"> </span>The<span class="w"> </span><span class="nb">test</span><span class="w"> </span>allocated<span class="w"> </span><span class="m">123</span>.9MiB,<span class="w"> </span>which<span class="w"> </span>is<span class="w"> </span>more<span class="w"> </span>than<span class="w"> </span>the<span class="w"> </span><span class="m">100</span>.0MiB<span class="w"> </span>limit.
<span class="o">===========================</span><span class="w"> </span>short<span class="w"> </span><span class="nb">test</span><span class="w"> </span>summary<span class="w"> </span><span class="nv">info</span><span class="w"> </span><span class="o">============================</span>
FAILED<span class="w"> </span>tests/test_memory.py::test_create_large_object_with_limit<span class="w"> </span>-<span class="w"> </span>HighWater...
<span class="o">==============================</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>failed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.45s<span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p><strong>Let's parse this section by section</strong>:</p>
<ol>
<li>
<p><strong>The summary line</strong>: <code>FAILED ... - HighWatermarkExceededError</code></p>
<ul>
<li>What this tells us: The test failed not because of an <code>AssertionError</code>, but because of a specific error from the <code>memray</code> plugin.</li>
</ul>
</li>
<li>
<p><strong>The error message</strong>: <code>HighWatermarkExceededError: The test allocated 123.9MiB, which is more than the 100.0MiB limit.</code></p>
<ul>
<li>What this tells us: This is an incredibly clear and actionable error. It tells us exactly how much memory was allocated and how much it exceeded the limit by.</li>
</ul>
</li>
</ol>
<p><strong>Root cause identified</strong>: The <code>create_large_object</code> function allocates more than 100 MiB of memory.
<strong>What we need</strong>: We can now make an informed decision. Either the memory usage is acceptable and we should raise the limit in the test (e.g., <code>@pytest.mark.limit_memory("150 MB")</code>), or the function needs to be optimized to use less memory. This marker turns an invisible problem into a concrete, failing test.</p>
<h2 id="identifying-and-fixing-slow-tests">Identifying and Fixing Slow Tests</h2>
<h2 id="keeping-the-test-suite-fast">Keeping the Test Suite Fast</h2>
<p>So far, we've focused on the performance of the application code. Now, let's turn our attention to the test suite itself. As a project grows, the test suite can become slow, increasing CI/CD times and harming developer productivity.</p>
<p>We'll revisit the <code>--durations</code> flag, which is the perfect tool for this job.</p>
<h3 id="the-problem-a-slow-fixture">The Problem: A Slow Fixture</h3>
<p>A common cause of slow test suites is an expensive fixture that is set up more often than necessary. Imagine a fixture that reads a large configuration file or connects to a database. If it's <code>function</code>-scoped, this expensive operation will happen before every single test that uses it.</p>
<p>Let's simulate this. We'll create a fixture that simulates reading a large data file by sleeping for a short duration.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_slow_suite.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">large_dataset</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A function-scoped fixture that simulates a slow data load.</span>
<span class="sd">    This will run for EACH test that uses it.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">(Loading large dataset...)&quot;</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_data_mean</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">sum</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">)</span> <span class="o">==</span> <span class="mf">49.5</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_data_max</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">max</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">)</span> <span class="o">==</span> <span class="mi">99</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_data_min</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">min</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div>

<p>We have three tests that all use the <code>large_dataset</code> fixture. Since the fixture is function-scoped by default, the <code>time.sleep(0.2)</code> will run three times. The total run time should be over 0.6 seconds.</p>
<p>Let's confirm this with <code>--durations</code>.</p>
<div class="codehilite"><pre><span></span><code>pytest<span class="w"> </span>-v<span class="w"> </span>--durations<span class="o">=</span><span class="m">5</span><span class="w"> </span>tests/test_slow_suite.py
</code></pre></div>

<h3 id="diagnostic-analysis-pinpointing-the-slowdown">Diagnostic Analysis: Pinpointing the Slowdown</h3>
<p><strong>The complete output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
...
collected<span class="w"> </span><span class="m">3</span><span class="w"> </span>items

tests/test_slow_suite.py::test_data_mean<span class="w"> </span>
<span class="o">(</span>Loading<span class="w"> </span>large<span class="w"> </span>dataset...<span class="o">)</span>
PASSED<span class="w">                         </span><span class="o">[</span><span class="w"> </span><span class="m">33</span>%<span class="o">]</span>
tests/test_slow_suite.py::test_data_max<span class="w"> </span>
<span class="o">(</span>Loading<span class="w"> </span>large<span class="w"> </span>dataset...<span class="o">)</span>
PASSED<span class="w">                         </span><span class="o">[</span><span class="w"> </span><span class="m">66</span>%<span class="o">]</span>
tests/test_slow_suite.py::test_data_min<span class="w"> </span>
<span class="o">(</span>Loading<span class="w"> </span>large<span class="w"> </span>dataset...<span class="o">)</span>
PASSED<span class="w">                         </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">===========================</span><span class="w"> </span>slowest<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="nv">durations</span><span class="w"> </span><span class="o">===========================</span>
<span class="m">0</span>.20s<span class="w"> </span>setup<span class="w">    </span>tests/test_slow_suite.py::test_data_min
<span class="m">0</span>.20s<span class="w"> </span>setup<span class="w">    </span>tests/test_slow_suite.py::test_data_max
<span class="m">0</span>.20s<span class="w"> </span>setup<span class="w">    </span>tests/test_slow_suite.py::test_data_mean
<span class="m">0</span>.00s<span class="w"> </span>call<span class="w">     </span>tests/test_slow_suite.py::test_data_min
<span class="m">0</span>.00s<span class="w"> </span>call<span class="w">     </span>tests/test_slow_suite.py::test_data_max
<span class="o">==============================</span><span class="w"> </span><span class="m">3</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.61s<span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p><strong>Let's parse this section by section</strong>:</p>
<ol>
<li><strong>The print statements</strong>: We see <code>(Loading large dataset...)</code> printed three times, confirming our hypothesis that the fixture runs for each test.</li>
<li><strong>The durations table</strong>: This is the key insight. The three slowest items are all <code>setup</code> phases, each taking around 0.2 seconds. The <code>call</code> phases are nearly instantaneous.</li>
<li><strong>The total time</strong>: The suite took <code>0.61s</code>, which is roughly <code>3 * 0.2s</code> plus a small overhead.</li>
</ol>
<p><strong>Root cause identified</strong>: The slowness is not in the test logic (<code>call</code>) but in the fixture setup (<code>setup</code>). The <code>large_dataset</code> fixture is being wastefully re-created for every test.
<strong>What we need</strong>: A way to run the expensive setup operation only once for all tests that need it. This is a perfect use case for changing the fixture's scope.</p>
<h3 id="the-solution-changing-fixture-scope">The Solution: Changing Fixture Scope</h3>
<p>If the data being loaded is read-only and doesn't change between tests, we can change the fixture's scope from <code>function</code> to <code>module</code> or <code>session</code>. This will cause the fixture to be set up only once per module or once per the entire test session, respectively.</p>
<p><strong>Before: Default <code>function</code> scope</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_slow_suite.py (excerpt)</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">large_dataset</span><span class="p">():</span>
    <span class="c1"># ...</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</code></pre></div>

<p><strong>After: Efficient <code>module</code> scope</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_slow_suite.py (modified)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;module&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">large_dataset</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A module-scoped fixture.</span>
<span class="sd">    This will run only ONCE for all tests in this file.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">(Loading large dataset ONCE...)&quot;</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>

<span class="c1"># ... tests remain the same ...</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_data_mean</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">sum</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">)</span> <span class="o">==</span> <span class="mf">49.5</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_data_max</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">max</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">)</span> <span class="o">==</span> <span class="mi">99</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_data_min</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">min</span><span class="p">(</span><span class="n">large_dataset</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div>

<p>Let's run the tests again and observe the dramatic improvement.</p>
<div class="codehilite"><pre><span></span><code>pytest<span class="w"> </span>-v<span class="w"> </span>--durations<span class="o">=</span><span class="m">5</span><span class="w"> </span>tests/test_slow_suite.py
</code></pre></div>

<p><strong>The verification output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
...
collected<span class="w"> </span><span class="m">3</span><span class="w"> </span>items

tests/test_slow_suite.py::test_data_mean<span class="w"> </span>
<span class="o">(</span>Loading<span class="w"> </span>large<span class="w"> </span>dataset<span class="w"> </span>ONCE...<span class="o">)</span>
PASSED<span class="w">                         </span><span class="o">[</span><span class="w"> </span><span class="m">33</span>%<span class="o">]</span>
tests/test_slow_suite.py::test_data_max<span class="w"> </span>PASSED<span class="w">                         </span><span class="o">[</span><span class="w"> </span><span class="m">66</span>%<span class="o">]</span>
tests/test_slow_suite.py::test_data_min<span class="w"> </span>PASSED<span class="w">                         </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">===========================</span><span class="w"> </span>slowest<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span><span class="nv">durations</span><span class="w"> </span><span class="o">===========================</span>
<span class="m">0</span>.20s<span class="w"> </span>setup<span class="w">    </span>tests/test_slow_suite.py::test_data_mean
<span class="m">0</span>.00s<span class="w"> </span>call<span class="w">     </span>tests/test_slow_suite.py::test_data_min
<span class="m">0</span>.00s<span class="w"> </span>call<span class="w">     </span>tests/test_slow_suite.py::test_data_max
<span class="m">0</span>.00s<span class="w"> </span>call<span class="w">     </span>tests/test_slow_suite.py::test_data_mean
<span class="m">0</span>.00s<span class="w"> </span>teardown<span class="w"> </span>tests/test_slow_suite.py::test_data_mean
<span class="o">==============================</span><span class="w"> </span><span class="m">3</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.21s<span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p>The improvement is clear:
-   The print statement <code>(Loading large dataset ONCE...)</code> appears only once.
-   The total test time has dropped from <code>0.61s</code> to <code>0.21s</code>, a 3x speedup.
-   The durations table shows only one slow <code>setup</code> phase. Pytest is smart enough to attribute the setup time to the first test that requested the fixture.</p>
<p>This simple change‚Äîadding <code>scope="module"</code>‚Äîis one of the most effective ways to speed up a test suite.</p>
<h2 id="performance-testing-in-cicd">Performance Testing in CI/CD</h2>
<h2 id="automating-performance-checks">Automating Performance Checks</h2>
<p>The true power of these tools is realized when they are integrated into your Continuous Integration / Continuous Deployment (CI/CD) pipeline. This allows you to automatically catch performance regressions before they are merged.</p>
<h3 id="the-journey-from-problem-to-solution">The Journey: From Problem to Solution</h3>
<table>
<thead>
<tr>
<th>Iteration</th>
<th>Failure Mode</th>
<th>Technique Applied</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Slow tests are hidden in the suite</td>
<td>None</td>
<td>Slow CI builds, developer frustration</td>
</tr>
<tr>
<td>1</td>
<td>Noisy, unreliable performance measurements</td>
<td><code>pytest --durations</code></td>
<td>Can find slow tests, but useless for benchmarking</td>
</tr>
<tr>
<td>2</td>
<td>Inability to compare code performance</td>
<td><code>pytest-benchmark</code> fixture</td>
<td>Reliable, statistical measurement of a single function</td>
</tr>
<tr>
<td>3</td>
<td>No context for performance numbers</td>
<td>Parametrized benchmark tests</td>
<td>Direct, side-by-side comparison of implementations</td>
</tr>
<tr>
<td>4</td>
<td>Memory usage is completely invisible</td>
<td><code>pytest --memray</code></td>
<td>Detailed reports of memory allocation</td>
</tr>
<tr>
<td>5</td>
<td>Memory regressions are not prevented</td>
<td><code>@pytest.mark.limit_memory</code></td>
<td>Automated failure if memory usage exceeds a threshold</td>
</tr>
<tr>
<td>6</td>
<td>Inefficient fixtures slow down the suite</td>
<td><code>scope="module"</code>/<code>"session"</code></td>
<td>Drastically reduced test suite execution time</td>
</tr>
<tr>
<td>7</td>
<td>Regressions are caught manually, if at all</td>
<td>CI/CD integration</td>
<td>Automated, preventative performance testing</td>
</tr>
</tbody>
</table>
<h3 id="a-cicd-workflow-for-performance">A CI/CD Workflow for Performance</h3>
<p>Here is a practical workflow for integrating <code>pytest-benchmark</code> into a CI pipeline (e.g., GitHub Actions, GitLab CI).</p>
<p><strong>Step 1: Establish a Baseline</strong></p>
<p>On your main branch, run your benchmarks and save the results to a file. This file represents the "known good" performance of your code.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># This command runs the benchmarks and saves the results to .benchmarks/</span>
pytest<span class="w"> </span>--benchmark-save<span class="o">=</span>baseline
</code></pre></div>

<p>This will create a JSON file in a <code>.benchmarks/</code> directory. Commit this file to your repository. This baseline should be updated periodically as your application's performance characteristics intentionally change.</p>
<p><strong>Step 2: Compare on Pull Requests</strong></p>
<p>In your CI configuration for pull requests, add a step that runs the benchmarks and compares them against the saved baseline.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># This command compares the current run against the saved baseline</span>
<span class="c1"># It will fail the build if a statistically significant regression is detected.</span>
pytest<span class="w"> </span>--benchmark-compare<span class="o">=</span>baseline<span class="w"> </span>--benchmark-compare-fail<span class="o">=</span>mean:5%
</code></pre></div>

<p>Let's break down that command:
-   <code>--benchmark-compare=baseline</code>: Specifies the group of saved results to compare against.
-   <code>--benchmark-compare-fail=mean:5%</code>: This is the crucial part. It tells <code>pytest-benchmark</code> to fail the CI job if the <code>mean</code> time of any benchmark increases by more than <code>5%</code>. You can set thresholds for <code>min</code>, <code>max</code>, <code>median</code>, etc., and use different percentages.</p>
<p>If a developer pushes a change that makes <code>find_common_elements_optimized</code> 10% slower, the CI build will fail with a clear message indicating a performance regression.</p>
<p><strong>Step 3: Monitor Memory and Slow Tests</strong></p>
<p>While <code>pytest-benchmark</code> is great for micro-benchmarks, you should also monitor the overall health of your suite.</p>
<ul>
<li><strong>Memory Limits</strong>: Keep tests with <code>@pytest.mark.limit_memory</code> in your suite. They will automatically fail in CI if a change causes memory usage to spike.</li>
<li><strong>Slow Test Report</strong>: Periodically run <code>pytest --durations=20</code> in a CI job and publish the report. This doesn't need to fail the build, but it creates visibility into the test suite's health, allowing the team to proactively refactor the slowest tests.</li>
</ul>
<p>By combining these techniques, you create a robust, automated safety net that guards not just the correctness of your code, but also its performance and efficiency.</p>
        </div>
        <div class="footer">
            Generated on 2025-11-24 14:31:16 | Made with ‚ù§Ô∏è by GitHub Pages Generator
        </div>
    </div>
    <script>
        // Syntax highlighting for code blocks
        document.addEventListener('DOMContentLoaded', (event) => {
            // Highlight code blocks
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightElement(block);
            });
            
            // Add copy buttons to code blocks
            document.querySelectorAll('pre').forEach((pre) => {
                const button = document.createElement('button');
                button.className = 'copy-btn';
                button.textContent = 'Copy';
                
                button.addEventListener('click', () => {
                    const code = pre.querySelector('code').textContent;
                    navigator.clipboard.writeText(code).then(() => {
                        button.textContent = 'Copied!';
                        button.classList.add('copied');
                        setTimeout(() => {
                            button.textContent = 'Copy';
                            button.classList.remove('copied');
                        }, 2000);
                    }).catch(err => {
                        console.error('Failed to copy:', err);
                        button.textContent = 'Error';
                        setTimeout(() => {
                            button.textContent = 'Copy';
                        }, 2000);
                    });
                });
                
                pre.appendChild(button);
            });
        });
    </script>
</body>
</html>