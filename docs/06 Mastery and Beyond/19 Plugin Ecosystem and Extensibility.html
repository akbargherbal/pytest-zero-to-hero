<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>19 Plugin Ecosystem and Extensibility</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <style>
        :root {
            --primary: #6366f1;
            --primary-dark: #4f46e5;
            --secondary: #8b5cf6;
            --bg-main: #0f172a;
            --bg-card: #1e293b;
            --bg-code: #0f172a;
            --text-primary: #f1f5f9;
            --text-secondary: #94a3b8;
            --border: #334155;
            --accent: #06b6d4;
            --success: #10b981;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body { 
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.7; 
            color: var(--text-primary); 
            background: var(--bg-main);
        }
        
        .container { 
            max-width: 1200px; 
            margin: 0 auto; 
            padding: 20px; 
        }
        
        .home-btn { 
            position: fixed; 
            top: 20px; 
            right: 20px; 
            background: var(--accent);
            color: white; 
            width: 50px;
            height: 50px;
            border-radius: 50%;
            text-decoration: none; 
            z-index: 1000;
            box-shadow: 0 4px 20px rgba(6, 182, 212, 0.4);
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            font-size: 24px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .home-btn:hover { 
            background: #0891b2;
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(6, 182, 212, 0.5);
        }
        
        .breadcrumb { 
            background: var(--bg-card);
            padding: 12px 0; 
            margin-bottom: 24px; 
            font-size: 14px;
            border-radius: 8px;
            border: 1px solid var(--border);
        }
        
        .breadcrumb a { 
            color: var(--accent);
            text-decoration: none;
            transition: color 0.2s;
        }
        
        .breadcrumb a:hover { 
            color: var(--primary);
            text-decoration: underline;
        }
        
        .content { 
            background: var(--bg-card);
            padding: 3rem; 
            border-radius: 16px; 
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
            border: 1px solid var(--border);
        }
        
        .file-list { 
            display: grid; 
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr)); 
            gap: 1.5rem; 
            margin: 2rem 0; 
        }
        
        .file-item { 
            padding: 1.5rem; 
            border: 1px solid var(--border);
            border-radius: 12px; 
            background: var(--bg-main);
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            overflow: hidden;
        }
        
        .file-item::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 3px;
            background: linear-gradient(90deg, var(--primary), var(--secondary));
            opacity: 0;
            transition: opacity 0.3s;
        }
        
        .file-item:hover { 
            transform: translateY(-4px); 
            box-shadow: 0 8px 30px rgba(99, 102, 241, 0.3);
            border-color: var(--primary);
        }
        
        .file-item:hover::before {
            opacity: 1;
        }
        
        .file-item a { 
            color: var(--text-primary);
            text-decoration: none; 
            font-weight: 600; 
            display: block;
            font-size: 1.1rem;
        }
        
        .file-item a:hover { 
            color: var(--primary);
        }
        
        .file-type { 
            font-size: 13px; 
            color: var(--text-secondary);
            margin-top: 8px; 
            font-weight: 500;
        }
        
        /* Code Blocks */
        pre { 
            background: var(--bg-code);
            padding: 1.5rem; 
            border-radius: 12px; 
            overflow-x: auto;
            border: 1px solid var(--border);
            margin: 1.5rem 0;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3);
            position: relative;
        }
        
        pre code { 
            background: none;
            padding: 0;
            font-family: 'Fira Code', 'Consolas', monospace;
            font-size: 0.9em;
            line-height: 1.6;
        }
        
        /* Copy Button */
        .copy-btn {
            position: absolute;
            top: 8px;
            right: 8px;
            background: var(--primary);
            color: white;
            border: none;
            padding: 6px 12px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 12px;
            font-weight: 600;
            transition: all 0.2s;
            opacity: 0.7;
            z-index: 10;
        }
        
        .copy-btn:hover {
            opacity: 1;
            background: var(--primary-dark);
            transform: translateY(-1px);
        }
        
        .copy-btn.copied {
            background: var(--success);
        }
        
        pre:hover .copy-btn {
            opacity: 1;
        }
        
        /* Inline Code */
        code { 
            background: var(--bg-code);
            color: #8b5cf6;
            padding: 3px 8px; 
            border-radius: 6px; 
            font-size: 1.1em;
            font-family: 'Fira Code', 'Consolas', monospace;
            border: 1px solid var(--border);
        }
        
        /* Headings */
        h1, h2, h3, h4, h5, h6 { 
            color: var(--text-primary);
            margin: 2rem 0 1rem 0;
            font-weight: 700;
            letter-spacing: -0.5px;
        }
        
        h1 { 
            font-size: 2.5rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            border-bottom: 3px solid var(--primary);
            padding-bottom: 12px;
            margin-bottom: 1.5rem;
        }
        
        h2 { 
            font-size: 2rem;
            color: var(--primary);
            border-bottom: 2px solid var(--border);
            padding-bottom: 8px;
        }
        
        h3 { 
            font-size: 1.5rem;
            color: var(--accent);
        }
        
        /* Links */
        a { 
            color: var(--accent);
            transition: color 0.2s;
        }
        
        a:hover { 
            color: var(--primary);
        }
        
        /* Paragraphs */
        p {
            margin: 1rem 0;
            color: var(--text-secondary);
        }
        
        /* Lists */
        ul, ol {
            margin: 1rem 0;
            padding-left: 2rem;
            color: var(--text-secondary);
        }
        
        li {
            margin: 0.5rem 0;
        }
        
        /* Tables */
        table { 
            border-collapse: collapse;
            width: 100%;
            margin: 1.5rem 0;
            background: var(--bg-main);
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
        }
        
        th, td { 
            border: 1px solid var(--border);
            padding: 12px 16px;
            text-align: left;
        }
        
        th { 
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            font-weight: 600;
        }
        
        tr:hover {
            background: var(--bg-card);
        }
        
        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--primary);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            background: var(--bg-main);
            border-radius: 0 8px 8px 0;
            color: var(--text-secondary);
            font-style: italic;
        }
        
        /* Horizontal Rule */
        hr {
            border: none;
            border-top: 2px solid var(--border);
            margin: 2rem 0;
        }
        
        .footer { 
            text-align: center; 
            padding: 2rem; 
            color: var(--text-secondary);
            border-top: 1px solid var(--border);
            margin-top: 3rem; 
            font-size: 14px;
        }
        
        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 12px;
            height: 12px;
        }
        
        ::-webkit-scrollbar-track {
            background: var(--bg-main);
        }
        
        ::-webkit-scrollbar-thumb {
            background: var(--border);
            border-radius: 6px;
        }
        
        ::-webkit-scrollbar-thumb:hover {
            background: var(--primary);
        }
        
        @media (max-width: 768px) {
            .container { padding: 10px; }
            .file-list { grid-template-columns: 1fr; }
            .content { padding: 1.5rem; }
            h1 { font-size: 2rem; }
            h2 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>
    <a href="../index.html" class="home-btn">üè†</a>
    <div class="container">
        <div class="breadcrumb">
            <div style="padding: 0 20px;">
                <a href="../index.html">üè† Home</a> <span style="color: #64748b;">/</span> <a href="index.html">06 Mastery and Beyond</a>
            </div>
        </div>
        <div class="content">
            <h1 id="chapter-19-plugin-ecosystem-and-extensibility">Chapter 19: Plugin Ecosystem and Extensibility</h1>
<h2 id="popular-pytest-plugins-pytest-html-pytest-xdist-pytest-sugar">Popular Pytest Plugins (pytest-html, pytest-xdist, pytest-sugar)</h2>
<h2 id="the-plugin-ecosystem-extending-pytests-capabilities">The Plugin Ecosystem: Extending Pytest's Capabilities</h2>
<p>Pytest's power comes not just from its core features, but from its extensible architecture. The plugin ecosystem transforms pytest from a testing framework into a complete testing platform. In this chapter, we'll explore how plugins work, how to use popular ones, and how to create your own.</p>
<p>Before diving into plugin creation, let's understand what plugins can do by examining three widely-used examples that solve real testing problems.</p>
<h3 id="the-reference-scenario-a-growing-test-suite">The Reference Scenario: A Growing Test Suite</h3>
<p>Let's establish a realistic testing scenario that will demonstrate why plugins matter. We're testing a web API client that handles user authentication, data retrieval, and error handling.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># src/api_client.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">class</span><span class="w"> </span><span class="nc">APIClient</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">api_key</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_url</span> <span class="o">=</span> <span class="n">base_url</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">api_key</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">session</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;Authorization&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;Bearer </span><span class="si">{</span><span class="n">api_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">})</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_user</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fetch user data from the API.&quot;&quot;&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/users/</span><span class="si">{</span><span class="n">user_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_user</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">username</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">email</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new user.&quot;&quot;&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">post</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/users&quot;</span><span class="p">,</span>
            <span class="n">json</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="n">username</span><span class="p">,</span> <span class="s2">&quot;email&quot;</span><span class="p">:</span> <span class="n">email</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update_user</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update user data.&quot;&quot;&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">patch</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/users/</span><span class="si">{</span><span class="n">user_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">json</span><span class="o">=</span><span class="n">data</span>
        <span class="p">)</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">delete_user</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">user_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Delete a user.&quot;&quot;&quot;</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/users/</span><span class="si">{</span><span class="n">user_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">search_users</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">limit</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Search for users matching query.&quot;&quot;&quot;</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># Simulates slow API call</span>
        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">base_url</span><span class="si">}</span><span class="s2">/users/search&quot;</span><span class="p">,</span>
            <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;q&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="s2">&quot;limit&quot;</span><span class="p">:</span> <span class="n">limit</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&quot;results&quot;</span><span class="p">]</span>
</code></pre></div>

<p>Our initial test suite covers the basic functionality:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_api_client.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">unittest.mock</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mock</span><span class="p">,</span> <span class="n">patch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.api_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">APIClient</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">api_client</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create an API client for testing.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">APIClient</span><span class="p">(</span><span class="s2">&quot;https://api.example.com&quot;</span><span class="p">,</span> <span class="s2">&quot;test-key-123&quot;</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mock_response</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Create a mock response object.&quot;&quot;&quot;</span>
    <span class="n">mock</span> <span class="o">=</span> <span class="n">Mock</span><span class="p">()</span>
    <span class="n">mock</span><span class="o">.</span><span class="n">status_code</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">mock</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;testuser&quot;</span><span class="p">}</span>
    <span class="n">mock</span><span class="o">.</span><span class="n">raise_for_status</span> <span class="o">=</span> <span class="n">Mock</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mock</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_get_user_success</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test successful user retrieval.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">get_user</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;username&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;testuser&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_create_user_success</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test successful user creation.&quot;&quot;&quot;</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;newuser&quot;</span><span class="p">,</span>
        <span class="s2">&quot;email&quot;</span><span class="p">:</span> <span class="s2">&quot;new@example.com&quot;</span>
    <span class="p">}</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;post&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">create_user</span><span class="p">(</span><span class="s2">&quot;newuser&quot;</span><span class="p">,</span> <span class="s2">&quot;new@example.com&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;username&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;newuser&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_update_user_success</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test successful user update.&quot;&quot;&quot;</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;updateduser&quot;</span>
    <span class="p">}</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;patch&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">update_user</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;updateduser&quot;</span><span class="p">})</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;username&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;updateduser&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_delete_user_success</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test successful user deletion.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;delete&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">delete_user</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">result</span> <span class="ow">is</span> <span class="kc">True</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_search_users_slow</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test user search (slow operation).&quot;&quot;&quot;</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;results&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;user1&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;user2&quot;</span><span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">search_users</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
</code></pre></div>

<p>Let's run this test suite and observe what happens:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_client.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>============================= test session starts ==============================
platform linux -- Python 3.11.0, pytest-7.4.3, pluggy-1.3.0
cachedir: .pytest_cache
rootdir: /home/user/project
collected 5 items

tests/test_api_client.py::test_get_user_success PASSED                   [ 20%]
tests/test_api_client.py::test_create_user_success PASSED                [ 40%]
tests/test_api_client.py::test_update_user_success PASSED                [ 60%]
tests/test_api_client.py::test_delete_user_success PASSED                [ 80%]
tests/test_api_client.py::test_search_users_slow PASSED                  [100%]

============================== 5 passed in 2.73s ===============================
</code></pre></div>

<p><strong>Current state</strong>: Our tests pass, but we're facing three common problems:</p>
<ol>
<li><strong>Visibility problem</strong>: The output is functional but not visually appealing or easy to scan</li>
<li><strong>Speed problem</strong>: The test suite takes 2.73 seconds for just 5 tests (the search test includes a 0.5s sleep)</li>
<li><strong>Reporting problem</strong>: We have no persistent record of test results for CI/CD or historical analysis</li>
</ol>
<p>These are exactly the problems that popular pytest plugins solve. Let's address them one by one.</p>
<h2 id="pytest-sugar-making-test-output-beautiful">pytest-sugar: Making Test Output Beautiful</h2>
<p>The first plugin we'll explore is <code>pytest-sugar</code>, which transforms pytest's output from functional to delightful.</p>
<h3 id="installation-and-first-run">Installation and First Run</h3>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>pytest-sugar
</code></pre></div>

<p>Now run the same tests again:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_client.py
</code></pre></div>

<p><strong>Output with pytest-sugar</strong>:</p>
<div class="codehilite"><pre><span></span><code> tests/test_api_client.py ‚úì‚úì‚úì‚úì‚úì                                        100% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

Results (2.73s):
       5 passed
</code></pre></div>

<p><strong>What changed?</strong></p>
<ol>
<li><strong>Visual progress bar</strong>: Instead of line-by-line output, we see a progress bar with checkmarks</li>
<li><strong>Real-time feedback</strong>: Each test completion shows immediately with a ‚úì symbol</li>
<li><strong>Cleaner summary</strong>: The final summary is more compact and visually distinct</li>
<li><strong>Color coding</strong>: Passed tests show in green, failures in red (not visible in text)</li>
</ol>
<h3 id="understanding-pytest-sugars-enhancements">Understanding pytest-sugar's Enhancements</h3>
<p>pytest-sugar doesn't change how tests run‚Äîit changes how results are displayed. Let's see what happens when a test fails:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_api_client.py (add this test)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_get_user_wrong_id</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that demonstrates failure output.&quot;&quot;&quot;</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">999</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;wronguser&quot;</span><span class="p">}</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">get_user</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>  <span class="c1"># This will fail</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_client.py::test_get_user_wrong_id
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code> tests/test_api_client.py ‚®Ø                                             100% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï test_get_user_wrong_id ‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï

    def test_get_user_wrong_id(api_client, mock_response):
        &quot;&quot;&quot;Test that demonstrates failure output.&quot;&quot;&quot;
        mock_response.json.return_value = {&quot;id&quot;: 999, &quot;username&quot;: &quot;wronguser&quot;}
        with patch.object(api_client.session, &#39;get&#39;, return_value=mock_response):
            result = api_client.get_user(1)
<span class="k">&gt; </span><span class="ge">          assert result[&quot;id&quot;] == 1</span>
E           assert 999 == 1

tests/test_api_client.py:58: AssertionError

Results (0.12s):
       1 failed
         <span class="k">-</span> tests/test_api_client.py:53 test_get_user_wrong_id
</code></pre></div>

<p><strong>Key improvements in failure display</strong>:</p>
<ol>
<li><strong>‚®Ø symbol</strong>: Immediately shows which test failed in the progress bar</li>
<li><strong>Section separator</strong>: Clear visual boundary around the failure details</li>
<li><strong>Preserved detail</strong>: All pytest's assertion introspection remains intact</li>
<li><strong>Summary with location</strong>: The results section shows exactly where the failure occurred</li>
</ol>
<h3 id="when-pytest-sugar-shines">When pytest-sugar Shines</h3>
<p><strong>Best for</strong>:
- Development workflows where you're running tests frequently
- Teams that value visual feedback and modern terminal aesthetics
- Large test suites where scanning output quickly matters</p>
<p><strong>Limitations</strong>:
- Adds minimal overhead (usually imperceptible)
- Some CI systems may not render the enhanced output correctly
- Can be disabled with <code>pytest --no-sugar</code> if needed</p>
<h2 id="pytest-xdist-parallel-test-execution">pytest-xdist: Parallel Test Execution</h2>
<p>Now let's address the speed problem. Our test suite takes 2.73 seconds for 5 tests. As test suites grow to hundreds or thousands of tests, execution time becomes a bottleneck.</p>
<h3 id="the-problem-sequential-execution">The Problem: Sequential Execution</h3>
<p>By default, pytest runs tests sequentially‚Äîone after another. Let's add more tests to make the problem more visible:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_api_client.py (add these tests)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_search_users_multiple_queries</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test multiple search queries.&quot;&quot;&quot;</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;results&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}]}</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">results1</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">search_users</span><span class="p">(</span><span class="s2">&quot;alice&quot;</span><span class="p">)</span>
        <span class="n">results2</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">search_users</span><span class="p">(</span><span class="s2">&quot;bob&quot;</span><span class="p">)</span>
        <span class="n">results3</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">search_users</span><span class="p">(</span><span class="s2">&quot;charlie&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">results1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_search_users_with_limit</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test search with custom limit.&quot;&quot;&quot;</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;results&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="n">i</span><span class="p">}</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]}</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">search_users</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_search_users_empty_results</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test search with no results.&quot;&quot;&quot;</span>
    <span class="n">mock_response</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;results&quot;</span><span class="p">:</span> <span class="p">[]}</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">search_users</span><span class="p">(</span><span class="s2">&quot;nonexistent&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_client.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>============================= test session starts ==============================
collected 8 items

tests/test_api_client.py::test_get_user_success PASSED                   [ 12%]
tests/test_api_client.py::test_create_user_success PASSED                [ 25%]
tests/test_api_client.py::test_update_user_success PASSED                [ 37%]
tests/test_api_client.py::test_delete_user_success PASSED                [ 50%]
tests/test_api_client.py::test_search_users_slow PASSED                  [ 62%]
tests/test_api_client.py::test_search_users_multiple_queries PASSED      [ 75%]
tests/test_api_client.py::test_search_users_with_limit PASSED            [ 87%]
tests/test_api_client.py::test_search_users_empty_results PASSED         [100%]

============================== 8 passed in 4.23s ===============================
</code></pre></div>

<p><strong>The problem</strong>: 8 tests now take 4.23 seconds. The search tests each include a 0.5s sleep, and they run sequentially. With 4 search tests, that's 2 seconds of just waiting.</p>
<h3 id="installing-pytest-xdist">Installing pytest-xdist</h3>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>pytest-xdist
</code></pre></div>

<p>Now run the tests with parallel execution:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_client.py<span class="w"> </span>-n<span class="w"> </span>auto
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>============================= test session starts ==============================
platform linux -- Python 3.11.0, pytest-7.4.3, pluggy-1.3.0
cachedir: .pytest_cache
rootdir: /home/user/project
plugins: xdist-3.5.0
gw0 [8] / gw1 [8] / gw2 [8] / gw3 [8]
<span class="k">........</span>                                                                  [100%]

============================== 8 passed in 1.47s ===============================
</code></pre></div>

<p><strong>What happened?</strong></p>
<ol>
<li><strong>Parallel workers</strong>: <code>gw0</code>, <code>gw1</code>, <code>gw2</code>, <code>gw3</code> indicate 4 worker processes (auto-detected from CPU cores)</li>
<li><strong>Execution time</strong>: Dropped from 4.23s to 1.47s‚Äîa 65% reduction</li>
<li><strong>Test distribution</strong>: pytest-xdist automatically distributed tests across workers</li>
</ol>
<h3 id="understanding-the-n-flag">Understanding the -n Flag</h3>
<p>The <code>-n</code> flag controls parallelization:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Use auto-detection (recommended)</span>
$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span>auto

<span class="c1"># Specify exact number of workers</span>
$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span><span class="m">4</span>

<span class="c1"># Use logical CPU count</span>
$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span>logical
</code></pre></div>

<h3 id="diagnostic-analysis-how-parallel-execution-works">Diagnostic Analysis: How Parallel Execution Works</h3>
<p>Let's understand what's happening under the hood by adding some instrumentation:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_api_client.py (modify a test)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_search_users_slow</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test user search (slow operation).&quot;&quot;&quot;</span>
    <span class="n">worker_id</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;PYTEST_XDIST_WORKER&#39;</span><span class="p">,</span> <span class="s1">&#39;main&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[Worker </span><span class="si">{</span><span class="n">worker_id</span><span class="si">}</span><span class="s2">] Running test_search_users_slow&quot;</span><span class="p">)</span>

    <span class="n">mock_response</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;results&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;user1&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;user2&quot;</span><span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">search_users</span><span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_client.py::test_search_users_slow<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>-s
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=============================</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">session</span><span class="w"> </span><span class="n">starts</span><span class="w"> </span><span class="o">==============================</span>
<span class="n">gw0</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">gw1</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="p">[</span><span class="n">Worker</span><span class="w"> </span><span class="n">gw0</span><span class="p">]</span><span class="w"> </span><span class="n">Running</span><span class="w"> </span><span class="n">test_search_users_slow</span>
<span class="p">.</span><span class="w">                                                                         </span><span class="p">[</span><span class="mi">100</span><span class="o">%</span><span class="p">]</span>

<span class="o">==============================</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">passed</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mf">0.67</span><span class="n">s</span><span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p><strong>Key insight</strong>: Each test runs in a separate worker process with its own environment. The <code>PYTEST_XDIST_WORKER</code> environment variable identifies which worker is running the test.</p>
<h3 id="when-parallel-execution-fails-test-isolation-issues">When Parallel Execution Fails: Test Isolation Issues</h3>
<p>Parallel execution exposes a critical requirement: <strong>tests must be independent</strong>. Let's see what happens when they're not:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_shared_state.py</span>
<span class="c1"># WARNING: This demonstrates BAD practice</span>
<span class="n">shared_counter</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_increment_counter_1</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;First test that modifies shared state.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">shared_counter</span>
    <span class="n">shared_counter</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">shared_counter</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_increment_counter_2</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Second test that depends on shared state.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">shared_counter</span>
    <span class="n">shared_counter</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">shared_counter</span> <span class="o">==</span> <span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_increment_counter_3</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Third test that depends on shared state.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">shared_counter</span>
    <span class="n">shared_counter</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">shared_counter</span> <span class="o">==</span> <span class="mi">3</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_shared_state.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output (sequential)</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_shared_state.py::test_increment_counter_1 PASSED              [ 33%]
tests/test_shared_state.py::test_increment_counter_2 PASSED              [ 66%]
tests/test_shared_state.py::test_increment_counter_3 PASSED              [100%]

============================== 3 passed in 0.01s ===============================
</code></pre></div>

<p>Now run with parallelization:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_shared_state.py<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>-v
</code></pre></div>

<p><strong>Output (parallel)</strong>:</p>
<div class="codehilite"><pre><span></span><code>gw0 [3] / gw1 [3]
tests/test_shared_state.py::test_increment_counter_1 PASSED              [ 33%]
tests/test_shared_state.py::test_increment_counter_2 FAILED              [ 66%]
tests/test_shared_state.py::test_increment_counter_3 PASSED              [100%]

=================================== FAILURES ===================================
__________________________ test_increment_counter_2 ____________________________

    def test_increment_counter_2():
        &quot;&quot;&quot;Second test that depends on shared state.&quot;&quot;&quot;
        global shared_counter
        shared_counter += 1
<span class="k">&gt; </span><span class="ge">      assert shared_counter == 2</span>
E       assert 1 == 2

tests/test_shared_state.py:13: AssertionError
</code></pre></div>

<h3 id="diagnostic-analysis-reading-the-parallel-failure">Diagnostic Analysis: Reading the Parallel Failure</h3>
<p><strong>The complete output shows</strong>:</p>
<ol>
<li><strong>Worker distribution</strong>: Tests ran on <code>gw0</code> and <code>gw1</code> simultaneously</li>
<li><strong>Failure pattern</strong>: <code>test_increment_counter_2</code> failed because it ran in a different worker than <code>test_increment_counter_1</code></li>
<li><strong>Root cause</strong>: Each worker has its own process memory‚Äî<code>shared_counter</code> starts at 0 in each worker</li>
</ol>
<p><strong>What this tells us</strong>: Parallel execution reveals hidden dependencies between tests. Tests that pass sequentially but fail in parallel are <strong>not truly independent</strong>.</p>
<p><strong>The fix</strong>: Use fixtures for setup instead of shared state:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_shared_state_fixed.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">counter</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Provide a fresh counter for each test.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_increment_counter_1</span><span class="p">(</span><span class="n">counter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;First test with isolated state.&quot;&quot;&quot;</span>
    <span class="n">counter</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">counter</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_increment_counter_2</span><span class="p">(</span><span class="n">counter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Second test with isolated state.&quot;&quot;&quot;</span>
    <span class="n">counter</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">counter</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>  <span class="c1"># Each test starts fresh</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_increment_counter_3</span><span class="p">(</span><span class="n">counter</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Third test with isolated state.&quot;&quot;&quot;</span>
    <span class="n">counter</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">counter</span><span class="p">[</span><span class="s2">&quot;value&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_shared_state_fixed.py<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span><span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>gw0 [3] / gw1 [3]
tests/test_shared_state_fixed.py::test_increment_counter_1 PASSED        [ 33%]
tests/test_shared_state_fixed.py::test_increment_counter_2 PASSED        [ 66%]
tests/test_shared_state_fixed.py::test_increment_counter_3 PASSED        [100%]

============================== 3 passed in 0.23s ===============================
</code></pre></div>

<p><strong>Result</strong>: All tests pass in parallel because each has isolated state through fixtures.</p>
<h3 id="when-to-use-pytest-xdist">When to Use pytest-xdist</h3>
<p><strong>Best for</strong>:
- Large test suites (100+ tests) where execution time matters
- CI/CD pipelines where faster feedback is critical
- Tests that are truly independent (no shared state, no database dependencies)</p>
<p><strong>Avoid when</strong>:
- Tests share resources (databases, files, network ports)
- Tests have strict ordering requirements
- Debugging failures (parallel output is harder to read)</p>
<p><strong>Pro tip</strong>: Use <code>-n 0</code> to disable parallelization temporarily:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span><span class="m">0</span><span class="w">  </span><span class="c1"># Forces sequential execution even if xdist is installed</span>
</code></pre></div>

<h2 id="pytest-html-generating-test-reports">pytest-html: Generating Test Reports</h2>
<p>The third common need is <strong>persistent test reports</strong>. Console output disappears after the test run, but HTML reports provide:</p>
<ol>
<li><strong>Historical record</strong>: Archive test results over time</li>
<li><strong>CI/CD integration</strong>: Attach reports to build artifacts</li>
<li><strong>Stakeholder communication</strong>: Share results with non-technical team members</li>
</ol>
<h3 id="installation-and-basic-usage">Installation and Basic Usage</h3>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>pytest-html
</code></pre></div>

<p>Generate a report for our API client tests:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_client.py<span class="w"> </span>--html<span class="o">=</span>report.html<span class="w"> </span>--self-contained-html
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>============================= test session starts ==============================
collected 8 items

tests/test_api_client.py ........                                        [100%]

============================== 8 passed in 2.15s ===============================
Generated html report: file:///home/user/project/report.html
</code></pre></div>

<p><strong>What was created?</strong></p>
<p>The <code>--self-contained-html</code> flag creates a single HTML file with all assets embedded. Open <code>report.html</code> in a browser to see:</p>
<ol>
<li><strong>Summary section</strong>: Total tests, pass/fail counts, duration</li>
<li><strong>Environment details</strong>: Python version, pytest version, plugins</li>
<li><strong>Test results table</strong>: Each test with status, duration, and details</li>
<li><strong>Expandable failures</strong>: Click to see full traceback and assertion details</li>
</ol>
<h3 id="customizing-report-content">Customizing Report Content</h3>
<p>Let's add more information to our tests that will appear in the report:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_api_client.py (modify tests)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_get_user_success</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test successful user retrieval.&quot;&quot;&quot;</span>
    <span class="c1"># Add extra information for the report</span>
    <span class="n">pytest</span><span class="o">.</span><span class="n">extra</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;endpoint&quot;</span><span class="p">:</span> <span class="s2">&quot;/users/1&quot;</span><span class="p">,</span>
        <span class="s2">&quot;method&quot;</span><span class="p">:</span> <span class="s2">&quot;GET&quot;</span><span class="p">,</span>
        <span class="s2">&quot;expected_status&quot;</span><span class="p">:</span> <span class="mi">200</span>
    <span class="p">}</span>

    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">get_user</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;username&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;testuser&quot;</span>
</code></pre></div>

<h3 id="adding-screenshots-and-logs-to-reports">Adding Screenshots and Logs to Reports</h3>
<p>For web testing or complex scenarios, you can attach additional artifacts:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook to add extra information to test reports.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="c1"># Add timestamp to all tests</span>
    <span class="n">report</span><span class="o">.</span><span class="n">timestamp</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">()</span>

    <span class="c1"># Add extra details for failures</span>
    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span> <span class="ow">and</span> <span class="n">report</span><span class="o">.</span><span class="n">failed</span><span class="p">:</span>
        <span class="c1"># You could add screenshots, logs, etc. here</span>
        <span class="n">report</span><span class="o">.</span><span class="n">extra_info</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;test_name&quot;</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="s2">&quot;failure_time&quot;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">()</span>
        <span class="p">}</span>
</code></pre></div>

<h3 id="combining-plugins-the-complete-workflow">Combining Plugins: The Complete Workflow</h3>
<p>Now let's use all three plugins together for the optimal development experience:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_client.py<span class="w"> </span>-n<span class="w"> </span>auto<span class="w"> </span>--html<span class="o">=</span>report.html<span class="w"> </span>--self-contained-html
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code> tests/test_api_client.py ‚úì‚úì‚úì‚úì‚úì‚úì‚úì‚úì                                     100% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
gw0 [8] / gw1 [8] / gw2 [8] / gw3 [8]

Results (1.52s):
       8 passed

Generated html report: file:///home/user/project/report.html
</code></pre></div>

<p><strong>What we achieved</strong>:</p>
<ol>
<li><strong>pytest-sugar</strong>: Beautiful, real-time visual feedback during test execution</li>
<li><strong>pytest-xdist</strong>: Parallel execution reduced time from 4.23s to 1.52s (64% faster)</li>
<li><strong>pytest-html</strong>: Persistent report for documentation and CI/CD integration</li>
</ol>
<h3 id="configuration-for-consistent-usage">Configuration for Consistent Usage</h3>
<p>Instead of typing flags every time, configure plugins in <code>pytest.ini</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="na">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>
<span class="w">    </span><span class="na">-n auto</span>
<span class="w">    </span><span class="na">--html</span><span class="o">=</span><span class="s">reports/test_report.html</span>
<span class="w">    </span><span class="na">--self-contained-html</span>
<span class="w">    </span><span class="na">-v</span>

<span class="c1"># Disable plugins when needed</span>
<span class="c1"># addopts = --no-sugar -n 0</span>
</code></pre></div>

<p>Now simply run:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_client.py
</code></pre></div>

<p>All configured options apply automatically.</p>
<h2 id="decision-framework-which-plugins-when">Decision Framework: Which Plugins When?</h2>
<table>
<thead>
<tr>
<th>Plugin</th>
<th>Primary Benefit</th>
<th>Primary Cost</th>
<th>Best For</th>
<th>Avoid When</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>pytest-sugar</strong></td>
<td>Visual clarity</td>
<td>Minimal overhead</td>
<td>Development, frequent test runs</td>
<td>CI systems with poor terminal support</td>
</tr>
<tr>
<td><strong>pytest-xdist</strong></td>
<td>Speed (parallelization)</td>
<td>Requires test isolation</td>
<td>Large suites, CI/CD</td>
<td>Tests with shared state, debugging</td>
</tr>
<tr>
<td><strong>pytest-html</strong></td>
<td>Persistent reports</td>
<td>Disk space, slight overhead</td>
<td>CI/CD, documentation</td>
<td>Local development (unless needed)</td>
</tr>
</tbody>
</table>
<h3 id="combining-plugins-compatibility-matrix">Combining Plugins: Compatibility Matrix</h3>
<table>
<thead>
<tr>
<th>Combination</th>
<th>Compatible?</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>sugar + xdist</td>
<td>‚úÖ Yes</td>
<td>Sugar shows parallel progress</td>
</tr>
<tr>
<td>sugar + html</td>
<td>‚úÖ Yes</td>
<td>Sugar affects console, html affects file</td>
</tr>
<tr>
<td>xdist + html</td>
<td>‚úÖ Yes</td>
<td>Report includes parallel execution details</td>
</tr>
<tr>
<td>All three</td>
<td>‚úÖ Yes</td>
<td>Recommended for CI/CD pipelines</td>
</tr>
</tbody>
</table>
<h2 id="the-journey-from-basic-tests-to-optimized-workflow">The Journey: From Basic Tests to Optimized Workflow</h2>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Problem</th>
<th>Plugin Solution</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Basic tests work but output is plain</td>
<td>None</td>
<td>Functional but not optimal</td>
</tr>
<tr>
<td>1</td>
<td>Hard to scan test results quickly</td>
<td>pytest-sugar</td>
<td>Visual feedback, easier scanning</td>
</tr>
<tr>
<td>2</td>
<td>Test suite takes too long</td>
<td>pytest-xdist</td>
<td>64% faster execution</td>
</tr>
<tr>
<td>3</td>
<td>No persistent record of results</td>
<td>pytest-html</td>
<td>Archived reports for CI/CD</td>
</tr>
<tr>
<td>4</td>
<td>Want all benefits together</td>
<td>All three + config</td>
<td>Optimal development workflow</td>
</tr>
</tbody>
</table>
<h3 id="lessons-learned">Lessons Learned</h3>
<ol>
<li><strong>Plugins solve real problems</strong>: Each plugin addresses a specific pain point in the testing workflow</li>
<li><strong>Composition over configuration</strong>: Plugins work together without conflicts</li>
<li><strong>Configuration matters</strong>: Use <code>pytest.ini</code> to make plugin usage consistent across the team</li>
<li><strong>Test isolation is critical</strong>: Parallel execution exposes hidden dependencies between tests</li>
<li><strong>Choose based on context</strong>: Development workflows differ from CI/CD requirements</li>
</ol>
<h2 id="installing-and-configuring-plugins">Installing and Configuring Plugins</h2>
<h2 id="installing-and-configuring-plugins_1">Installing and Configuring Plugins</h2>
<p>Now that we've seen what plugins can do, let's understand how to install, configure, and manage them systematically. Plugin management becomes critical as your test suite grows and your team adopts more tools.</p>
<h3 id="the-plugin-discovery-mechanism">The Plugin Discovery Mechanism</h3>
<p>Before we install plugins, let's understand how pytest finds and loads them. This knowledge will help you troubleshoot plugin issues and understand configuration options.</p>
<h4 id="how-pytest-discovers-plugins">How Pytest Discovers Plugins</h4>
<p>Pytest uses Python's entry point system to discover plugins automatically. When you install a plugin with pip, it registers itself with pytest.</p>
<p>Let's see which plugins are currently active:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>--version
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>pytest 7.4.3
plugins: sugar-0.9.7, xdist-3.5.0, html-4.1.1
</code></pre></div>

<p>This shows pytest found three plugins. Let's get more detail:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>--trace-config
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">============================</span><span class="p">=</span><span class="w"> </span><span class="nx">test</span><span class="w"> </span><span class="nx">session</span><span class="w"> </span><span class="nx">starts</span><span class="w"> </span><span class="o">==============================</span>
<span class="nx">active</span><span class="w"> </span><span class="nx">plugins</span><span class="p">:</span>
<span class="w">    </span><span class="nx">sugar</span><span class="o">-</span><span class="m m-Double">0.9.7</span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="nx">path</span><span class="o">/</span><span class="nx">to</span><span class="o">/</span><span class="nx">venv</span><span class="o">/</span><span class="nx">lib</span><span class="o">/</span><span class="nx">python3</span><span class="m m-Double">.11</span><span class="o">/</span><span class="nx">site</span><span class="o">-</span><span class="nx">packages</span><span class="o">/</span><span class="nx">pytest_sugar</span><span class="p">.</span><span class="nx">py</span>
<span class="w">    </span><span class="nx">xdist</span><span class="o">-</span><span class="m m-Double">3.5.0</span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="nx">path</span><span class="o">/</span><span class="nx">to</span><span class="o">/</span><span class="nx">venv</span><span class="o">/</span><span class="nx">lib</span><span class="o">/</span><span class="nx">python3</span><span class="m m-Double">.11</span><span class="o">/</span><span class="nx">site</span><span class="o">-</span><span class="nx">packages</span><span class="o">/</span><span class="nx">xdist</span><span class="o">/</span><span class="nx">plugin</span><span class="p">.</span><span class="nx">py</span>
<span class="w">    </span><span class="nx">html</span><span class="o">-</span><span class="m m-Double">4.1.1</span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="nx">path</span><span class="o">/</span><span class="nx">to</span><span class="o">/</span><span class="nx">venv</span><span class="o">/</span><span class="nx">lib</span><span class="o">/</span><span class="nx">python3</span><span class="m m-Double">.11</span><span class="o">/</span><span class="nx">site</span><span class="o">-</span><span class="nx">packages</span><span class="o">/</span><span class="nx">pytest_html</span><span class="o">/</span><span class="nx">plugin</span><span class="p">.</span><span class="nx">py</span>
</code></pre></div>

<p><strong>What this tells us</strong>:</p>
<ol>
<li><strong>Plugin names</strong>: The registered name (e.g., <code>sugar-0.9.7</code>)</li>
<li><strong>Plugin location</strong>: Where the plugin code lives</li>
<li><strong>Load order</strong>: Plugins load in the order shown</li>
</ol>
<h3 id="installation-methods">Installation Methods</h3>
<h4 id="method-1-direct-installation-development">Method 1: Direct Installation (Development)</h4>
<p>For local development, install plugins directly:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Install a single plugin</span>
$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>pytest-cov

<span class="c1"># Install multiple plugins</span>
$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>pytest-cov<span class="w"> </span>pytest-timeout<span class="w"> </span>pytest-mock

<span class="c1"># Install with specific version</span>
$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>pytest-cov<span class="o">==</span><span class="m">4</span>.1.0
</code></pre></div>

<h4 id="method-2-requirements-file-team-projects">Method 2: Requirements File (Team Projects)</h4>
<p>For team projects, maintain a <code>requirements-dev.txt</code>:</p>
<div class="codehilite"><pre><span></span><code># requirements-dev.txt
pytest==7.4.3
pytest-cov==4.1.0
pytest-xdist==3.5.0
pytest-html==4.1.1
pytest-sugar==0.9.7
pytest-timeout==2.2.0
pytest-mock==3.12.0
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements-dev.txt
</code></pre></div>

<h4 id="method-3-pyprojecttoml-modern-projects">Method 3: pyproject.toml (Modern Projects)</h4>
<p>For modern Python projects using <code>pyproject.toml</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pyproject.toml</span>
<span class="k">[project.optional-dependencies]</span>
<span class="n">dev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;pytest&gt;=7.4.3&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;pytest-cov&gt;=4.1.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;pytest-xdist&gt;=3.5.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;pytest-html&gt;=4.1.1&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;pytest-sugar&gt;=0.9.7&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;pytest-timeout&gt;=2.2.0&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;pytest-mock&gt;=3.12.0&quot;</span><span class="p">,</span>
<span class="p">]</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span><span class="s2">&quot;.[dev]&quot;</span>
</code></pre></div>

<h3 id="configuration-hierarchy">Configuration Hierarchy</h3>
<p>Pytest plugins can be configured in multiple places. Understanding the hierarchy helps you control plugin behavior effectively.</p>
<h4 id="configuration-priority-highest-to-lowest">Configuration Priority (Highest to Lowest)</h4>
<ol>
<li><strong>Command-line flags</strong>: Override everything</li>
<li><strong>Environment variables</strong>: Override config files</li>
<li><strong>pytest.ini / pyproject.toml</strong>: Project-level defaults</li>
<li><strong>conftest.py</strong>: Programmatic configuration</li>
<li><strong>Plugin defaults</strong>: Built-in plugin settings</li>
</ol>
<p>Let's demonstrate this hierarchy with a concrete example using pytest-timeout:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>pytest-timeout
</code></pre></div>

<h4 id="level-1-plugin-defaults">Level 1: Plugin Defaults</h4>
<p>Without any configuration, pytest-timeout uses its defaults (no timeout):</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_timeout.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_slow_operation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that takes 2 seconds.&quot;&quot;&quot;</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">assert</span> <span class="kc">True</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_timeout.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_timeout.py::test_slow_operation PASSED                        [100%]

============================== 1 passed in 2.01s ===============================
</code></pre></div>

<p>Test passes because no timeout is configured.</p>
<h4 id="level-2-configuration-file">Level 2: Configuration File</h4>
<p>Add timeout to <code>pytest.ini</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="na">timeout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_timeout.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_timeout.py::test_slow_operation FAILED                        [100%]

=================================== FAILURES ===================================
__________________________ test_slow_operation _________________________________
E   Failed: Timeout &gt;1.0s

tests/test_timeout.py:5: Failed
============================== 1 failed in 1.23s ===============================
</code></pre></div>

<p><strong>Diagnostic Analysis</strong>:</p>
<ol>
<li><strong>Summary line</strong>: <code>FAILED</code> with <code>Timeout &gt;1.0s</code> message</li>
<li><strong>Root cause</strong>: Test exceeded the 1-second timeout configured in pytest.ini</li>
<li><strong>What this tells us</strong>: Configuration file settings apply to all tests</li>
</ol>
<h4 id="level-3-environment-variables">Level 3: Environment Variables</h4>
<p>Override the config file with an environment variable:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span><span class="nv">PYTEST_TIMEOUT</span><span class="o">=</span><span class="m">3</span><span class="w"> </span>pytest<span class="w"> </span>tests/test_timeout.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_timeout.py::test_slow_operation PASSED                        [100%]

============================== 1 passed in 2.01s ===============================
</code></pre></div>

<p>Test passes because environment variable (3 seconds) overrides pytest.ini (1 second).</p>
<h4 id="level-4-command-line-flags">Level 4: Command-Line Flags</h4>
<p>Override everything with a command-line flag:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span><span class="nv">PYTEST_TIMEOUT</span><span class="o">=</span><span class="m">3</span><span class="w"> </span>pytest<span class="w"> </span>tests/test_timeout.py<span class="w"> </span>-v<span class="w"> </span>--timeout<span class="o">=</span><span class="m">0</span>.5
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_timeout.py::test_slow_operation FAILED                        [100%]

=================================== FAILURES ===================================
__________________________ test_slow_operation _________________________________
E   Failed: Timeout &gt;0.5s

tests/test_timeout.py:5: Failed
============================== 1 failed in 0.73s ===============================
</code></pre></div>

<p>Command-line flag (0.5 seconds) overrides both environment variable and config file.</p>
<h3 id="common-configuration-patterns">Common Configuration Patterns</h3>
<h4 id="pattern-1-different-settings-for-different-environments">Pattern 1: Different Settings for Different Environments</h4>
<p>Use environment variables to adjust plugin behavior:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Development: No timeout, pretty output</span>
$<span class="w"> </span>pytest

<span class="c1"># CI: Strict timeout, parallel execution, HTML report</span>
$<span class="w"> </span><span class="nv">PYTEST_TIMEOUT</span><span class="o">=</span><span class="m">30</span><span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span>auto<span class="w"> </span>--html<span class="o">=</span>report.html
</code></pre></div>

<h4 id="pattern-2-per-test-configuration">Pattern 2: Per-Test Configuration</h4>
<p>Some plugins support per-test configuration via markers:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_timeout.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">timeout</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_slow_operation_allowed</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This test gets 5 seconds instead of the default.&quot;&quot;&quot;</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">timeout</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Disable timeout for this test</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_very_slow_operation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This test has no timeout.&quot;&quot;&quot;</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">assert</span> <span class="kc">True</span>
</code></pre></div>

<h4 id="pattern-3-conditional-plugin-activation">Pattern 3: Conditional Plugin Activation</h4>
<p>Disable plugins conditionally:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="c1"># Enable plugins by default</span>
<span class="na">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">-n auto --html=report.html</span>

<span class="c1"># Override in specific scenarios</span>
<span class="c1"># pytest -p no:xdist  # Disable xdist</span>
<span class="c1"># pytest -p no:sugar  # Disable sugar</span>
</code></pre></div>

<h3 id="plugin-configuration-reference">Plugin Configuration Reference</h3>
<p>Let's configure the plugins we've used so far comprehensively:</p>
<h4 id="pytest-xdist-configuration">pytest-xdist Configuration</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="c1"># Number of workers (auto, logical, or specific number)</span>
<span class="na">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">-n auto</span>

<span class="c1"># Distribution strategy</span>
<span class="c1"># - load: distribute by file (default)</span>
<span class="c1"># - loadscope: distribute by test scope (class, module)</span>
<span class="c1"># - loadfile: distribute by file, but keep file tests together</span>
<span class="c1"># - no: disable distribution</span>
<span class="c1"># Example: --dist=loadscope</span>
</code></pre></div>

<p><strong>Advanced xdist options</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Run tests on remote machines</span>
$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span><span class="m">4</span><span class="w"> </span>--tx<span class="w"> </span><span class="nv">ssh</span><span class="o">=</span>user@host1<span class="w"> </span>--tx<span class="w"> </span><span class="nv">ssh</span><span class="o">=</span>user@host2

<span class="c1"># Control test distribution</span>
$<span class="w"> </span>pytest<span class="w"> </span>--dist<span class="o">=</span>loadscope<span class="w">  </span><span class="c1"># Keep test classes together</span>

<span class="c1"># Maximum workers</span>
$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span><span class="m">8</span><span class="w"> </span>--maxprocesses<span class="o">=</span><span class="m">4</span><span class="w">  </span><span class="c1"># Use max 4 processes even if more CPUs</span>
</code></pre></div>

<h4 id="pytest-html-configuration">pytest-html Configuration</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="c1"># Basic HTML report</span>
<span class="na">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">--html=reports/report.html --self-contained-html</span>

<span class="c1"># Additional options</span>
<span class="c1"># --html=report.html              # Report location</span>
<span class="c1"># --self-contained-html           # Embed assets</span>
<span class="c1"># --css=custom.css                # Custom styling</span>
<span class="c1"># --html-report=report.html       # Alternative syntax</span>
</code></pre></div>

<p><strong>Customizing HTML reports</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">datetime</span><span class="w"> </span><span class="kn">import</span> <span class="n">datetime</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom information to HTML report.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="c1"># Add custom metadata</span>
    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span><span class="p">:</span>
        <span class="n">report</span><span class="o">.</span><span class="n">user_properties</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;timestamp&quot;</span><span class="p">,</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">isoformat</span><span class="p">()))</span>
        <span class="n">report</span><span class="o">.</span><span class="n">user_properties</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;test_id&quot;</span><span class="p">,</span> <span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_html_report_title</span><span class="p">(</span><span class="n">report</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Customize report title.&quot;&quot;&quot;</span>
    <span class="n">report</span><span class="o">.</span><span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;API Client Test Report&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom metadata to report.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="s2">&quot;Project&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;API Client&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">_metadata</span><span class="p">[</span><span class="s2">&quot;Tester&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;QA Team&quot;</span>
</code></pre></div>

<h4 id="pytest-sugar-configuration">pytest-sugar Configuration</h4>
<p>pytest-sugar has minimal configuration but can be disabled:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Disable sugar temporarily</span>
$<span class="w"> </span>pytest<span class="w"> </span>--no-sugar

<span class="c1"># Disable via plugin system</span>
$<span class="w"> </span>pytest<span class="w"> </span>-p<span class="w"> </span>no:sugar
</code></pre></div>

<h3 id="managing-plugin-conflicts">Managing Plugin Conflicts</h3>
<p>Sometimes plugins conflict with each other or with your test code. Let's see how to diagnose and resolve conflicts.</p>
<h4 id="conflict-example-output-capture">Conflict Example: Output Capture</h4>
<p>Let's create a scenario where plugins interfere with each other:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_output.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_with_print</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that prints output.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Debug information&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;More debug information&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="kc">True</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_output.py<span class="w"> </span>-v<span class="w"> </span>-s
</code></pre></div>

<p><strong>Output with pytest-sugar</strong>:</p>
<div class="codehilite"><pre><span></span><code> tests/test_output.py ‚úì                                                 100% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Debug information
More debug information

Results (0.01s):
       1 passed
</code></pre></div>

<p><strong>Output without pytest-sugar</strong>:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_output.py<span class="w"> </span>-v<span class="w"> </span>-s<span class="w"> </span>-p<span class="w"> </span>no:sugar
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_output.py::test_with_print Debug information
More debug information
PASSED

============================== 1 passed in 0.01s ===============================
</code></pre></div>

<p><strong>Diagnostic Analysis</strong>:</p>
<ol>
<li><strong>With sugar</strong>: Output appears after the progress bar</li>
<li><strong>Without sugar</strong>: Output appears inline with test execution</li>
<li><strong>Root cause</strong>: pytest-sugar buffers output for cleaner display</li>
<li><strong>Solution</strong>: Use <code>-p no:sugar</code> when you need immediate output visibility</li>
</ol>
<h4 id="conflict-resolution-strategies">Conflict Resolution Strategies</h4>
<p><strong>Strategy 1: Selective Plugin Disabling</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Disable specific plugin for one run</span>
$<span class="w"> </span>pytest<span class="w"> </span>-p<span class="w"> </span>no:sugar

<span class="c1"># Disable multiple plugins</span>
$<span class="w"> </span>pytest<span class="w"> </span>-p<span class="w"> </span>no:sugar<span class="w"> </span>-p<span class="w"> </span>no:xdist
</code></pre></div>

<p><strong>Strategy 2: Plugin Load Order Control</strong></p>
<p>Some plugins must load before others. Control this via <code>conftest.py</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="n">pytest_plugins</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;pytest_html&quot;</span><span class="p">,</span>  <span class="c1"># Load first</span>
    <span class="s2">&quot;pytest_sugar&quot;</span><span class="p">,</span> <span class="c1"># Load second</span>
<span class="p">]</span>
</code></pre></div>

<p><strong>Strategy 3: Configuration Isolation</strong></p>
<p>Use different config files for different scenarios:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest-dev.ini (for development)</span>
<span class="k">[pytest]</span>
<span class="na">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">-v --no-sugar</span>

<span class="c1"># pytest-ci.ini (for CI/CD)</span>
<span class="k">[pytest]</span>
<span class="na">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">-n auto --html=report.html --self-contained-html</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># Use specific config</span>
$<span class="w"> </span>pytest<span class="w"> </span>-c<span class="w"> </span>pytest-dev.ini
$<span class="w"> </span>pytest<span class="w"> </span>-c<span class="w"> </span>pytest-ci.ini
</code></pre></div>

<h3 id="verifying-plugin-configuration">Verifying Plugin Configuration</h3>
<p>After configuring plugins, verify they're working as expected:</p>
<h4 id="verification-checklist">Verification Checklist</h4>
<p><strong>1. Check plugin loading</strong>:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>--version
<span class="c1"># Should list all installed plugins</span>

$<span class="w"> </span>pytest<span class="w"> </span>--trace-config
<span class="c1"># Shows detailed plugin information</span>
</code></pre></div>

<p><strong>2. Test plugin functionality</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Test xdist</span>
$<span class="w"> </span>pytest<span class="w"> </span>--collect-only<span class="w"> </span>-n<span class="w"> </span><span class="m">2</span>
<span class="c1"># Should show worker distribution</span>

<span class="c1"># Test html</span>
$<span class="w"> </span>pytest<span class="w"> </span>--collect-only<span class="w"> </span>--html<span class="o">=</span>test.html
<span class="c1"># Should create HTML file</span>

<span class="c1"># Test sugar</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/<span class="w"> </span>-v
<span class="c1"># Should show progress bar</span>
</code></pre></div>

<p><strong>3. Verify configuration application</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Show effective configuration</span>
$<span class="w"> </span>pytest<span class="w"> </span>--showlocals<span class="w"> </span>--tb<span class="o">=</span>short<span class="w"> </span>--help
<span class="c1"># Lists all active options including plugin options</span>
</code></pre></div>

<h3 id="complete-configuration-example">Complete Configuration Example</h3>
<p>Here's a production-ready configuration combining all best practices:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pyproject.toml</span>
<span class="k">[tool.pytest.ini_options]</span>
<span class="n">minversion</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;7.0&quot;</span>
<span class="n">testpaths</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;tests&quot;</span><span class="p">]</span>
<span class="n">python_files</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;test_*.py&quot;</span><span class="p">]</span>
<span class="n">python_classes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;Test*&quot;</span><span class="p">]</span>
<span class="n">python_functions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;test_*&quot;</span><span class="p">]</span>

<span class="c1"># Plugin configuration</span>
<span class="n">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    -v</span>
<span class="s2">    --strict-markers</span>
<span class="s2">    --tb=short</span>
<span class="s2">    --cov=src</span>
<span class="s2">    --cov-report=html</span>
<span class="s2">    --cov-report=term-missing</span>
<span class="s2">    --html=reports/test_report.html</span>
<span class="s2">    --self-contained-html</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Markers</span>
<span class="n">markers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;slow: marks tests as slow (deselect with &#39;-m </span><span class="se">\&quot;</span><span class="s2">not slow</span><span class="se">\&quot;</span><span class="s2">&#39;)&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;integration: marks tests as integration tests&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;unit: marks tests as unit tests&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Timeout configuration</span>
<span class="n">timeout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">300</span>
<span class="n">timeout_method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;thread&quot;</span>

<span class="c1"># Coverage configuration</span>
<span class="k">[tool.coverage.run]</span>
<span class="n">source</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;src&quot;</span><span class="p">]</span>
<span class="n">omit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;*/tests/*&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;*/test_*.py&quot;</span><span class="p">]</span>

<span class="k">[tool.coverage.report]</span>
<span class="n">exclude_lines</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;pragma: no cover&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;def __repr__&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;raise AssertionError&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;raise NotImplementedError&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;if __name__ == .__main__.:&quot;</span><span class="p">,</span>
<span class="p">]</span>
</code></pre></div>

<p><strong>Environment-specific overrides</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Development: Fast feedback, no coverage</span>
$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span>auto<span class="w"> </span>-p<span class="w"> </span>no:cov

<span class="c1"># CI: Full coverage, parallel, reports</span>
$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span>auto<span class="w"> </span>--cov<span class="w"> </span>--html<span class="o">=</span>report.html

<span class="c1"># Debugging: Sequential, verbose, no sugar</span>
$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span><span class="m">0</span><span class="w"> </span>-vv<span class="w"> </span>-p<span class="w"> </span>no:sugar<span class="w"> </span>--pdb
</code></pre></div>

<h2 id="decision-framework-configuration-strategies">Decision Framework: Configuration Strategies</h2>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Configuration Method</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Local development</strong></td>
<td>pytest.ini with minimal options</td>
<td>Fast feedback, easy to override</td>
</tr>
<tr>
<td><strong>Team project</strong></td>
<td>pyproject.toml with comprehensive options</td>
<td>Single source of truth, version controlled</td>
</tr>
<tr>
<td><strong>CI/CD pipeline</strong></td>
<td>Command-line flags + environment variables</td>
<td>Flexibility, environment-specific settings</td>
</tr>
<tr>
<td><strong>Multiple environments</strong></td>
<td>Separate config files (pytest-dev.ini, pytest-ci.ini)</td>
<td>Clear separation, explicit control</td>
</tr>
<tr>
<td><strong>Plugin conflicts</strong></td>
<td>Selective disabling with -p no:plugin</td>
<td>Temporary fixes, debugging</td>
</tr>
</tbody>
</table>
<h3 id="lessons-learned_1">Lessons Learned</h3>
<ol>
<li><strong>Configuration hierarchy matters</strong>: Understand which settings override others</li>
<li><strong>Start minimal, add as needed</strong>: Don't configure everything upfront</li>
<li><strong>Document your choices</strong>: Explain why specific plugins are configured certain ways</li>
<li><strong>Test your configuration</strong>: Verify plugins work as expected after configuration changes</li>
<li><strong>Environment-specific settings</strong>: Use different configurations for development vs. CI/CD</li>
<li><strong>Plugin conflicts are solvable</strong>: Use selective disabling and load order control</li>
</ol>
<h2 id="creating-custom-plugins">Creating Custom Plugins</h2>
<h2 id="creating-custom-plugins_1">Creating Custom Plugins</h2>
<p>Now that we understand how to use and configure existing plugins, let's learn how to create our own. Custom plugins let you extend pytest with project-specific functionality that doesn't exist in the ecosystem.</p>
<h3 id="when-to-create-a-custom-plugin">When to Create a Custom Plugin</h3>
<p>Before writing a plugin, ask yourself:</p>
<ol>
<li><strong>Is this functionality reusable across multiple test files?</strong> If yes, consider a plugin.</li>
<li><strong>Does this solve a problem specific to your project?</strong> If yes, a local plugin might be appropriate.</li>
<li><strong>Could this benefit the wider community?</strong> If yes, consider publishing it.</li>
</ol>
<p>Let's build a custom plugin that solves a real problem: <strong>tracking test execution time and automatically marking slow tests</strong>.</p>
<h3 id="phase-1-the-problem-identifying-slow-tests">Phase 1: The Problem - Identifying Slow Tests</h3>
<p>Our API client test suite has grown, and some tests are noticeably slower than others. We want to:</p>
<ol>
<li>Automatically identify tests that exceed a time threshold</li>
<li>Mark them with a custom marker for easy filtering</li>
<li>Generate a report of slow tests</li>
</ol>
<p>Let's start with our current test suite:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_api_performance.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">unittest.mock</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mock</span><span class="p">,</span> <span class="n">patch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.api_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">APIClient</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">api_client</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">APIClient</span><span class="p">(</span><span class="s2">&quot;https://api.example.com&quot;</span><span class="p">,</span> <span class="s2">&quot;test-key&quot;</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mock_response</span><span class="p">():</span>
    <span class="n">mock</span> <span class="o">=</span> <span class="n">Mock</span><span class="p">()</span>
    <span class="n">mock</span><span class="o">.</span><span class="n">status_code</span> <span class="o">=</span> <span class="mi">200</span>
    <span class="n">mock</span><span class="o">.</span><span class="n">json</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="s2">&quot;test&quot;</span><span class="p">}</span>
    <span class="n">mock</span><span class="o">.</span><span class="n">raise_for_status</span> <span class="o">=</span> <span class="n">Mock</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mock</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_fast_operation</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fast test - completes in &lt;0.1s.&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">get_user</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_medium_operation</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Medium test - completes in ~0.3s.&quot;&quot;&quot;</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">get_user</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_slow_operation</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Slow test - completes in ~1s.&quot;&quot;&quot;</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">get_user</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_very_slow_operation</span><span class="p">(</span><span class="n">api_client</span><span class="p">,</span> <span class="n">mock_response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Very slow test - completes in ~2s.&quot;&quot;&quot;</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">patch</span><span class="o">.</span><span class="n">object</span><span class="p">(</span><span class="n">api_client</span><span class="o">.</span><span class="n">session</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">return_value</span><span class="o">=</span><span class="n">mock_response</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">api_client</span><span class="o">.</span><span class="n">get_user</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>--durations<span class="o">=</span><span class="m">0</span>
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_api_performance.py::test_fast_operation PASSED               [ 25%]
tests/test_api_performance.py::test_medium_operation PASSED             [ 50%]
tests/test_api_performance.py::test_slow_operation PASSED               [ 75%]
tests/test_api_performance.py::test_very_slow_operation PASSED          [100%]

============================== slowest durations ===============================
2.00s call     tests/test_api_performance.py::test_very_slow_operation
1.00s call     tests/test_api_performance.py::test_slow_operation
0.30s call     tests/test_api_performance.py::test_medium_operation
0.01s call     tests/test_api_performance.py::test_fast_operation

============================== 4 passed in 3.32s ===============================
</code></pre></div>

<p><strong>Current limitation</strong>: We can see slow tests with <code>--durations</code>, but:</p>
<ol>
<li>We have to remember to use the flag</li>
<li>We can't filter slow tests with <code>-m</code></li>
<li>We have no automated way to track slow tests over time</li>
<li>The threshold is not configurable</li>
</ol>
<p><strong>What we need</strong>: A plugin that automatically marks tests exceeding a threshold and provides filtering capabilities.</p>
<h3 id="phase-2-plugin-structure-the-basics">Phase 2: Plugin Structure - The Basics</h3>
<p>A pytest plugin is simply a Python module that defines hook functions. Let's create our first plugin:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py (local plugin)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register custom markers.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;slow: marks tests as slow (automatically applied based on duration)&quot;</span>
    <span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook that runs after each test phase.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="c1"># Only process the actual test call (not setup/teardown)</span>
    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[DEBUG] Test </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> took </span><span class="si">{</span><span class="n">report</span><span class="o">.</span><span class="n">duration</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py::test_fast_operation<span class="w"> </span>-v<span class="w"> </span>-s
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="n">tests</span><span class="o">/</span><span class="n">test_api_performance</span><span class="p">.</span><span class="nl">py</span><span class="p">:</span><span class="err">:</span><span class="n">test_fast_operation</span><span class="w"> </span>
<span class="o">[</span><span class="n">DEBUG</span><span class="o">]</span><span class="w"> </span><span class="n">Test</span><span class="w"> </span><span class="n">test_fast_operation</span><span class="w"> </span><span class="n">took</span><span class="w"> </span><span class="mf">0.01</span><span class="n">s</span>
<span class="n">PASSED</span>

<span class="o">==============================</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">passed</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="mf">0.02</span><span class="n">s</span><span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p><strong>What happened?</strong></p>
<ol>
<li><strong>pytest_configure</strong>: Registered our custom <code>slow</code> marker</li>
<li><strong>pytest_runtest_makereport</strong>: Hook that runs after each test phase</li>
<li><strong>hookwrapper=True</strong>: Allows us to wrap around the normal hook execution</li>
<li><strong>report.when == "call"</strong>: Filters to only the actual test execution (not setup/teardown)</li>
</ol>
<h3 id="diagnostic-analysis-understanding-hook-execution">Diagnostic Analysis: Understanding Hook Execution</h3>
<p>Let's add more instrumentation to understand the hook lifecycle:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook that runs after each test phase.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[HOOK] pytest_runtest_makereport called&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  item.name: </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  call.when: </span><span class="si">{</span><span class="n">call</span><span class="o">.</span><span class="n">when</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  report.when: </span><span class="si">{</span><span class="n">report</span><span class="o">.</span><span class="n">when</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  report.duration: </span><span class="si">{</span><span class="n">report</span><span class="o">.</span><span class="n">duration</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  report.outcome: </span><span class="si">{</span><span class="n">report</span><span class="o">.</span><span class="n">outcome</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py::test_fast_operation<span class="w"> </span>-v<span class="w"> </span>-s
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_runtest_makereport</span><span class="w"> </span><span class="k">called</span>
<span class="w">  </span><span class="n">item</span><span class="p">.</span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">test_fast_operation</span>
<span class="w">  </span><span class="k">call</span><span class="p">.</span><span class="k">when</span><span class="err">:</span><span class="w"> </span><span class="n">setup</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="k">when</span><span class="err">:</span><span class="w"> </span><span class="n">setup</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="nl">duration</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0001</span><span class="n">s</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="nl">outcome</span><span class="p">:</span><span class="w"> </span><span class="n">passed</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_runtest_makereport</span><span class="w"> </span><span class="k">called</span>
<span class="w">  </span><span class="n">item</span><span class="p">.</span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">test_fast_operation</span>
<span class="w">  </span><span class="k">call</span><span class="p">.</span><span class="k">when</span><span class="err">:</span><span class="w"> </span><span class="k">call</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="k">when</span><span class="err">:</span><span class="w"> </span><span class="k">call</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="nl">duration</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0089</span><span class="n">s</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="nl">outcome</span><span class="p">:</span><span class="w"> </span><span class="n">passed</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_runtest_makereport</span><span class="w"> </span><span class="k">called</span>
<span class="w">  </span><span class="n">item</span><span class="p">.</span><span class="nl">name</span><span class="p">:</span><span class="w"> </span><span class="n">test_fast_operation</span>
<span class="w">  </span><span class="k">call</span><span class="p">.</span><span class="k">when</span><span class="err">:</span><span class="w"> </span><span class="n">teardown</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="k">when</span><span class="err">:</span><span class="w"> </span><span class="n">teardown</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="nl">duration</span><span class="p">:</span><span class="w"> </span><span class="mf">0.0001</span><span class="n">s</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="nl">outcome</span><span class="p">:</span><span class="w"> </span><span class="n">passed</span>

<span class="n">PASSED</span>

<span class="o">==============================</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">passed</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="mf">0.01</span><span class="n">s</span><span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p><strong>Key insights</strong>:</p>
<ol>
<li><strong>Three phases</strong>: Hook runs three times per test (setup, call, teardown)</li>
<li><strong>Duration tracking</strong>: Each phase has its own duration</li>
<li><strong>Outcome tracking</strong>: Each phase can pass or fail independently</li>
<li><strong>call phase</strong>: This is where the actual test runs‚Äîwhat we want to measure</li>
</ol>
<h3 id="phase-3-implementing-automatic-slow-test-marking">Phase 3: Implementing Automatic Slow Test Marking</h3>
<p>Now let's implement the core functionality: automatically mark tests that exceed a threshold:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="c1"># Configuration</span>
<span class="n">SLOW_TEST_THRESHOLD</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># seconds</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register custom markers.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;slow: marks tests as slow (automatically applied based on duration)&quot;</span>
    <span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Automatically mark slow tests.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="c1"># Only process the actual test call</span>
    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span> <span class="ow">and</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span> <span class="o">&gt;</span> <span class="n">SLOW_TEST_THRESHOLD</span><span class="p">:</span>
        <span class="c1"># Add the slow marker to the test item</span>
        <span class="n">item</span><span class="o">.</span><span class="n">add_marker</span><span class="p">(</span><span class="n">pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">slow</span><span class="p">)</span>

        <span class="c1"># Store duration for later reporting</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s2">&quot;slow_duration&quot;</span><span class="p">):</span>
            <span class="n">item</span><span class="o">.</span><span class="n">slow_duration</span> <span class="o">=</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>-m<span class="w"> </span>slow
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_api_performance.py::test_slow_operation PASSED               [ 50%]
tests/test_api_performance.py::test_very_slow_operation PASSED          [100%]

============================== 2 passed in 3.01s ===============================
</code></pre></div>

<p><strong>What we achieved</strong>:</p>
<ol>
<li><strong>Automatic marking</strong>: Tests exceeding 0.5s were automatically marked as <code>slow</code></li>
<li><strong>Filtering works</strong>: <code>-m slow</code> successfully filtered to only slow tests</li>
<li><strong>No manual marking needed</strong>: Developers don't need to remember to add <code>@pytest.mark.slow</code></li>
</ol>
<h3 id="phase-4-making-configuration-flexible">Phase 4: Making Configuration Flexible</h3>
<p>Hardcoding the threshold isn't ideal. Let's make it configurable:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom command-line options.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addoption</span><span class="p">(</span>
        <span class="s2">&quot;--slow-threshold&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Threshold in seconds for marking tests as slow (default: 0.5)&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register custom markers and store configuration.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;slow: marks tests as slow (automatically applied based on duration)&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Store threshold for use in hooks</span>
    <span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getoption</span><span class="p">(</span><span class="s2">&quot;--slow-threshold&quot;</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Automatically mark slow tests.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="c1"># Get threshold from config</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span>

    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span> <span class="ow">and</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="n">item</span><span class="o">.</span><span class="n">add_marker</span><span class="p">(</span><span class="n">pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">slow</span><span class="p">)</span>
        <span class="n">item</span><span class="o">.</span><span class="n">slow_duration</span> <span class="o">=</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span>
</code></pre></div>

<p>Now we can configure the threshold:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Use default threshold (0.5s)</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>-m<span class="w"> </span>slow

<span class="c1"># Use custom threshold (1.0s)</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>-m<span class="w"> </span>slow<span class="w"> </span>--slow-threshold<span class="o">=</span><span class="m">1</span>.0
</code></pre></div>

<p><strong>Output with --slow-threshold=1.0</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_api_performance.py::test_very_slow_operation PASSED          [100%]

============================== 1 passed in 2.01s ===============================
</code></pre></div>

<p>Only the 2-second test is marked as slow now.</p>
<h3 id="phase-5-adding-a-summary-report">Phase 5: Adding a Summary Report</h3>
<p>Let's add a summary report showing all slow tests at the end:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom command-line options.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addoption</span><span class="p">(</span>
        <span class="s2">&quot;--slow-threshold&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Threshold in seconds for marking tests as slow&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addoption</span><span class="p">(</span>
        <span class="s2">&quot;--show-slow-summary&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Show summary of slow tests at the end&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register custom markers and initialize storage.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;slow: marks tests as slow (automatically applied based on duration)&quot;</span>
    <span class="p">)</span>
    <span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getoption</span><span class="p">(</span><span class="s2">&quot;--slow-threshold&quot;</span><span class="p">)</span>
    <span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Store slow test information</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Automatically mark slow tests and collect information.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="n">threshold</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span>

    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span> <span class="ow">and</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="n">item</span><span class="o">.</span><span class="n">add_marker</span><span class="p">(</span><span class="n">pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">slow</span><span class="p">)</span>

        <span class="c1"># Store slow test information</span>
        <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s2">&quot;nodeid&quot;</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">,</span>
            <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span><span class="p">,</span>
            <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">threshold</span>
        <span class="p">})</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_terminal_summary</span><span class="p">(</span><span class="n">terminalreporter</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom section to terminal summary.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">getoption</span><span class="p">(</span><span class="s2">&quot;--show-slow-summary&quot;</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">slow_tests</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">slow_tests</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Sort by duration (slowest first)</span>
    <span class="n">slow_tests</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;duration&quot;</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">section</span><span class="p">(</span><span class="s2">&quot;Slow Tests Summary&quot;</span><span class="p">)</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">slow_tests</span><span class="p">)</span><span class="si">}</span><span class="s2"> test(s) exceeding </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span><span class="si">}</span><span class="s2">s threshold:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">slow_tests</span><span class="p">:</span>
        <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;duration&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s - </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;nodeid&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>--show-slow-summary
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_api_performance.py::test_fast_operation PASSED               [ 25%]
tests/test_api_performance.py::test_medium_operation PASSED             [ 50%]
tests/test_api_performance.py::test_slow_operation PASSED               [ 75%]
tests/test_api_performance.py::test_very_slow_operation PASSED          [100%]

============================== Slow Tests Summary ==============================
Found 2 test(s) exceeding 0.5s threshold:

  2.00s - tests/test_api_performance.py::test_very_slow_operation
  1.00s - tests/test_api_performance.py::test_slow_operation

============================== 4 passed in 3.32s ===============================
</code></pre></div>

<p><strong>What we achieved</strong>:</p>
<ol>
<li><strong>Custom summary section</strong>: Added a new section to pytest's terminal output</li>
<li><strong>Sorted by duration</strong>: Slowest tests appear first</li>
<li><strong>Configurable display</strong>: Only shows when <code>--show-slow-summary</code> is used</li>
<li><strong>Actionable information</strong>: Developers can immediately see which tests to optimize</li>
</ol>
<h3 id="phase-6-adding-configuration-file-support">Phase 6: Adding Configuration File Support</h3>
<p>Let's make the plugin configurable via <code>pytest.ini</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="na">slow_threshold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0.5</span>
<span class="na">show_slow_summary</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">true</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py (updated)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom command-line options.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;slow_threshold&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;0.5&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Threshold in seconds for marking tests as slow&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;show_slow_summary&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;bool&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Show summary of slow tests at the end&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Also support command-line override</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addoption</span><span class="p">(</span>
        <span class="s2">&quot;--slow-threshold&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Override slow_threshold from config&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register custom markers and initialize storage.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;slow: marks tests as slow (automatically applied based on duration)&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Get threshold from command-line or config file</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getoption</span><span class="p">(</span><span class="s2">&quot;--slow-threshold&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">threshold</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;slow_threshold&quot;</span><span class="p">))</span>

    <span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span> <span class="o">=</span> <span class="n">threshold</span>
    <span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Automatically mark slow tests and collect information.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="n">threshold</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span>

    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span> <span class="ow">and</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="n">item</span><span class="o">.</span><span class="n">add_marker</span><span class="p">(</span><span class="n">pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">slow</span><span class="p">)</span>
        <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s2">&quot;nodeid&quot;</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">,</span>
            <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span><span class="p">,</span>
            <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">threshold</span>
        <span class="p">})</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_terminal_summary</span><span class="p">(</span><span class="n">terminalreporter</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom section to terminal summary.&quot;&quot;&quot;</span>
    <span class="n">show_summary</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;show_slow_summary&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">show_summary</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">slow_tests</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">slow_tests</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">slow_tests</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;duration&quot;</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">section</span><span class="p">(</span><span class="s2">&quot;Slow Tests Summary&quot;</span><span class="p">)</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">slow_tests</span><span class="p">)</span><span class="si">}</span><span class="s2"> test(s) exceeding </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span><span class="si">}</span><span class="s2">s threshold:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">slow_tests</span><span class="p">:</span>
        <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;duration&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s - </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;nodeid&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
</code></pre></div>

<p>Now the plugin reads configuration from <code>pytest.ini</code> by default:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Uses pytest.ini configuration</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v

<span class="c1"># Override with command-line</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>--slow-threshold<span class="o">=</span><span class="m">1</span>.0
</code></pre></div>

<h3 id="the-complete-plugin-production-ready-version">The Complete Plugin: Production-Ready Version</h3>
<p>Here's the final, production-ready version with error handling and documentation:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Pytest plugin for automatic slow test detection and reporting.</span>

<span class="sd">This plugin automatically marks tests that exceed a configurable duration</span>
<span class="sd">threshold and provides filtering and reporting capabilities.</span>

<span class="sd">Configuration:</span>
<span class="sd">    pytest.ini:</span>
<span class="sd">        slow_threshold = 0.5  # seconds</span>
<span class="sd">        show_slow_summary = true</span>

<span class="sd">Command-line:</span>
<span class="sd">    --slow-threshold=1.0      # Override threshold</span>
<span class="sd">    -m slow                   # Run only slow tests</span>
<span class="sd">    -m &quot;not slow&quot;             # Skip slow tests</span>

<span class="sd">Example:</span>
<span class="sd">    $ pytest -v --show-slow-summary</span>
<span class="sd">    $ pytest -m slow</span>
<span class="sd">    $ pytest --slow-threshold=1.0</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom command-line options and ini-file values.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;slow_threshold&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;0.5&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Threshold in seconds for marking tests as slow (default: 0.5)&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;show_slow_summary&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;bool&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Show summary of slow tests at the end (default: False)&quot;</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">addoption</span><span class="p">(</span>
        <span class="s2">&quot;--slow-threshold&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Override slow_threshold from config file&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register custom markers and initialize plugin state.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;slow: marks tests as slow (automatically applied based on duration)&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Get threshold from command-line or config file</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getoption</span><span class="p">(</span><span class="s2">&quot;--slow-threshold&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">threshold</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;slow_threshold&quot;</span><span class="p">))</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">):</span>
            <span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># Fallback to default</span>

    <span class="c1"># Validate threshold</span>
    <span class="k">if</span> <span class="n">threshold</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">pytest</span><span class="o">.</span><span class="n">UsageError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;slow_threshold must be non-negative, got </span><span class="si">{</span><span class="n">threshold</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span> <span class="o">=</span> <span class="n">threshold</span>
    <span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Automatically mark slow tests and collect information.</span>

<span class="sd">    This hook runs after each test phase (setup, call, teardown).</span>
<span class="sd">    We only process the &#39;call&#39; phase to measure actual test execution time.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="c1"># Only process the actual test call (not setup/teardown)</span>
    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">!=</span> <span class="s2">&quot;call&quot;</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">threshold</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span>

    <span class="c1"># Mark test as slow if it exceeds threshold</span>
    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="c1"># Add marker for filtering</span>
        <span class="n">item</span><span class="o">.</span><span class="n">add_marker</span><span class="p">(</span><span class="n">pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">slow</span><span class="p">)</span>

        <span class="c1"># Store information for summary report</span>
        <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s2">&quot;nodeid&quot;</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">,</span>
            <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span><span class="p">,</span>
            <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">threshold</span><span class="p">,</span>
            <span class="s2">&quot;outcome&quot;</span><span class="p">:</span> <span class="n">report</span><span class="o">.</span><span class="n">outcome</span>
        <span class="p">})</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_terminal_summary</span><span class="p">(</span><span class="n">terminalreporter</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add custom section to terminal summary showing slow tests.</span>

<span class="sd">    This hook runs at the end of the test session to display</span>
<span class="sd">    a summary of all tests that exceeded the slow threshold.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check if summary should be shown</span>
    <span class="n">show_summary</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;show_slow_summary&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">show_summary</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">slow_tests</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">slow_tests</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Sort by duration (slowest first)</span>
    <span class="n">slow_tests</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;duration&quot;</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Add custom section to output</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">section</span><span class="p">(</span><span class="s2">&quot;Slow Tests Summary&quot;</span><span class="p">)</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">slow_tests</span><span class="p">)</span><span class="si">}</span><span class="s2"> test(s) exceeding &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span><span class="si">}</span><span class="s2">s threshold:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Display each slow test with duration and status</span>
    <span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">slow_tests</span><span class="p">:</span>
        <span class="n">status_symbol</span> <span class="o">=</span> <span class="s2">&quot;‚úì&quot;</span> <span class="k">if</span> <span class="n">test</span><span class="p">[</span><span class="s2">&quot;outcome&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;passed&quot;</span> <span class="k">else</span> <span class="s2">&quot;‚úó&quot;</span>
        <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">status_symbol</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;duration&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s - </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;nodeid&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Add helpful tip</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Tip: Run &#39;pytest -m slow&#39; to run only slow tests&quot;</span>
    <span class="p">)</span>
</code></pre></div>

<h3 id="testing-the-plugin">Testing the Plugin</h3>
<p>Let's verify our plugin works correctly:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Test 1: Basic functionality</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v

<span class="c1"># Test 2: Filter slow tests</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>-m<span class="w"> </span>slow

<span class="c1"># Test 3: Exclude slow tests</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;not slow&quot;</span>

<span class="c1"># Test 4: Custom threshold</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>--slow-threshold<span class="o">=</span><span class="m">1</span>.5

<span class="c1"># Test 5: Show summary</span>
$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v<span class="w"> </span>--show-slow-summary
</code></pre></div>

<p><strong>Output for Test 5</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_api_performance.py::test_fast_operation PASSED               [ 25%]
tests/test_api_performance.py::test_medium_operation PASSED             [ 50%]
tests/test_api_performance.py::test_slow_operation PASSED               [ 75%]
tests/test_api_performance.py::test_very_slow_operation PASSED          [100%]

============================== Slow Tests Summary ==============================
Found 2 test(s) exceeding 0.5s threshold:

  ‚úì 2.00s - tests/test_api_performance.py::test_very_slow_operation
  ‚úì 1.00s - tests/test_api_performance.py::test_slow_operation

Tip: Run &#39;pytest -m slow&#39; to run only slow tests

============================== 4 passed in 3.32s ===============================
</code></pre></div>

<h3 id="common-plugin-patterns">Common Plugin Patterns</h3>
<p>Our slow test plugin demonstrates several common patterns:</p>
<h4 id="pattern-1-configuration-management">Pattern 1: Configuration Management</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># Support both ini-file and command-line configuration</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span><span class="s2">&quot;config_name&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>  <span class="c1"># Config file</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addoption</span><span class="p">(</span><span class="s2">&quot;--flag&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>    <span class="c1"># Command-line</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="c1"># Command-line overrides config file</span>
    <span class="n">value</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getoption</span><span class="p">(</span><span class="s2">&quot;--flag&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;config_name&quot;</span><span class="p">)</span>
</code></pre></div>

<h4 id="pattern-2-state-management">Pattern 2: State Management</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># Store plugin state in config object</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
    <span class="n">config</span><span class="o">.</span><span class="n">plugin_state</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Initialize state</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
    <span class="c1"># Access state via item.config</span>
    <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">plugin_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</code></pre></div>

<h4 id="pattern-3-hook-wrapping">Pattern 3: Hook Wrapping</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># Wrap around existing hooks to add functionality</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>  <span class="c1"># Let other hooks run</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>  <span class="c1"># Get the result</span>
    <span class="c1"># Add custom logic here</span>
</code></pre></div>

<h4 id="pattern-4-terminal-output">Pattern 4: Terminal Output</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># Add custom sections to terminal output</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_terminal_summary</span><span class="p">(</span><span class="n">terminalreporter</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">section</span><span class="p">(</span><span class="s2">&quot;Custom Section&quot;</span><span class="p">)</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span><span class="s2">&quot;Custom output&quot;</span><span class="p">)</span>
</code></pre></div>

<h2 id="decision-framework-local-vs-distributed-plugin">Decision Framework: Local vs. Distributed Plugin</h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Local Plugin (conftest.py)</th>
<th>Distributed Plugin (Package)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Scope</strong></td>
<td>Single project</td>
<td>Multiple projects</td>
</tr>
<tr>
<td><strong>Installation</strong></td>
<td>Automatic (in project)</td>
<td>Manual (pip install)</td>
</tr>
<tr>
<td><strong>Maintenance</strong></td>
<td>Project-specific</td>
<td>Requires versioning</td>
</tr>
<tr>
<td><strong>Sharing</strong></td>
<td>Copy conftest.py</td>
<td>Publish to PyPI</td>
</tr>
<tr>
<td><strong>Best for</strong></td>
<td>Project-specific needs</td>
<td>Reusable functionality</td>
</tr>
</tbody>
</table>
<h3 id="when-to-keep-it-local">When to Keep It Local</h3>
<p>Keep your plugin in <code>conftest.py</code> when:</p>
<ol>
<li><strong>Project-specific</strong>: Functionality is unique to your project</li>
<li><strong>Rapid iteration</strong>: You're still experimenting with the design</li>
<li><strong>Simple scope</strong>: Plugin is under 200 lines</li>
<li><strong>Team size</strong>: Small team that doesn't need formal distribution</li>
</ol>
<h3 id="when-to-distribute">When to Distribute</h3>
<p>Create a separate package when:</p>
<ol>
<li><strong>Reusable</strong>: Multiple projects could benefit</li>
<li><strong>Stable</strong>: API is well-defined and unlikely to change frequently</li>
<li><strong>Complex</strong>: Plugin exceeds 200 lines or has multiple modules</li>
<li><strong>Community value</strong>: Others might find it useful</li>
</ol>
<h2 id="the-journey-from-problem-to-plugin">The Journey: From Problem to Plugin</h2>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Problem</th>
<th>Solution</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Slow tests hard to identify</td>
<td>Manual inspection</td>
<td>Time-consuming</td>
</tr>
<tr>
<td>1</td>
<td>Need automatic detection</td>
<td>Hook into test execution</td>
<td>Tests marked automatically</td>
</tr>
<tr>
<td>2</td>
<td>Hardcoded threshold</td>
<td>Add configuration</td>
<td>Flexible threshold</td>
</tr>
<tr>
<td>3</td>
<td>No visibility</td>
<td>Add summary report</td>
<td>Clear actionable output</td>
</tr>
<tr>
<td>4</td>
<td>Configuration scattered</td>
<td>Support pytest.ini</td>
<td>Centralized config</td>
</tr>
<tr>
<td>5</td>
<td>Production use</td>
<td>Add error handling</td>
<td>Robust plugin</td>
</tr>
</tbody>
</table>
<h3 id="lessons-learned_2">Lessons Learned</h3>
<ol>
<li><strong>Start simple</strong>: Begin with minimal functionality and iterate</li>
<li><strong>Use hooks wisely</strong>: Understand the hook lifecycle before implementing</li>
<li><strong>Configuration matters</strong>: Support both ini-file and command-line options</li>
<li><strong>State management</strong>: Store plugin state in the config object</li>
<li><strong>User experience</strong>: Provide clear output and helpful error messages</li>
<li><strong>Test your plugin</strong>: Verify it works in various scenarios</li>
<li><strong>Document thoroughly</strong>: Include docstrings and usage examples</li>
</ol>
<h2 id="hooks-how-pytest-plugins-work-under-the-hood">Hooks: How Pytest Plugins Work Under the Hood</h2>
<h2 id="hooks-how-pytest-plugins-work-under-the-hood_1">Hooks: How Pytest Plugins Work Under the Hood</h2>
<p>We've created a custom plugin, but to truly master plugin development, we need to understand pytest's hook system. Hooks are the foundation of pytest's extensibility‚Äîthey're the mechanism that allows plugins to modify pytest's behavior at specific points in the test lifecycle.</p>
<h3 id="the-hook-system-a-mental-model">The Hook System: A Mental Model</h3>
<p>Think of pytest's execution as a journey through a series of checkpoints. At each checkpoint, pytest calls specific hook functions, allowing plugins to:</p>
<ol>
<li><strong>Observe</strong>: See what's happening (read-only)</li>
<li><strong>Modify</strong>: Change behavior or data</li>
<li><strong>Extend</strong>: Add new functionality</li>
</ol>
<p>Let's visualize the test execution lifecycle and where hooks are called:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Test</span><span class="w"> </span><span class="n">Session</span><span class="w"> </span><span class="n">Start</span>
<span class="w">    </span><span class="err">‚Üì</span>
<span class="n">pytest_configure</span><span class="p">()</span><span class="w">          </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">Configure</span><span class="w"> </span><span class="n">plugins</span><span class="p">,</span><span class="w"> </span><span class="n">register</span><span class="w"> </span><span class="n">markers</span>
<span class="w">    </span><span class="err">‚Üì</span>
<span class="n">pytest_collection</span><span class="p">()</span><span class="w">         </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">Discover</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">files</span>
<span class="w">    </span><span class="err">‚Üì</span>
<span class="n">pytest_collect_file</span><span class="p">()</span><span class="w">       </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">Process</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">file</span>
<span class="w">    </span><span class="err">‚Üì</span>
<span class="n">pytest_generate_tests</span><span class="p">()</span><span class="w">     </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">Generate</span><span class="w"> </span><span class="n">parametrized</span><span class="w"> </span><span class="n">tests</span>
<span class="w">    </span><span class="err">‚Üì</span>
<span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="nl">test:</span>
<span class="w">    </span><span class="n">pytest_runtest_setup</span><span class="p">()</span><span class="w">      </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">Before</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">setup</span>
<span class="w">        </span><span class="err">‚Üì</span>
<span class="w">    </span><span class="n">pytest_runtest_call</span><span class="p">()</span><span class="w">       </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">Run</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">actual</span><span class="w"> </span><span class="n">test</span>
<span class="w">        </span><span class="err">‚Üì</span>
<span class="w">    </span><span class="n">pytest_runtest_teardown</span><span class="p">()</span><span class="w">   </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">After</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">teardown</span>
<span class="w">        </span><span class="err">‚Üì</span>
<span class="w">    </span><span class="n">pytest_runtest_makereport</span><span class="p">()</span><span class="w"> </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">Create</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">report</span>
<span class="w">    </span><span class="err">‚Üì</span>
<span class="n">pytest_terminal_summary</span><span class="p">()</span><span class="w">   </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">Display</span><span class="w"> </span><span class="k">final</span><span class="w"> </span><span class="n">summary</span>
<span class="w">    </span><span class="err">‚Üì</span>
<span class="n">pytest_unconfigure</span><span class="p">()</span><span class="w">        </span><span class="err">‚Üê</span><span class="w"> </span><span class="n">Cleanup</span>
<span class="w">    </span><span class="err">‚Üì</span>
<span class="n">Test</span><span class="w"> </span><span class="n">Session</span><span class="w"> </span><span class="n">End</span>
</code></pre></div>

<h3 id="phase-1-exploring-hook-execution-order">Phase 1: Exploring Hook Execution Order</h3>
<p>Let's create a diagnostic plugin that logs every hook call to understand the execution flow:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="c1"># Track hook calls</span>
<span class="n">hook_calls</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">log_hook</span><span class="p">(</span><span class="n">hook_name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Log a hook call with its arguments.&quot;&quot;&quot;</span>
    <span class="n">hook_calls</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
        <span class="s2">&quot;hook&quot;</span><span class="p">:</span> <span class="n">hook_name</span><span class="p">,</span>
        <span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)[:</span><span class="mi">50</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="p">})</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[HOOK] </span><span class="si">{</span><span class="n">hook_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)[:</span><span class="mi">50</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Configuration hooks</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called after command-line options have been parsed.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_configure&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_sessionstart</span><span class="p">(</span><span class="n">session</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called after Session object has been created.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_sessionstart&quot;</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>

<span class="c1"># Collection hooks</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_collection</span><span class="p">(</span><span class="n">session</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called to perform collection.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_collection&quot;</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_collectstart</span><span class="p">(</span><span class="n">collector</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called before collecting from a collector.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_collectstart&quot;</span><span class="p">,</span> <span class="n">collector</span><span class="o">=</span><span class="n">collector</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_collect_file</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="n">parent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called for each file in the test directory.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_collect_file&quot;</span><span class="p">,</span> <span class="n">file_path</span><span class="o">=</span><span class="n">file_path</span><span class="p">,</span> <span class="n">parent</span><span class="o">=</span><span class="n">parent</span><span class="p">)</span>

<span class="c1"># Test execution hooks</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_setup</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called before test setup.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_runtest_setup&quot;</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="n">item</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_call</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called to run the test.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_runtest_call&quot;</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="n">item</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_teardown</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called after test teardown.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_runtest_teardown&quot;</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="n">item</span><span class="p">)</span>

<span class="c1"># Reporting hooks</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called to create test report.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_runtest_makereport&quot;</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="o">=</span><span class="n">call</span><span class="p">)</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  report.when: </span><span class="si">{</span><span class="n">report</span><span class="o">.</span><span class="n">when</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  report.outcome: </span><span class="si">{</span><span class="n">report</span><span class="o">.</span><span class="n">outcome</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_terminal_summary</span><span class="p">(</span><span class="n">terminalreporter</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called to add information to terminal summary.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_terminal_summary&quot;</span><span class="p">,</span> 
             <span class="n">terminalreporter</span><span class="o">=</span><span class="n">terminalreporter</span><span class="p">,</span>
             <span class="n">exitstatus</span><span class="o">=</span><span class="n">exitstatus</span><span class="p">)</span>

<span class="c1"># Cleanup hooks</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_sessionfinish</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called after whole test run finished.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_sessionfinish&quot;</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="n">session</span><span class="p">,</span> <span class="n">exitstatus</span><span class="o">=</span><span class="n">exitstatus</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_unconfigure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Called before test process is exited.&quot;&quot;&quot;</span>
    <span class="n">log_hook</span><span class="p">(</span><span class="s2">&quot;pytest_unconfigure&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

    <span class="c1"># Print summary of all hook calls</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;HOOK EXECUTION SUMMARY&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="o">*</span><span class="mi">70</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">call</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hook_calls</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">. </span><span class="si">{</span><span class="n">call</span><span class="p">[</span><span class="s1">&#39;hook&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>Let's create a simple test to observe the hook execution:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_hooks_demo.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_simple</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A simple test to observe hook execution.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">2</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_hooks_demo.py<span class="w"> </span>-v<span class="w"> </span>-s
</code></pre></div>

<p><strong>Output</strong> (abbreviated for clarity):</p>
<div class="codehilite"><pre><span></span><code><span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_configure</span>
<span class="w">  </span><span class="nl">config</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">_pytest</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">Config</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="mi">0</span><span class="n">x</span><span class="p">...</span><span class="o">&gt;</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_sessionstart</span>
<span class="w">  </span><span class="k">session</span><span class="err">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Session</span><span class="w"> </span><span class="n">pytest</span><span class="w"> </span><span class="n">exitstatus</span><span class="o">=&lt;</span><span class="n">UNSET</span><span class="o">&gt;</span><span class="w"> </span><span class="n">testsfailed</span><span class="o">=</span><span class="mi">0</span><span class="o">&gt;</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_collection</span>
<span class="w">  </span><span class="k">session</span><span class="err">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Session</span><span class="w"> </span><span class="n">pytest</span><span class="w"> </span><span class="n">exitstatus</span><span class="o">=&lt;</span><span class="n">UNSET</span><span class="o">&gt;</span><span class="w"> </span><span class="n">testsfailed</span><span class="o">=</span><span class="mi">0</span><span class="o">&gt;</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_collectstart</span>
<span class="w">  </span><span class="nl">collector</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">Dir</span><span class="w"> </span><span class="n">pytest</span><span class="o">&gt;</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_collect_file</span>
<span class="w">  </span><span class="nl">file_path</span><span class="p">:</span><span class="w"> </span><span class="n">tests</span><span class="o">/</span><span class="n">test_hooks_demo</span><span class="p">.</span><span class="n">py</span>
<span class="w">  </span><span class="nl">parent</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">Dir</span><span class="w"> </span><span class="n">pytest</span><span class="o">&gt;</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_collectstart</span>
<span class="w">  </span><span class="nl">collector</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Module</span><span class="w"> </span><span class="n">test_hooks_demo</span><span class="p">.</span><span class="n">py</span><span class="o">&gt;</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_runtest_setup</span>
<span class="w">  </span><span class="nl">item</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Function</span><span class="w"> </span><span class="n">test_simple</span><span class="o">&gt;</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_runtest_makereport</span>
<span class="w">  </span><span class="nl">item</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Function</span><span class="w"> </span><span class="n">test_simple</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">call</span><span class="err">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">CallInfo</span><span class="w"> </span><span class="k">when</span><span class="o">=</span><span class="s1">&#39;setup&#39;</span><span class="w"> </span><span class="n">outcome</span><span class="o">=</span><span class="s1">&#39;passed&#39;</span><span class="o">&gt;</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="k">when</span><span class="err">:</span><span class="w"> </span><span class="n">setup</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="nl">outcome</span><span class="p">:</span><span class="w"> </span><span class="n">passed</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_runtest_call</span>
<span class="w">  </span><span class="nl">item</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Function</span><span class="w"> </span><span class="n">test_simple</span><span class="o">&gt;</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_runtest_makereport</span>
<span class="w">  </span><span class="nl">item</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Function</span><span class="w"> </span><span class="n">test_simple</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">call</span><span class="err">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">CallInfo</span><span class="w"> </span><span class="k">when</span><span class="o">=</span><span class="s1">&#39;call&#39;</span><span class="w"> </span><span class="n">outcome</span><span class="o">=</span><span class="s1">&#39;passed&#39;</span><span class="o">&gt;</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="k">when</span><span class="err">:</span><span class="w"> </span><span class="k">call</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="nl">outcome</span><span class="p">:</span><span class="w"> </span><span class="n">passed</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_runtest_teardown</span>
<span class="w">  </span><span class="nl">item</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Function</span><span class="w"> </span><span class="n">test_simple</span><span class="o">&gt;</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_runtest_makereport</span>
<span class="w">  </span><span class="nl">item</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Function</span><span class="w"> </span><span class="n">test_simple</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">call</span><span class="err">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">CallInfo</span><span class="w"> </span><span class="k">when</span><span class="o">=</span><span class="s1">&#39;teardown&#39;</span><span class="w"> </span><span class="n">outcome</span><span class="o">=</span><span class="s1">&#39;passed&#39;</span><span class="o">&gt;</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="k">when</span><span class="err">:</span><span class="w"> </span><span class="n">teardown</span>
<span class="w">  </span><span class="n">report</span><span class="p">.</span><span class="nl">outcome</span><span class="p">:</span><span class="w"> </span><span class="n">passed</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_terminal_summary</span>
<span class="w">  </span><span class="nl">terminalreporter</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">_pytest</span><span class="p">.</span><span class="n">terminal</span><span class="p">.</span><span class="n">TerminalReporter</span><span class="w"> </span><span class="k">object</span><span class="o">&gt;</span>
<span class="w">  </span><span class="nl">exitstatus</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_sessionfinish</span>
<span class="w">  </span><span class="k">session</span><span class="err">:</span><span class="w"> </span><span class="o">&lt;</span><span class="k">Session</span><span class="w"> </span><span class="n">pytest</span><span class="w"> </span><span class="n">exitstatus</span><span class="o">=</span><span class="mi">0</span><span class="w"> </span><span class="n">testsfailed</span><span class="o">=</span><span class="mi">0</span><span class="o">&gt;</span>
<span class="w">  </span><span class="nl">exitstatus</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span>

<span class="o">[</span><span class="n">HOOK</span><span class="o">]</span><span class="w"> </span><span class="n">pytest_unconfigure</span>
<span class="w">  </span><span class="nl">config</span><span class="p">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">_pytest</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">Config</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="mi">0</span><span class="n">x</span><span class="p">...</span><span class="o">&gt;</span>

<span class="o">======================================================================</span>
<span class="n">HOOK</span><span class="w"> </span><span class="n">EXECUTION</span><span class="w"> </span><span class="n">SUMMARY</span>
<span class="o">======================================================================</span>
<span class="mf">1.</span><span class="w"> </span><span class="n">pytest_configure</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">pytest_sessionstart</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">pytest_collection</span>
<span class="mf">4.</span><span class="w"> </span><span class="n">pytest_collectstart</span>
<span class="mf">5.</span><span class="w"> </span><span class="n">pytest_collect_file</span>
<span class="mf">6.</span><span class="w"> </span><span class="n">pytest_collectstart</span>
<span class="mf">7.</span><span class="w"> </span><span class="n">pytest_runtest_setup</span>
<span class="mf">8.</span><span class="w"> </span><span class="n">pytest_runtest_makereport</span>
<span class="mf">9.</span><span class="w"> </span><span class="n">pytest_runtest_call</span>
<span class="mf">10.</span><span class="w"> </span><span class="n">pytest_runtest_makereport</span>
<span class="mf">11.</span><span class="w"> </span><span class="n">pytest_runtest_teardown</span>
<span class="mf">12.</span><span class="w"> </span><span class="n">pytest_runtest_makereport</span>
<span class="mf">13.</span><span class="w"> </span><span class="n">pytest_terminal_summary</span>
<span class="mf">14.</span><span class="w"> </span><span class="n">pytest_sessionfinish</span>
<span class="mf">15.</span><span class="w"> </span><span class="n">pytest_unconfigure</span>
</code></pre></div>

<h3 id="diagnostic-analysis-understanding-hook-execution_1">Diagnostic Analysis: Understanding Hook Execution</h3>
<p><strong>Key observations</strong>:</p>
<ol>
<li><strong>Configuration first</strong>: <code>pytest_configure</code> runs before anything else</li>
<li><strong>Collection phase</strong>: Multiple hooks for discovering and collecting tests</li>
<li><strong>Three-phase execution</strong>: Each test goes through setup ‚Üí call ‚Üí teardown</li>
<li><strong>Three reports</strong>: <code>pytest_runtest_makereport</code> is called three times per test</li>
<li><strong>Summary last</strong>: <code>pytest_terminal_summary</code> runs after all tests complete</li>
</ol>
<p><strong>What this tells us</strong>: Hooks provide fine-grained control over every stage of test execution.</p>
<h3 id="phase-2-hook-implementation-patterns">Phase 2: Hook Implementation Patterns</h3>
<p>Now let's explore the three main patterns for implementing hooks:</p>
<h4 id="pattern-1-simple-hook-direct-implementation">Pattern 1: Simple Hook (Direct Implementation)</h4>
<p>The simplest pattern‚Äîjust implement the hook function:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Simple hook - just implement the function.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Configuring pytest...&quot;</span><span class="p">)</span>
    <span class="n">config</span><span class="o">.</span><span class="n">custom_data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;initialized&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
</code></pre></div>

<p><strong>When to use</strong>: When you need to execute code at a specific point without modifying pytest's behavior.</p>
<h4 id="pattern-2-hook-wrapper-observe-and-modify">Pattern 2: Hook Wrapper (Observe and Modify)</h4>
<p>Use <code>@pytest.hookimpl(hookwrapper=True)</code> to wrap around existing hooks:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_call</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Wrapper hook - measure test execution time.&quot;&quot;&quot;</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="c1"># Let the test run</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>

    <span class="c1"># Code here runs after the test</span>
    <span class="n">duration</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[TIMING] </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2"> took </span><span class="si">{</span><span class="n">duration</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>

    <span class="c1"># You can access the result</span>
    <span class="c1"># outcome.get_result() would return the test result</span>
</code></pre></div>

<p><strong>When to use</strong>: When you need to execute code before AND after an existing hook, or when you need to modify the result.</p>
<h4 id="pattern-3-hook-specification-firstlast-execution">Pattern 3: Hook Specification (First/Last Execution)</h4>
<p>Control execution order with <code>tryfirst</code> and <code>trylast</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_setup</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Runs before other setup hooks.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[FIRST] Setting up </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">trylast</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_teardown</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Runs after other teardown hooks.&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">[LAST] Tearing down </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p><strong>When to use</strong>: When hook execution order matters (e.g., you need to run before/after other plugins).</p>
<h3 id="phase-3-practical-hook-examples">Phase 3: Practical Hook Examples</h3>
<p>Let's implement several practical plugins using different hooks:</p>
<h4 id="example-1-test-retry-plugin">Example 1: Test Retry Plugin</h4>
<p>Automatically retry flaky tests:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add retry configuration.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;retries&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;0&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of times to retry failed tests&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register retry marker.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;flaky: mark test as flaky (will be retried on failure)&quot;</span>
    <span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implement retry logic.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="c1"># Only retry on test call failures (not setup/teardown)</span>
    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">!=</span> <span class="s2">&quot;call&quot;</span> <span class="ow">or</span> <span class="n">report</span><span class="o">.</span><span class="n">outcome</span> <span class="o">!=</span> <span class="s2">&quot;failed&quot;</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Check if test is marked as flaky</span>
    <span class="n">flaky_marker</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">get_closest_marker</span><span class="p">(</span><span class="s2">&quot;flaky&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">flaky_marker</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Get retry count</span>
    <span class="n">max_retries</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;retries&quot;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">max_retries</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Track retry attempts</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s2">&quot;retry_count&quot;</span><span class="p">):</span>
        <span class="n">item</span><span class="o">.</span><span class="n">retry_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">item</span><span class="o">.</span><span class="n">retry_count</span> <span class="o">&lt;</span> <span class="n">max_retries</span><span class="p">:</span>
        <span class="n">item</span><span class="o">.</span><span class="n">retry_count</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">report</span><span class="o">.</span><span class="n">outcome</span> <span class="o">=</span> <span class="s2">&quot;rerun&quot;</span>
        <span class="n">report</span><span class="o">.</span><span class="n">wasxfail</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Retry </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">retry_count</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">max_retries</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre></div>

<p>Test the retry plugin:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_retry.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">flaky</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_flaky_operation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that fails randomly.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.7</span><span class="p">:</span>  <span class="c1"># 70% chance of failure</span>
        <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span><span class="s2">&quot;Random failure&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="kc">True</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="na">retries</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">3</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_retry.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong> (example run):</p>
<div class="codehilite"><pre><span></span><code>tests/test_retry.py::test_flaky_operation RERUN (Retry 1/3)           [ 50%]
tests/test_retry.py::test_flaky_operation RERUN (Retry 2/3)           [ 50%]
tests/test_retry.py::test_flaky_operation PASSED                      [100%]

============================== 1 passed in 0.03s ===============================
</code></pre></div>

<h4 id="example-2-test-dependency-plugin">Example 2: Test Dependency Plugin</h4>
<p>Ensure tests run in a specific order:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register dependency marker.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;depends(name): mark test as dependent on another test&quot;</span>
    <span class="p">)</span>
    <span class="n">config</span><span class="o">.</span><span class="n">test_results</span> <span class="o">=</span> <span class="p">{}</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_setup</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check dependencies before running test.&quot;&quot;&quot;</span>
    <span class="n">depends_marker</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">get_closest_marker</span><span class="p">(</span><span class="s2">&quot;depends&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">depends_marker</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Get dependency name</span>
    <span class="n">dependency</span> <span class="o">=</span> <span class="n">depends_marker</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Check if dependency passed</span>
    <span class="k">if</span> <span class="n">dependency</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">test_results</span><span class="p">:</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dependency &#39;</span><span class="si">{</span><span class="n">dependency</span><span class="si">}</span><span class="s2">&#39; has not run yet&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">test_results</span><span class="p">[</span><span class="n">dependency</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;passed&quot;</span><span class="p">:</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dependency &#39;</span><span class="si">{</span><span class="n">dependency</span><span class="si">}</span><span class="s2">&#39; did not pass&quot;</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Store test results for dependency checking.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span><span class="p">:</span>
        <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">test_results</span><span class="p">[</span><span class="n">item</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">report</span><span class="o">.</span><span class="n">outcome</span>
</code></pre></div>

<p>Test the dependency plugin:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_dependencies.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_database_connection</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that must pass first.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>  <span class="c1"># Simulates successful DB connection</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">depends</span><span class="p">(</span><span class="s2">&quot;test_database_connection&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_database_query</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that depends on database connection.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>  <span class="c1"># Simulates successful query</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">depends</span><span class="p">(</span><span class="s2">&quot;test_database_query&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_database_transaction</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that depends on query working.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>  <span class="c1"># Simulates successful transaction</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_dependencies.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_dependencies.py::test_database_connection PASSED           [ 33%]
tests/test_dependencies.py::test_database_query PASSED                [ 66%]
tests/test_dependencies.py::test_database_transaction PASSED          [100%]

============================== 3 passed in 0.02s ===============================
</code></pre></div>

<p>If the first test fails:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_dependencies.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_database_connection</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that must pass first.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">False</span>  <span class="c1"># Simulates failed DB connection</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_dependencies.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_dependencies.py::test_database_connection FAILED           [ 33%]
tests/test_dependencies.py::test_database_query SKIPPED               [ 66%]
tests/test_dependencies.py::test_database_transaction SKIPPED         [100%]

=================================== FAILURES ===================================
________________________ test_database_connection _____________________________
    def test_database_connection():
        &quot;&quot;&quot;Test that must pass first.&quot;&quot;&quot;
<span class="k">&gt; </span><span class="ge">      assert False</span>
E       assert False

tests/test_dependencies.py:5: AssertionError

========================= 1 failed, 2 skipped in 0.03s =========================
</code></pre></div>

<p><strong>What happened</strong>: Dependent tests were automatically skipped because the dependency failed.</p>
<h3 id="phase-4-advanced-hook-techniques">Phase 4: Advanced Hook Techniques</h3>
<h4 id="technique-1-modifying-test-collection">Technique 1: Modifying Test Collection</h4>
<p>Dynamically modify which tests are collected:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_collection_modifyitems</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">items</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Modify collected test items.&quot;&quot;&quot;</span>
    <span class="c1"># Reorder tests: run fast tests first</span>
    <span class="n">items</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">get_closest_marker</span><span class="p">(</span><span class="s2">&quot;slow&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>

    <span class="c1"># Add markers based on test name patterns</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;integration&quot;</span> <span class="ow">in</span> <span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">:</span>
            <span class="n">item</span><span class="o">.</span><span class="n">add_marker</span><span class="p">(</span><span class="n">pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;unit&quot;</span> <span class="ow">in</span> <span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">:</span>
            <span class="n">item</span><span class="o">.</span><span class="n">add_marker</span><span class="p">(</span><span class="n">pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">unit</span><span class="p">)</span>
</code></pre></div>

<h4 id="technique-2-custom-test-outcomes">Technique 2: Custom Test Outcomes</h4>
<p>Create custom test outcomes beyond pass/fail:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register custom outcome.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;expected_failure: mark test as expected to fail&quot;</span>
    <span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Handle expected failures.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="n">expected_failure</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">get_closest_marker</span><span class="p">(</span><span class="s2">&quot;expected_failure&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">expected_failure</span> <span class="ow">and</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">outcome</span> <span class="o">==</span> <span class="s2">&quot;failed&quot;</span><span class="p">:</span>
            <span class="n">report</span><span class="o">.</span><span class="n">outcome</span> <span class="o">=</span> <span class="s2">&quot;passed&quot;</span>
            <span class="n">report</span><span class="o">.</span><span class="n">wasxfail</span> <span class="o">=</span> <span class="s2">&quot;Expected failure occurred&quot;</span>
        <span class="k">elif</span> <span class="n">report</span><span class="o">.</span><span class="n">outcome</span> <span class="o">==</span> <span class="s2">&quot;passed&quot;</span><span class="p">:</span>
            <span class="n">report</span><span class="o">.</span><span class="n">outcome</span> <span class="o">=</span> <span class="s2">&quot;failed&quot;</span>
            <span class="n">report</span><span class="o">.</span><span class="n">longrepr</span> <span class="o">=</span> <span class="s2">&quot;Test passed but was expected to fail&quot;</span>
</code></pre></div>

<h4 id="technique-3-fixture-injection-via-hooks">Technique 3: Fixture Injection via Hooks</h4>
<p>Dynamically inject fixtures into tests:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">auto_injected_fixture</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fixture that will be auto-injected.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;auto&quot;</span><span class="p">:</span> <span class="s2">&quot;injected&quot;</span><span class="p">}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_generate_tests</span><span class="p">(</span><span class="n">metafunc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Automatically inject fixtures into tests.&quot;&quot;&quot;</span>
    <span class="c1"># Inject fixture into all tests in specific modules</span>
    <span class="k">if</span> <span class="s2">&quot;auto_test&quot;</span> <span class="ow">in</span> <span class="n">metafunc</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="vm">__name__</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;auto_injected_fixture&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">metafunc</span><span class="o">.</span><span class="n">fixturenames</span><span class="p">:</span>
            <span class="n">metafunc</span><span class="o">.</span><span class="n">fixturenames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;auto_injected_fixture&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="hook-reference-most-commonly-used-hooks">Hook Reference: Most Commonly Used Hooks</h3>
<table>
<thead>
<tr>
<th>Hook</th>
<th>Purpose</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>pytest_configure</strong></td>
<td>Configure pytest</td>
<td>Register markers, initialize plugin state</td>
</tr>
<tr>
<td><strong>pytest_addoption</strong></td>
<td>Add CLI options</td>
<td>Add custom command-line flags</td>
</tr>
<tr>
<td><strong>pytest_collection_modifyitems</strong></td>
<td>Modify test collection</td>
<td>Reorder tests, add markers dynamically</td>
</tr>
<tr>
<td><strong>pytest_generate_tests</strong></td>
<td>Parametrize tests</td>
<td>Dynamic parametrization, fixture injection</td>
</tr>
<tr>
<td><strong>pytest_runtest_setup</strong></td>
<td>Before test setup</td>
<td>Dependency checking, pre-test validation</td>
</tr>
<tr>
<td><strong>pytest_runtest_call</strong></td>
<td>During test execution</td>
<td>Timing, monitoring</td>
</tr>
<tr>
<td><strong>pytest_runtest_teardown</strong></td>
<td>After test teardown</td>
<td>Cleanup, resource release</td>
</tr>
<tr>
<td><strong>pytest_runtest_makereport</strong></td>
<td>Create test report</td>
<td>Custom outcomes, result tracking</td>
</tr>
<tr>
<td><strong>pytest_terminal_summary</strong></td>
<td>Terminal output</td>
<td>Custom summary sections</td>
</tr>
</tbody>
</table>
<h3 id="hook-execution-order-complete-reference">Hook Execution Order: Complete Reference</h3>
<p>For a single test, hooks execute in this order:</p>
<ol>
<li><strong>Session Start</strong></li>
<li><code>pytest_configure</code></li>
<li>
<p><code>pytest_sessionstart</code></p>
</li>
<li>
<p><strong>Collection</strong></p>
</li>
<li><code>pytest_collection</code></li>
<li><code>pytest_collect_file</code></li>
<li>
<p><code>pytest_collection_modifyitems</code></p>
</li>
<li>
<p><strong>Test Execution</strong> (per test)</p>
</li>
<li><code>pytest_runtest_protocol</code> (wrapper)</li>
<li><code>pytest_runtest_logstart</code></li>
<li><code>pytest_runtest_setup</code></li>
<li><code>pytest_runtest_makereport</code> (setup phase)</li>
<li><code>pytest_runtest_call</code></li>
<li><code>pytest_runtest_makereport</code> (call phase)</li>
<li><code>pytest_runtest_teardown</code></li>
<li><code>pytest_runtest_makereport</code> (teardown phase)</li>
<li>
<p><code>pytest_runtest_logfinish</code></p>
</li>
<li>
<p><strong>Session End</strong></p>
</li>
<li><code>pytest_terminal_summary</code></li>
<li><code>pytest_sessionfinish</code></li>
<li><code>pytest_unconfigure</code></li>
</ol>
<h2 id="decision-framework-which-hook-to-use">Decision Framework: Which Hook to Use?</h2>
<table>
<thead>
<tr>
<th>Goal</th>
<th>Hook to Use</th>
<th>Pattern</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Add configuration</strong></td>
<td><code>pytest_configure</code></td>
<td>Simple</td>
</tr>
<tr>
<td><strong>Add CLI options</strong></td>
<td><code>pytest_addoption</code></td>
<td>Simple</td>
</tr>
<tr>
<td><strong>Modify test collection</strong></td>
<td><code>pytest_collection_modifyitems</code></td>
<td>Simple</td>
</tr>
<tr>
<td><strong>Measure test timing</strong></td>
<td><code>pytest_runtest_call</code></td>
<td>Wrapper</td>
</tr>
<tr>
<td><strong>Track test results</strong></td>
<td><code>pytest_runtest_makereport</code></td>
<td>Wrapper</td>
</tr>
<tr>
<td><strong>Add custom output</strong></td>
<td><code>pytest_terminal_summary</code></td>
<td>Simple</td>
</tr>
<tr>
<td><strong>Implement retries</strong></td>
<td><code>pytest_runtest_makereport</code></td>
<td>Wrapper + tryfirst</td>
</tr>
<tr>
<td><strong>Check dependencies</strong></td>
<td><code>pytest_runtest_setup</code></td>
<td>Simple + tryfirst</td>
</tr>
</tbody>
</table>
<h2 id="the-journey-from-hook-confusion-to-hook-mastery">The Journey: From Hook Confusion to Hook Mastery</h2>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Understanding</th>
<th>Capability</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Hooks are mysterious</td>
<td>Can't extend pytest</td>
</tr>
<tr>
<td>1</td>
<td>Hooks are checkpoints</td>
<td>Can observe execution</td>
</tr>
<tr>
<td>2</td>
<td>Hooks have patterns</td>
<td>Can implement simple plugins</td>
</tr>
<tr>
<td>3</td>
<td>Hooks have order</td>
<td>Can control execution flow</td>
</tr>
<tr>
<td>4</td>
<td>Hooks compose</td>
<td>Can build complex plugins</td>
</tr>
<tr>
<td>5</td>
<td>Hooks are powerful</td>
<td>Can modify pytest behavior completely</td>
</tr>
</tbody>
</table>
<h3 id="lessons-learned_3">Lessons Learned</h3>
<ol>
<li><strong>Hooks are checkpoints</strong>: They're called at specific points in pytest's execution</li>
<li><strong>Three main patterns</strong>: Simple, wrapper, and execution order control</li>
<li><strong>Execution order matters</strong>: Use <code>tryfirst</code>/<code>trylast</code> when order is critical</li>
<li><strong>Wrappers are powerful</strong>: Use <code>hookwrapper=True</code> to execute code before and after</li>
<li><strong>State management</strong>: Store plugin state in the config object</li>
<li><strong>Test your hooks</strong>: Verify they work in various scenarios</li>
<li><strong>Document hook usage</strong>: Explain which hooks your plugin uses and why</li>
</ol>
<h2 id="common-plugin-use-cases">Common Plugin Use Cases</h2>
<h2 id="common-plugin-use-cases_1">Common Plugin Use Cases</h2>
<p>Now that we understand how hooks work, let's explore common real-world scenarios where custom plugins solve practical problems. Each use case demonstrates a different aspect of plugin development and addresses actual pain points in testing workflows.</p>
<h3 id="use-case-1-test-data-management">Use Case 1: Test Data Management</h3>
<p><strong>Problem</strong>: Tests need consistent, isolated test data, but setting it up manually in each test is repetitive and error-prone.</p>
<p><strong>Solution</strong>: A plugin that automatically provides fresh test data for each test.</p>
<h4 id="implementation-auto-fixture-plugin">Implementation: Auto-Fixture Plugin</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add test data configuration.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;test_data_dir&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;tests/data&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Directory containing test data files&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Load test data at session start.&quot;&quot;&quot;</span>
    <span class="n">data_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;test_data_dir&quot;</span><span class="p">))</span>
    <span class="n">config</span><span class="o">.</span><span class="n">test_data</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">if</span> <span class="n">data_dir</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">data_file</span> <span class="ow">in</span> <span class="n">data_dir</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;*.json&quot;</span><span class="p">):</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">config</span><span class="o">.</span><span class="n">test_data</span><span class="p">[</span><span class="n">data_file</span><span class="o">.</span><span class="n">stem</span><span class="p">]</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_data</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Provide test data based on test name or marker.&quot;&quot;&quot;</span>
    <span class="c1"># Check for explicit data marker</span>
    <span class="n">data_marker</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">get_closest_marker</span><span class="p">(</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">data_marker</span><span class="p">:</span>
        <span class="n">data_name</span> <span class="o">=</span> <span class="n">data_marker</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">request</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">test_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">data_name</span><span class="p">,</span> <span class="p">{})</span>

    <span class="c1"># Auto-detect based on test name</span>
    <span class="n">test_name</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">node</span><span class="o">.</span><span class="n">name</span>
    <span class="k">for</span> <span class="n">data_name</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">request</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">test_data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">data_name</span> <span class="ow">in</span> <span class="n">test_name</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data</span>

    <span class="k">return</span> <span class="p">{}</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_generate_tests</span><span class="p">(</span><span class="n">metafunc</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Auto-inject test_data fixture if test data exists.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="s2">&quot;test_data&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">metafunc</span><span class="o">.</span><span class="n">fixturenames</span><span class="p">:</span>
        <span class="c1"># Check if test name matches any data file</span>
        <span class="n">test_name</span> <span class="o">=</span> <span class="n">metafunc</span><span class="o">.</span><span class="n">function</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">for</span> <span class="n">data_name</span> <span class="ow">in</span> <span class="n">metafunc</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">test_data</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">data_name</span> <span class="ow">in</span> <span class="n">test_name</span><span class="p">:</span>
                <span class="n">metafunc</span><span class="o">.</span><span class="n">fixturenames</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;test_data&quot;</span><span class="p">)</span>
                <span class="k">break</span>
</code></pre></div>

<p>Create test data files:</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// tests/data/users.json</span>
<span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;valid_user&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;username&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;testuser&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;email&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;test@example.com&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;age&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">25</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;invalid_user&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;username&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;email&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;invalid&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;age&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">-1</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="c1">// tests/data/products.json</span>
<span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;laptop&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Test Laptop&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;price&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">999.99</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;stock&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span>
<span class="w">  </span><span class="p">},</span>
<span class="w">  </span><span class="nt">&quot;phone&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;name&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Test Phone&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;price&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">599.99</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;stock&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>Use the plugin in tests:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_users.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_users_valid_user</span><span class="p">(</span><span class="n">test_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test automatically gets users.json data.&quot;&quot;&quot;</span>
    <span class="n">user</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="s2">&quot;valid_user&quot;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">user</span><span class="p">[</span><span class="s2">&quot;username&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;testuser&quot;</span>
    <span class="k">assert</span> <span class="n">user</span><span class="p">[</span><span class="s2">&quot;age&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">25</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_users_invalid_user</span><span class="p">(</span><span class="n">test_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test automatically gets users.json data.&quot;&quot;&quot;</span>
    <span class="n">user</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="s2">&quot;invalid_user&quot;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">user</span><span class="p">[</span><span class="s2">&quot;username&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">user</span><span class="p">[</span><span class="s2">&quot;age&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">data</span><span class="p">(</span><span class="s2">&quot;products&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_explicit_data_loading</span><span class="p">(</span><span class="n">test_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test explicitly requests products.json data.&quot;&quot;&quot;</span>
    <span class="n">laptop</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="s2">&quot;laptop&quot;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">laptop</span><span class="p">[</span><span class="s2">&quot;price&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">999.99</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_users.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="n">tests</span><span class="o">/</span><span class="n">test_users</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_users_valid_user</span><span class="w"> </span><span class="n">PASSED</span><span class="w">                     </span><span class="p">[</span><span class="w"> </span><span class="mi">33</span><span class="o">%</span><span class="p">]</span>
<span class="n">tests</span><span class="o">/</span><span class="n">test_users</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_users_invalid_user</span><span class="w"> </span><span class="n">PASSED</span><span class="w">                   </span><span class="p">[</span><span class="w"> </span><span class="mi">66</span><span class="o">%</span><span class="p">]</span>
<span class="n">tests</span><span class="o">/</span><span class="n">test_users</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_explicit_data_loading</span><span class="w"> </span><span class="n">PASSED</span><span class="w">                </span><span class="p">[</span><span class="mi">100</span><span class="o">%</span><span class="p">]</span>

<span class="o">==============================</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">passed</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="mf">0.02</span><span class="n">s</span><span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p><strong>What we achieved</strong>:</p>
<ol>
<li><strong>Automatic data loading</strong>: Test data loads once at session start</li>
<li><strong>Auto-injection</strong>: Tests automatically get relevant data based on naming</li>
<li><strong>Explicit control</strong>: Tests can explicitly request specific data with markers</li>
<li><strong>Centralized management</strong>: All test data in one location</li>
</ol>
<h3 id="use-case-2-test-environment-validation">Use Case 2: Test Environment Validation</h3>
<p><strong>Problem</strong>: Tests fail in CI/CD because required environment variables or dependencies are missing, but the failures are cryptic.</p>
<p><strong>Solution</strong>: A plugin that validates the test environment before running tests.</p>
<h4 id="implementation-environment-validator-plugin">Implementation: Environment Validator Plugin</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add environment validation options.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;required_env_vars&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;linelist&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;List of required environment variables&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;required_packages&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;linelist&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;List of required Python packages&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Validate environment before running tests.&quot;&quot;&quot;</span>
    <span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Check required environment variables</span>
    <span class="n">required_vars</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;required_env_vars&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">required_vars</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">var</span><span class="p">):</span>
            <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Missing required environment variable: </span><span class="si">{</span><span class="n">var</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Check required packages</span>
    <span class="n">required_packages</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;required_packages&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">package</span> <span class="ow">in</span> <span class="n">required_packages</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="nb">__import__</span><span class="p">(</span><span class="n">package</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
            <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Missing required package: </span><span class="si">{</span><span class="n">package</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Check Python version if specified</span>
    <span class="n">min_python</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;min_python_version&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">min_python</span><span class="p">:</span>
        <span class="n">current</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">required</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">min_python</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">current</span> <span class="o">&lt;</span> <span class="n">required</span><span class="p">:</span>
            <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Python </span><span class="si">{</span><span class="n">required</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">required</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">+ required, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;but running </span><span class="si">{</span><span class="n">current</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">current</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">if</span> <span class="n">errors</span><span class="p">:</span>
        <span class="n">error_msg</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span>
            <span class="s2">&quot;Environment validation failed:&quot;</span><span class="p">,</span>
            <span class="o">*</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;  - </span><span class="si">{</span><span class="n">error</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">error</span> <span class="ow">in</span> <span class="n">errors</span><span class="p">],</span>
            <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Please fix these issues before running tests.&quot;</span>
        <span class="p">])</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="n">error_msg</span><span class="p">,</span> <span class="n">returncode</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_report_header</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add environment information to test report header.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="sa">f</span><span class="s2">&quot;Python: </span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;Platform: </span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">platform</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;Environment: </span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;TEST_ENV&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;development&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">]</span>
</code></pre></div>

<p>Configure environment requirements:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="na">required_env_vars</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span><span class="na">DATABASE_URL</span>
<span class="w">    </span><span class="na">API_KEY</span>
<span class="w">    </span><span class="na">TEST_ENV</span>

<span class="na">required_packages</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span><span class="na">requests</span>
<span class="w">    </span><span class="na">pytest-xdist</span>

<span class="na">min_python_version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">3.8</span>
</code></pre></div>

<p>Run tests without required environment:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_users.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Environment</span><span class="w"> </span><span class="n">validation</span><span class="w"> </span><span class="n">failed</span><span class="p">:</span>
<span class="w">  </span><span class="o">-</span><span class="w"> </span><span class="n">Missing</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="n">variable</span><span class="p">:</span><span class="w"> </span><span class="n">DATABASE_URL</span>
<span class="w">  </span><span class="o">-</span><span class="w"> </span><span class="n">Missing</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="n">variable</span><span class="p">:</span><span class="w"> </span><span class="n">API_KEY</span>
<span class="w">  </span><span class="o">-</span><span class="w"> </span><span class="n">Missing</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="n">variable</span><span class="p">:</span><span class="w"> </span><span class="n">TEST_ENV</span>

<span class="n">Please</span><span class="w"> </span><span class="n">fix</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">issues</span><span class="w"> </span><span class="n">before</span><span class="w"> </span><span class="n">running</span><span class="w"> </span><span class="n">tests</span><span class="o">.</span>
</code></pre></div>

<p>Run tests with proper environment:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span><span class="nv">DATABASE_URL</span><span class="o">=</span>postgres://localhost<span class="w"> </span><span class="nv">API_KEY</span><span class="o">=</span>test-key<span class="w"> </span><span class="nv">TEST_ENV</span><span class="o">=</span>ci<span class="w"> </span>pytest<span class="w"> </span>tests/test_users.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">=============================</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="n">session</span><span class="w"> </span><span class="n">starts</span><span class="w"> </span><span class="o">==============================</span>
<span class="n">platform</span><span class="w"> </span><span class="n">linux</span><span class="w"> </span><span class="o">--</span><span class="w"> </span><span class="n">Python</span><span class="w"> </span><span class="mf">3.11</span><span class="o">.</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">pytest</span><span class="o">-</span><span class="mf">7.4</span><span class="o">.</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">pluggy</span><span class="o">-</span><span class="mf">1.3</span><span class="o">.</span><span class="mi">0</span>
<span class="n">Python</span><span class="p">:</span><span class="w"> </span><span class="mf">3.11</span><span class="o">.</span><span class="mi">0</span>
<span class="n">Platform</span><span class="p">:</span><span class="w"> </span><span class="n">linux</span>
<span class="n">Environment</span><span class="p">:</span><span class="w"> </span><span class="n">ci</span>
<span class="n">cachedir</span><span class="p">:</span><span class="w"> </span><span class="o">.</span><span class="n">pytest_cache</span>
<span class="n">rootdir</span><span class="p">:</span><span class="w"> </span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">user</span><span class="o">/</span><span class="n">project</span>

<span class="n">tests</span><span class="o">/</span><span class="n">test_users</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_users_valid_user</span><span class="w"> </span><span class="n">PASSED</span><span class="w">                     </span><span class="p">[</span><span class="w"> </span><span class="mi">33</span><span class="o">%</span><span class="p">]</span>
<span class="n">tests</span><span class="o">/</span><span class="n">test_users</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_users_invalid_user</span><span class="w"> </span><span class="n">PASSED</span><span class="w">                   </span><span class="p">[</span><span class="w"> </span><span class="mi">66</span><span class="o">%</span><span class="p">]</span>
<span class="n">tests</span><span class="o">/</span><span class="n">test_users</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_explicit_data_loading</span><span class="w"> </span><span class="n">PASSED</span><span class="w">                </span><span class="p">[</span><span class="mi">100</span><span class="o">%</span><span class="p">]</span>

<span class="o">==============================</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">passed</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="mf">0.02</span><span class="n">s</span><span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<h3 id="use-case-3-test-execution-monitoring">Use Case 3: Test Execution Monitoring</h3>
<p><strong>Problem</strong>: Need to track test execution metrics (timing, memory usage, API calls) for performance analysis.</p>
<p><strong>Solution</strong>: A plugin that monitors test execution and generates detailed metrics.</p>
<h4 id="implementation-test-monitor-plugin">Implementation: Test Monitor Plugin</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">psutil</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TestMetrics</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Store test execution metrics.&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tests</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getpid</span><span class="p">())</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">start_test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Record test start metrics.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">,</span>
            <span class="s2">&quot;start_time&quot;</span><span class="p">:</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">(),</span>
            <span class="s2">&quot;start_memory&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span><span class="o">.</span><span class="n">rss</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">,</span>  <span class="c1"># MB</span>
        <span class="p">}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">end_test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">outcome</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Record test end metrics.&quot;&quot;&quot;</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;end_time&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;end_memory&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">process</span><span class="o">.</span><span class="n">memory_info</span><span class="p">()</span><span class="o">.</span><span class="n">rss</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;duration&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;end_time&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;start_time&quot;</span><span class="p">]</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;memory_delta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;end_memory&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;start_memory&quot;</span><span class="p">]</span>
        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;outcome&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outcome</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize metrics tracking.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">test_metrics</span> <span class="o">=</span> <span class="n">TestMetrics</span><span class="p">()</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_protocol</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">nextitem</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Monitor test execution.&quot;&quot;&quot;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">test_metrics</span><span class="o">.</span><span class="n">start_test</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>

    <span class="c1"># Get test outcome</span>
    <span class="n">reports</span> <span class="o">=</span> <span class="p">[</span><span class="n">rep</span> <span class="k">for</span> <span class="n">rep</span> <span class="ow">in</span> <span class="n">item</span><span class="o">.</span><span class="n">stash</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">rep</span><span class="p">,</span> <span class="s1">&#39;outcome&#39;</span><span class="p">)]</span>
    <span class="n">test_outcome</span> <span class="o">=</span> <span class="n">reports</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">outcome</span> <span class="k">if</span> <span class="n">reports</span> <span class="k">else</span> <span class="s2">&quot;unknown&quot;</span>

    <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">test_metrics</span><span class="o">.</span><span class="n">end_test</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">test_outcome</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_terminal_summary</span><span class="p">(</span><span class="n">terminalreporter</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Display metrics summary.&quot;&quot;&quot;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">test_metrics</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">metrics</span><span class="o">.</span><span class="n">tests</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">section</span><span class="p">(</span><span class="s2">&quot;Test Execution Metrics&quot;</span><span class="p">)</span>

    <span class="c1"># Calculate statistics</span>
    <span class="n">total_duration</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="s2">&quot;duration&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">tests</span><span class="p">)</span>
    <span class="n">total_memory</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="s2">&quot;memory_delta&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">tests</span><span class="p">)</span>
    <span class="n">avg_duration</span> <span class="o">=</span> <span class="n">total_duration</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">tests</span><span class="p">)</span>

    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Total tests: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">tests</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Total duration: </span><span class="si">{</span><span class="n">total_duration</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span>
    <span class="p">)</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Average duration: </span><span class="si">{</span><span class="n">avg_duration</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s&quot;</span>
    <span class="p">)</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Total memory delta: </span><span class="si">{</span><span class="n">total_memory</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Show slowest tests</span>
    <span class="n">slowest</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">tests</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;duration&quot;</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span><span class="s2">&quot;Slowest tests:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">slowest</span><span class="p">:</span>
        <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;duration&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s - </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Show memory-intensive tests</span>
    <span class="n">memory_intensive</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">tests</span><span class="p">,</span> 
        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;memory_delta&quot;</span><span class="p">]),</span> 
        <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)[:</span><span class="mi">5</span><span class="p">]</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Memory-intensive tests:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">memory_intensive</span><span class="p">:</span>
        <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;memory_delta&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">+.2f</span><span class="si">}</span><span class="s2"> MB - </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
</code></pre></div>

<p>Run tests with monitoring:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_api_performance.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_api_performance.py::test_fast_operation PASSED             [ 25%]
tests/test_api_performance.py::test_medium_operation PASSED           [ 50%]
tests/test_api_performance.py::test_slow_operation PASSED             [ 75%]
tests/test_api_performance.py::test_very_slow_operation PASSED        [100%]

============================== Test Execution Metrics ==========================

Total tests: 4
Total duration: 3.32s
Average duration: 0.83s
Total memory delta: 0.15 MB

Slowest tests:
  2.00s - tests/test_api_performance.py::test_very_slow_operation
  1.00s - tests/test_api_performance.py::test_slow_operation
  0.30s - tests/test_api_performance.py::test_medium_operation
  0.01s - tests/test_api_performance.py::test_fast_operation

Memory-intensive tests:
  +0.05 MB - tests/test_api_performance.py::test_very_slow_operation
  +0.04 MB - tests/test_api_performance.py::test_slow_operation
  +0.03 MB - tests/test_api_performance.py::test_medium_operation
  +0.03 MB - tests/test_api_performance.py::test_fast_operation

============================== 4 passed in 3.32s ===============================
</code></pre></div>

<h3 id="use-case-4-test-result-notification">Use Case 4: Test Result Notification</h3>
<p><strong>Problem</strong>: Need to notify team members when tests fail in CI/CD, but don't want to parse pytest output manually.</p>
<p><strong>Solution</strong>: A plugin that sends notifications to Slack, email, or other services when tests fail.</p>
<h4 id="implementation-notification-plugin">Implementation: Notification Plugin</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">urllib.request</span><span class="w"> </span><span class="kn">import</span> <span class="n">Request</span><span class="p">,</span> <span class="n">urlopen</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">urllib.error</span><span class="w"> </span><span class="kn">import</span> <span class="n">URLError</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add notification options.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;slack_webhook_url&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Slack webhook URL for notifications&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;notify_on_failure&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;bool&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Send notification when tests fail&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize notification state.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">failed_tests</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">config</span><span class="o">.</span><span class="n">passed_tests</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Track test results.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">==</span> <span class="s2">&quot;call&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">outcome</span> <span class="o">==</span> <span class="s2">&quot;passed&quot;</span><span class="p">:</span>
            <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">passed_tests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">report</span><span class="o">.</span><span class="n">outcome</span> <span class="o">==</span> <span class="s2">&quot;failed&quot;</span><span class="p">:</span>
            <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">failed_tests</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">,</span>
                <span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">report</span><span class="o">.</span><span class="n">longrepr</span><span class="p">)[:</span><span class="mi">200</span><span class="p">]</span>  <span class="c1"># Truncate long errors</span>
            <span class="p">})</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_sessionfinish</span><span class="p">(</span><span class="n">session</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Send notification at end of test session.&quot;&quot;&quot;</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">config</span>

    <span class="c1"># Only notify on failure if configured</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;notify_on_failure&quot;</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">failed_tests</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Get webhook URL</span>
    <span class="n">webhook_url</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;slack_webhook_url&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">webhook_url</span><span class="p">:</span>
        <span class="n">webhook_url</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;SLACK_WEBHOOK_URL&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">webhook_url</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="c1"># Prepare notification message</span>
    <span class="n">total_tests</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">passed_tests</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">failed_tests</span><span class="p">)</span>
    <span class="n">message</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;‚ùå Test Suite Failed&quot;</span><span class="p">,</span>
        <span class="s2">&quot;blocks&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;header&quot;</span><span class="p">,</span>
                <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;plain_text&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;‚ùå Test Suite Failed&quot;</span>
                <span class="p">}</span>
            <span class="p">},</span>
            <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;section&quot;</span><span class="p">,</span>
                <span class="s2">&quot;fields&quot;</span><span class="p">:</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;mrkdwn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;*Total Tests:*</span><span class="se">\n</span><span class="si">{</span><span class="n">total_tests</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">},</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;mrkdwn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;*Failed:*</span><span class="se">\n</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">failed_tests</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">},</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;mrkdwn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;*Passed:*</span><span class="se">\n</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">passed_tests</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">},</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;mrkdwn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;*Environment:*</span><span class="se">\n</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;TEST_ENV&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;unknown&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">}</span>
                <span class="p">]</span>
            <span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>

    <span class="c1"># Add failed test details</span>
    <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">failed_tests</span><span class="p">:</span>
        <span class="n">failed_text</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span>
            <span class="sa">f</span><span class="s2">&quot;‚Ä¢ </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">failed_tests</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>  <span class="c1"># Limit to 5</span>
        <span class="p">])</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">failed_tests</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">failed_text</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">... and </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">failed_tests</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">5</span><span class="si">}</span><span class="s2"> more&quot;</span>

        <span class="n">message</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;section&quot;</span><span class="p">,</span>
            <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;mrkdwn&quot;</span><span class="p">,</span>
                <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;*Failed Tests:*</span><span class="se">\n</span><span class="si">{</span><span class="n">failed_text</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">}</span>
        <span class="p">})</span>

    <span class="c1"># Send notification</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">req</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span>
            <span class="n">webhook_url</span><span class="p">,</span>
            <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">message</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">),</span>
            <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;Content-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json&#39;</span><span class="p">}</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">req</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to send notification: </span><span class="si">{</span><span class="n">response</span><span class="o">.</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">URLError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to send notification: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>Configure notification:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="na">notify_on_failure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">true</span>
<span class="na">slack_webhook_url</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">https://hooks.slack.com/services/YOUR/WEBHOOK/URL</span>
</code></pre></div>

<p>Or use environment variable:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">SLACK_WEBHOOK_URL</span><span class="o">=</span>https://hooks.slack.com/services/YOUR/WEBHOOK/URL
$<span class="w"> </span>pytest<span class="w"> </span>tests/
</code></pre></div>

<p>When tests fail, the team receives a Slack notification with:
- Total test count
- Number of failures
- List of failed tests
- Environment information</p>
<h3 id="use-case-5-test-coverage-enforcement">Use Case 5: Test Coverage Enforcement</h3>
<p><strong>Problem</strong>: Want to enforce minimum coverage thresholds and fail the build if coverage drops below acceptable levels.</p>
<p><strong>Solution</strong>: A plugin that integrates with pytest-cov and enforces coverage rules.</p>
<h4 id="implementation-coverage-enforcer-plugin">Implementation: Coverage Enforcer Plugin</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add coverage enforcement options.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;min_coverage&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;80&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Minimum coverage percentage required&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;coverage_per_file&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;bool&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Enforce coverage per file&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_terminal_summary</span><span class="p">(</span><span class="n">terminalreporter</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Enforce coverage requirements.&quot;&quot;&quot;</span>
    <span class="c1"># Check if pytest-cov is active</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s1">&#39;_cov&#39;</span><span class="p">):</span>
        <span class="k">return</span>

    <span class="n">min_coverage</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;min_coverage&quot;</span><span class="p">))</span>
    <span class="n">per_file</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;coverage_per_file&quot;</span><span class="p">)</span>

    <span class="c1"># Get coverage data</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">_cov</span><span class="o">.</span><span class="n">cov</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">cov</span><span class="o">.</span><span class="n">report</span><span class="p">(</span><span class="n">file</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">section</span><span class="p">(</span><span class="s2">&quot;Coverage Enforcement&quot;</span><span class="p">)</span>

    <span class="c1"># Check total coverage</span>
    <span class="k">if</span> <span class="n">total</span> <span class="o">&lt;</span> <span class="n">min_coverage</span><span class="p">:</span>
        <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;‚ùå Total coverage </span><span class="si">{</span><span class="n">total</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% is below minimum </span><span class="si">{</span><span class="n">min_coverage</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">,</span>
            <span class="n">red</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Coverage </span><span class="si">{</span><span class="n">total</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% is below minimum </span><span class="si">{</span><span class="n">min_coverage</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">,</span>
            <span class="n">returncode</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;‚úì Total coverage </span><span class="si">{</span><span class="n">total</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% meets minimum </span><span class="si">{</span><span class="n">min_coverage</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">,</span>
            <span class="n">green</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="c1"># Check per-file coverage if enabled</span>
    <span class="k">if</span> <span class="n">per_file</span><span class="p">:</span>
        <span class="n">files_below_threshold</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">cov</span><span class="o">.</span><span class="n">get_data</span><span class="p">()</span><span class="o">.</span><span class="n">measured_files</span><span class="p">():</span>
            <span class="n">analysis</span> <span class="o">=</span> <span class="n">cov</span><span class="o">.</span><span class="n">analysis</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
            <span class="n">file_coverage</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">analysis</span><span class="o">.</span><span class="n">executed</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">analysis</span><span class="o">.</span><span class="n">statements</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
                <span class="k">if</span> <span class="n">analysis</span><span class="o">.</span><span class="n">statements</span> <span class="k">else</span> <span class="mi">100</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">file_coverage</span> <span class="o">&lt;</span> <span class="n">min_coverage</span><span class="p">:</span>
                <span class="n">files_below_threshold</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">filename</span><span class="p">,</span> <span class="n">file_coverage</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">files_below_threshold</span><span class="p">:</span>
            <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">‚ùå </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">files_below_threshold</span><span class="p">)</span><span class="si">}</span><span class="s2"> file(s) below </span><span class="si">{</span><span class="n">min_coverage</span><span class="si">}</span><span class="s2">% coverage:&quot;</span><span class="p">,</span>
                <span class="n">red</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">filename</span><span class="p">,</span> <span class="n">coverage</span> <span class="ow">in</span> <span class="n">files_below_threshold</span><span class="p">:</span>
                <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">coverage</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">% - </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="n">pytest</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">files_below_threshold</span><span class="p">)</span><span class="si">}</span><span class="s2"> files below coverage threshold&quot;</span><span class="p">,</span>
                <span class="n">returncode</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>
</code></pre></div>

<p>Configure coverage enforcement:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest.ini</span>
<span class="k">[pytest]</span>
<span class="na">min_coverage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">80</span>
<span class="na">coverage_per_file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">true</span>
<span class="na">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">--cov=src --cov-report=term-missing</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong> (if coverage is below threshold):</p>
<div class="codehilite"><pre><span></span><code>============================== Coverage Enforcement ============================
‚ùå Total coverage 65.3% is below minimum 80.0%

Coverage 65.3% is below minimum 80.0%
</code></pre></div>

<p><strong>Output</strong> (if coverage meets threshold):</p>
<div class="codehilite"><pre><span></span><code>============================== Coverage Enforcement ============================
‚úì Total coverage 85.7% meets minimum 80.0%

============================== 10 passed in 2.15s ===============================
</code></pre></div>

<h2 id="decision-framework-when-to-create-a-plugin">Decision Framework: When to Create a Plugin</h2>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Plugin?</th>
<th>Alternative</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Reusable across projects</strong></td>
<td>‚úÖ Yes</td>
<td>-</td>
</tr>
<tr>
<td><strong>Project-specific logic</strong></td>
<td>‚úÖ Yes (local)</td>
<td>conftest.py</td>
</tr>
<tr>
<td><strong>One-time use</strong></td>
<td>‚ùå No</td>
<td>Test helper function</td>
</tr>
<tr>
<td><strong>Modifies pytest behavior</strong></td>
<td>‚úÖ Yes</td>
<td>-</td>
</tr>
<tr>
<td><strong>Simple fixture</strong></td>
<td>‚ùå No</td>
<td>Regular fixture</td>
</tr>
<tr>
<td><strong>Complex workflow</strong></td>
<td>‚úÖ Yes</td>
<td>-</td>
</tr>
<tr>
<td><strong>Team needs it</strong></td>
<td>‚úÖ Yes</td>
<td>-</td>
</tr>
</tbody>
</table>
<h2 id="common-plugin-patterns-summary">Common Plugin Patterns Summary</h2>
<table>
<thead>
<tr>
<th>Pattern</th>
<th>Use Case</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Auto-fixture</strong></td>
<td>Automatic test data injection</td>
<td>Test data management</td>
</tr>
<tr>
<td><strong>Validator</strong></td>
<td>Pre-test environment checks</td>
<td>Environment validation</td>
</tr>
<tr>
<td><strong>Monitor</strong></td>
<td>Track execution metrics</td>
<td>Performance monitoring</td>
</tr>
<tr>
<td><strong>Notifier</strong></td>
<td>External integrations</td>
<td>Slack notifications</td>
</tr>
<tr>
<td><strong>Enforcer</strong></td>
<td>Quality gates</td>
<td>Coverage enforcement</td>
</tr>
</tbody>
</table>
<h2 id="the-journey-from-manual-to-automated">The Journey: From Manual to Automated</h2>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Approach</th>
<th>Efficiency</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Manual test data setup</td>
<td>Low</td>
</tr>
<tr>
<td>1</td>
<td>Shared fixtures</td>
<td>Medium</td>
</tr>
<tr>
<td>2</td>
<td>Auto-injection plugin</td>
<td>High</td>
</tr>
<tr>
<td>3</td>
<td>Environment validation</td>
<td>Higher</td>
</tr>
<tr>
<td>4</td>
<td>Monitoring + notifications</td>
<td>Highest</td>
</tr>
</tbody>
</table>
<h3 id="lessons-learned_4">Lessons Learned</h3>
<ol>
<li><strong>Plugins solve repetitive problems</strong>: If you're doing something manually in every test, consider a plugin</li>
<li><strong>Start local, distribute later</strong>: Begin with conftest.py, extract to package when needed</li>
<li><strong>Configuration is key</strong>: Make plugins configurable for different environments</li>
<li><strong>Fail fast</strong>: Validate environment before running tests</li>
<li><strong>Provide visibility</strong>: Add custom terminal sections for important information</li>
<li><strong>Integrate with tools</strong>: Plugins can bridge pytest with external services</li>
<li><strong>Think about the team</strong>: Plugins should make everyone's life easier</li>
</ol>
<h2 id="distributing-your-own-plugin">Distributing Your Own Plugin</h2>
<h2 id="distributing-your-own-plugin_1">Distributing Your Own Plugin</h2>
<p>You've created a useful plugin that solves a real problem. Now you want to share it with the community or use it across multiple projects. Let's learn how to package and distribute a pytest plugin properly.</p>
<h3 id="phase-1-from-conftestpy-to-package">Phase 1: From conftest.py to Package</h3>
<p>Our slow test detection plugin has been working well in <code>conftest.py</code>. Let's transform it into a distributable package.</p>
<h4 id="step-1-create-package-structure">Step 1: Create Package Structure</h4>
<p>First, create a proper package structure:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>mkdir<span class="w"> </span>pytest-slow-detector
$<span class="w"> </span><span class="nb">cd</span><span class="w"> </span>pytest-slow-detector
$<span class="w"> </span>mkdir<span class="w"> </span>pytest_slow_detector
$<span class="w"> </span>touch<span class="w"> </span>pytest_slow_detector/__init__.py
$<span class="w"> </span>touch<span class="w"> </span>pytest_slow_detector/plugin.py
$<span class="w"> </span>touch<span class="w"> </span>setup.py
$<span class="w"> </span>touch<span class="w"> </span>README.md
$<span class="w"> </span>touch<span class="w"> </span>LICENSE
</code></pre></div>

<p><strong>Directory structure</strong>:</p>
<div class="codehilite"><pre><span></span><code>pytest-slow-detector/
‚îú‚îÄ‚îÄ pytest_slow_detector/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ plugin.py
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ test_plugin.py
</code></pre></div>

<h4 id="step-2-move-plugin-code">Step 2: Move Plugin Code</h4>
<p>Move the plugin code from <code>conftest.py</code> to <code>plugin.py</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest_slow_detector/plugin.py</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Pytest plugin for automatic slow test detection and reporting.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Any</span>

<span class="n">__version__</span> <span class="o">=</span> <span class="s2">&quot;0.1.0&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom command-line options and ini-file values.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;slow_threshold&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;0.5&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Threshold in seconds for marking tests as slow (default: 0.5)&quot;</span>
    <span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addini</span><span class="p">(</span>
        <span class="s2">&quot;show_slow_summary&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;bool&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Show summary of slow tests at the end (default: False)&quot;</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">addoption</span><span class="p">(</span>
        <span class="s2">&quot;--slow-threshold&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Override slow_threshold from config file&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_configure</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Register custom markers and initialize plugin state.&quot;&quot;&quot;</span>
    <span class="n">config</span><span class="o">.</span><span class="n">addinivalue_line</span><span class="p">(</span>
        <span class="s2">&quot;markers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;slow: marks tests as slow (automatically applied based on duration)&quot;</span>
    <span class="p">)</span>

    <span class="c1"># Get threshold from command-line or config file</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getoption</span><span class="p">(</span><span class="s2">&quot;--slow-threshold&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">threshold</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">threshold</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;slow_threshold&quot;</span><span class="p">))</span>
        <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">):</span>
            <span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.5</span>

    <span class="k">if</span> <span class="n">threshold</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">pytest</span><span class="o">.</span><span class="n">UsageError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;slow_threshold must be non-negative, got </span><span class="si">{</span><span class="n">threshold</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span> <span class="o">=</span> <span class="n">threshold</span>
    <span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">hookimpl</span><span class="p">(</span><span class="n">tryfirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">hookwrapper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_makereport</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">call</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Automatically mark slow tests and collect information.&quot;&quot;&quot;</span>
    <span class="n">outcome</span> <span class="o">=</span> <span class="k">yield</span>
    <span class="n">report</span> <span class="o">=</span> <span class="n">outcome</span><span class="o">.</span><span class="n">get_result</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">when</span> <span class="o">!=</span> <span class="s2">&quot;call&quot;</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">threshold</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span>

    <span class="k">if</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="n">item</span><span class="o">.</span><span class="n">add_marker</span><span class="p">(</span><span class="n">pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">slow</span><span class="p">)</span>
        <span class="n">item</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
            <span class="s2">&quot;nodeid&quot;</span><span class="p">:</span> <span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="p">,</span>
            <span class="s2">&quot;duration&quot;</span><span class="p">:</span> <span class="n">report</span><span class="o">.</span><span class="n">duration</span><span class="p">,</span>
            <span class="s2">&quot;threshold&quot;</span><span class="p">:</span> <span class="n">threshold</span><span class="p">,</span>
            <span class="s2">&quot;outcome&quot;</span><span class="p">:</span> <span class="n">report</span><span class="o">.</span><span class="n">outcome</span>
        <span class="p">})</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_terminal_summary</span><span class="p">(</span><span class="n">terminalreporter</span><span class="p">,</span> <span class="n">exitstatus</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add custom section to terminal summary showing slow tests.&quot;&quot;&quot;</span>
    <span class="n">show_summary</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">getini</span><span class="p">(</span><span class="s2">&quot;show_slow_summary&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">show_summary</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">slow_tests</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">slow_tests</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">slow_tests</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">slow_tests</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;duration&quot;</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">section</span><span class="p">(</span><span class="s2">&quot;Slow Tests Summary&quot;</span><span class="p">)</span>
    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">slow_tests</span><span class="p">)</span><span class="si">}</span><span class="s2"> test(s) exceeding &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">slow_threshold</span><span class="si">}</span><span class="s2">s threshold:</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">slow_tests</span><span class="p">:</span>
        <span class="n">status_symbol</span> <span class="o">=</span> <span class="s2">&quot;‚úì&quot;</span> <span class="k">if</span> <span class="n">test</span><span class="p">[</span><span class="s2">&quot;outcome&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;passed&quot;</span> <span class="k">else</span> <span class="s2">&quot;‚úó&quot;</span>
        <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">status_symbol</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;duration&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">s - </span><span class="si">{</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;nodeid&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">terminalreporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Tip: Run &#39;pytest -m slow&#39; to run only slow tests&quot;</span>
    <span class="p">)</span>
</code></pre></div>

<h4 id="step-3-create-package-entry-point">Step 3: Create Package Entry Point</h4>
<p>Update <code>__init__.py</code> to expose the plugin:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pytest_slow_detector/__init__.py</span>
<span class="sd">&quot;&quot;&quot;Pytest plugin for automatic slow test detection.&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.plugin</span><span class="w"> </span><span class="kn">import</span> <span class="n">__version__</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;__version__&quot;</span><span class="p">]</span>
</code></pre></div>

<h4 id="step-4-create-setuppy">Step 4: Create setup.py</h4>
<p>The <code>setup.py</code> file is crucial‚Äîit tells pip how to install your plugin and registers it with pytest:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># setup.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">setuptools</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">find_packages</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;README.md&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fh</span><span class="p">:</span>
    <span class="n">long_description</span> <span class="o">=</span> <span class="n">fh</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="n">setup</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;pytest-slow-detector&quot;</span><span class="p">,</span>
    <span class="n">version</span><span class="o">=</span><span class="s2">&quot;0.1.0&quot;</span><span class="p">,</span>
    <span class="n">author</span><span class="o">=</span><span class="s2">&quot;Your Name&quot;</span><span class="p">,</span>
    <span class="n">author_email</span><span class="o">=</span><span class="s2">&quot;your.email@example.com&quot;</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Pytest plugin for automatic slow test detection&quot;</span><span class="p">,</span>
    <span class="n">long_description</span><span class="o">=</span><span class="n">long_description</span><span class="p">,</span>
    <span class="n">long_description_content_type</span><span class="o">=</span><span class="s2">&quot;text/markdown&quot;</span><span class="p">,</span>
    <span class="n">url</span><span class="o">=</span><span class="s2">&quot;https://github.com/yourusername/pytest-slow-detector&quot;</span><span class="p">,</span>
    <span class="n">packages</span><span class="o">=</span><span class="n">find_packages</span><span class="p">(),</span>
    <span class="n">classifiers</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;Development Status :: 3 - Alpha&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Framework :: Pytest&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Intended Audience :: Developers&quot;</span><span class="p">,</span>
        <span class="s2">&quot;License :: OSI Approved :: MIT License&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Operating System :: OS Independent&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Programming Language :: Python :: 3&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Programming Language :: Python :: 3.8&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Programming Language :: Python :: 3.9&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Programming Language :: Python :: 3.10&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Programming Language :: Python :: 3.11&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Topic :: Software Development :: Testing&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">python_requires</span><span class="o">=</span><span class="s2">&quot;&gt;=3.8&quot;</span><span class="p">,</span>
    <span class="n">install_requires</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;pytest&gt;=7.0.0&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="c1"># This is the critical part - registers the plugin with pytest</span>
    <span class="n">entry_points</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;pytest11&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="s2">&quot;slow_detector = pytest_slow_detector.plugin&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">},</span>
<span class="p">)</span>
</code></pre></div>

<p><strong>Key elements</strong>:</p>
<ol>
<li><strong>entry_points</strong>: Registers the plugin with pytest using the <code>pytest11</code> entry point</li>
<li><strong>install_requires</strong>: Lists pytest as a dependency</li>
<li><strong>classifiers</strong>: Helps users find your plugin on PyPI</li>
<li><strong>python_requires</strong>: Specifies minimum Python version</li>
</ol>
<h3 id="phase-2-testing-your-plugin">Phase 2: Testing Your Plugin</h3>
<p>Before distributing, thoroughly test your plugin:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_plugin.py</span>
<span class="sd">&quot;&quot;&quot;Tests for pytest-slow-detector plugin.&quot;&quot;&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="n">pytest_plugins</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;pytest_slow_detector.plugin&quot;</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_slow_marker_applied</span><span class="p">(</span><span class="n">testdir</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that slow marker is automatically applied.&quot;&quot;&quot;</span>
    <span class="n">testdir</span><span class="o">.</span><span class="n">makepyfile</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        import time</span>

<span class="s2">        def test_fast():</span>
<span class="s2">            assert True</span>

<span class="s2">        def test_slow():</span>
<span class="s2">            time.sleep(0.6)</span>
<span class="s2">            assert True</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>

    <span class="n">testdir</span><span class="o">.</span><span class="n">makeini</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        [pytest]</span>
<span class="s2">        slow_threshold = 0.5</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">testdir</span><span class="o">.</span><span class="n">runpytest</span><span class="p">(</span><span class="s2">&quot;-v&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;slow&quot;</span><span class="p">)</span>
    <span class="n">result</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">fnmatch_lines</span><span class="p">([</span>
        <span class="s2">&quot;*test_slow PASSED*&quot;</span><span class="p">,</span>
    <span class="p">])</span>
    <span class="k">assert</span> <span class="n">result</span><span class="o">.</span><span class="n">ret</span> <span class="o">==</span> <span class="mi">0</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_custom_threshold</span><span class="p">(</span><span class="n">testdir</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test custom threshold configuration.&quot;&quot;&quot;</span>
    <span class="n">testdir</span><span class="o">.</span><span class="n">makepyfile</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        import time</span>

<span class="s2">        def test_medium():</span>
<span class="s2">            time.sleep(0.8)</span>
<span class="s2">            assert True</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">testdir</span><span class="o">.</span><span class="n">runpytest</span><span class="p">(</span><span class="s2">&quot;-v&quot;</span><span class="p">,</span> <span class="s2">&quot;--slow-threshold=1.0&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;slow&quot;</span><span class="p">)</span>
    <span class="c1"># Should not match because 0.8s &lt; 1.0s threshold</span>
    <span class="k">assert</span> <span class="s2">&quot;test_medium&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">str</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_summary_display</span><span class="p">(</span><span class="n">testdir</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that summary is displayed when configured.&quot;&quot;&quot;</span>
    <span class="n">testdir</span><span class="o">.</span><span class="n">makepyfile</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        import time</span>

<span class="s2">        def test_slow():</span>
<span class="s2">            time.sleep(0.6)</span>
<span class="s2">            assert True</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>

    <span class="n">testdir</span><span class="o">.</span><span class="n">makeini</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        [pytest]</span>
<span class="s2">        slow_threshold = 0.5</span>
<span class="s2">        show_slow_summary = true</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">testdir</span><span class="o">.</span><span class="n">runpytest</span><span class="p">(</span><span class="s2">&quot;-v&quot;</span><span class="p">)</span>
    <span class="n">result</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">fnmatch_lines</span><span class="p">([</span>
        <span class="s2">&quot;*Slow Tests Summary*&quot;</span><span class="p">,</span>
        <span class="s2">&quot;*Found 1 test(s) exceeding 0.5s threshold*&quot;</span><span class="p">,</span>
    <span class="p">])</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_negative_threshold_error</span><span class="p">(</span><span class="n">testdir</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test that negative threshold raises error.&quot;&quot;&quot;</span>
    <span class="n">testdir</span><span class="o">.</span><span class="n">makepyfile</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        def test_anything():</span>
<span class="s2">            assert True</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">testdir</span><span class="o">.</span><span class="n">runpytest</span><span class="p">(</span><span class="s2">&quot;--slow-threshold=-1.0&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">result</span><span class="o">.</span><span class="n">ret</span> <span class="o">!=</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="s2">&quot;slow_threshold must be non-negative&quot;</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">str</span><span class="p">()</span>
</code></pre></div>

<p>Run the plugin tests:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>tests/test_plugin.py<span class="w"> </span>-v
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code>tests/test_plugin.py::test_slow_marker_applied PASSED                 [ 25%]
tests/test_plugin.py::test_custom_threshold PASSED                    [ 50%]
tests/test_plugin.py::test_summary_display PASSED                     [ 75%]
tests/test_plugin.py::test_negative_threshold_error PASSED            [100%]

============================== 4 passed in 1.23s ===============================
</code></pre></div>

<h3 id="phase-3-documentation">Phase 3: Documentation</h3>
<p>Create comprehensive documentation in <code>README.md</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="gh"># pytest-slow-detector</span>

Pytest plugin for automatic slow test detection and reporting.

<span class="gu">## Features</span>

<span class="k">-</span><span class="w"> </span>Automatically marks tests exceeding a configurable duration threshold
<span class="k">-</span><span class="w"> </span>Provides filtering capabilities with <span class="sb">`-m slow`</span> and <span class="sb">`-m &quot;not slow&quot;`</span>
<span class="k">-</span><span class="w"> </span>Generates summary report of slow tests
<span class="k">-</span><span class="w"> </span>Configurable via pytest.ini or command-line

<span class="gu">## Installation</span>

```bash
pip install pytest-slow-detector
</code></pre></div>

<h2 id="usage">Usage</h2>
<h3 id="basic-usage">Basic Usage</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># Run all tests and see slow test summary</span>
pytest<span class="w"> </span>--show-slow-summary

<span class="c1"># Run only slow tests</span>
pytest<span class="w"> </span>-m<span class="w"> </span>slow

<span class="c1"># Skip slow tests</span>
pytest<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;not slow&quot;</span>
</code></pre></div>

<h3 id="configuration">Configuration</h3>
<p>Configure in <code>pytest.ini</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="k">[pytest]</span>
<span class="na">slow_threshold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0.5</span><span class="w">  </span><span class="c1"># seconds</span>
<span class="na">show_slow_summary</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">true</span>
</code></pre></div>

<p>Or use command-line:</p>
<div class="codehilite"><pre><span></span><code>pytest<span class="w"> </span>--slow-threshold<span class="o">=</span><span class="m">1</span>.0
</code></pre></div>

<h2 id="examples">Examples</h2>
<h3 id="example-1-identify-slow-tests">Example 1: Identify Slow Tests</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_api.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_fast_endpoint</span><span class="p">():</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;/fast&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_slow_endpoint</span><span class="p">():</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;/slow&quot;</span><span class="p">)</span>  <span class="c1"># Takes 2 seconds</span>
    <span class="k">assert</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span>
</code></pre></div>

<p>Run with summary:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>--show-slow-summary

<span class="o">==============================</span><span class="w"> </span>Slow<span class="w"> </span>Tests<span class="w"> </span><span class="nv">Summary</span><span class="w"> </span><span class="o">==============================</span>
Found<span class="w"> </span><span class="m">1</span><span class="w"> </span>test<span class="o">(</span>s<span class="o">)</span><span class="w"> </span>exceeding<span class="w"> </span><span class="m">0</span>.5s<span class="w"> </span>threshold:

<span class="w">  </span>‚úì<span class="w"> </span><span class="m">2</span>.00s<span class="w"> </span>-<span class="w"> </span>tests/test_api.py::test_slow_endpoint

Tip:<span class="w"> </span>Run<span class="w"> </span><span class="s1">&#39;pytest -m slow&#39;</span><span class="w"> </span>to<span class="w"> </span>run<span class="w"> </span>only<span class="w"> </span>slow<span class="w"> </span>tests
</code></pre></div>

<h3 id="example-2-skip-slow-tests-in-ci">Example 2: Skip Slow Tests in CI</h3>
<div class="codehilite"><pre><span></span><code><span class="c1"># In CI pipeline, skip slow tests for fast feedback</span>
pytest<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;not slow&quot;</span>
</code></pre></div>

<h2 id="configuration-options">Configuration Options</h2>
<table>
<thead>
<tr>
<th>Option</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>slow_threshold</code></td>
<td>float</td>
<td>0.5</td>
<td>Threshold in seconds for marking tests as slow</td>
</tr>
<tr>
<td><code>show_slow_summary</code></td>
<td>bool</td>
<td>false</td>
<td>Show summary of slow tests at the end</td>
</tr>
<tr>
<td><code>--slow-threshold</code></td>
<td>float</td>
<td>-</td>
<td>Override threshold from command-line</td>
</tr>
</tbody>
</table>
<h2 id="requirements">Requirements</h2>
<ul>
<li>Python 3.8+</li>
<li>pytest 7.0.0+</li>
</ul>
<h2 id="license">License</h2>
<p>MIT License - see LICENSE file for details.</p>
<h2 id="contributing">Contributing</h2>
<p>Contributions welcome! Please open an issue or submit a pull request.</p>
<h2 id="changelog">Changelog</h2>
<h3 id="010-2024-01-15">0.1.0 (2024-01-15)</h3>
<ul>
<li>Initial release</li>
<li>Automatic slow test detection</li>
<li>Summary reporting</li>
<li>Configurable threshold</li>
</ul>
<div class="codehilite"><pre><span></span><code><span class="gu">##</span># Phase 4: Local Installation and Testing

Before publishing, test the installation locally:

```bash
<span class="gh">#</span> Install in development mode
$ pip install -e .

<span class="gh">#</span> Verify installation
$ pip list | grep pytest-slow-detector
pytest-slow-detector    0.1.0    /path/to/pytest-slow-detector

<span class="gh">#</span> Test in a real project
$ cd /path/to/your/project
$ pytest --help | grep slow
  --slow-threshold=SLOW_THRESHOLD
                        Override slow_threshold from config file
</code></pre></div>

<h3 id="phase-5-publishing-to-pypi">Phase 5: Publishing to PyPI</h3>
<h4 id="step-1-create-pypi-account">Step 1: Create PyPI Account</h4>
<ol>
<li>Go to https://pypi.org and create an account</li>
<li>Verify your email</li>
<li>Enable two-factor authentication (recommended)</li>
</ol>
<h4 id="step-2-install-build-tools">Step 2: Install Build Tools</h4>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>build<span class="w"> </span>twine
</code></pre></div>

<h4 id="step-3-build-distribution">Step 3: Build Distribution</h4>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>build
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="o">*</span><span class="w"> </span><span class="n">Creating</span><span class="w"> </span><span class="n">venv</span><span class="w"> </span><span class="n">isolated</span><span class="w"> </span><span class="n">environment</span><span class="o">...</span>
<span class="o">*</span><span class="w"> </span><span class="n">Installing</span><span class="w"> </span><span class="n">packages</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">isolated</span><span class="w"> </span><span class="n">environment</span><span class="o">...</span><span class="w"> </span><span class="p">(</span><span class="n">setuptools</span><span class="o">&gt;=</span><span class="mf">40.8</span><span class="o">.</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">wheel</span><span class="p">)</span>
<span class="o">*</span><span class="w"> </span><span class="n">Getting</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="n">dependencies</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">sdist</span><span class="o">...</span>
<span class="o">*</span><span class="w"> </span><span class="n">Building</span><span class="w"> </span><span class="n">sdist</span><span class="o">...</span>
<span class="o">*</span><span class="w"> </span><span class="n">Building</span><span class="w"> </span><span class="n">wheel</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">sdist</span>
<span class="o">*</span><span class="w"> </span><span class="n">Creating</span><span class="w"> </span><span class="n">venv</span><span class="w"> </span><span class="n">isolated</span><span class="w"> </span><span class="n">environment</span><span class="o">...</span>
<span class="o">*</span><span class="w"> </span><span class="n">Installing</span><span class="w"> </span><span class="n">packages</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">isolated</span><span class="w"> </span><span class="n">environment</span><span class="o">...</span><span class="w"> </span><span class="p">(</span><span class="n">setuptools</span><span class="o">&gt;=</span><span class="mf">40.8</span><span class="o">.</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">wheel</span><span class="p">)</span>
<span class="o">*</span><span class="w"> </span><span class="n">Getting</span><span class="w"> </span><span class="n">build</span><span class="w"> </span><span class="n">dependencies</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">wheel</span><span class="o">...</span>
<span class="o">*</span><span class="w"> </span><span class="n">Building</span><span class="w"> </span><span class="n">wheel</span><span class="o">...</span>
<span class="n">Successfully</span><span class="w"> </span><span class="n">built</span><span class="w"> </span><span class="n">pytest</span><span class="o">-</span><span class="n">slow</span><span class="o">-</span><span class="n">detector</span><span class="o">-</span><span class="mf">0.1</span><span class="o">.</span><span class="mf">0.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">pytest_slow_detector</span><span class="o">-</span><span class="mf">0.1</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="n">py3</span><span class="o">-</span><span class="n">none</span><span class="o">-</span><span class="n">any</span><span class="o">.</span><span class="n">whl</span>
</code></pre></div>

<p>This creates two files in the <code>dist/</code> directory:
- <code>pytest-slow-detector-0.1.0.tar.gz</code> (source distribution)
- <code>pytest_slow_detector-0.1.0-py3-none-any.whl</code> (wheel distribution)</p>
<h4 id="step-4-test-on-testpypi-first">Step 4: Test on TestPyPI First</h4>
<p>Before publishing to the real PyPI, test on TestPyPI:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Upload to TestPyPI</span>
$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>twine<span class="w"> </span>upload<span class="w"> </span>--repository<span class="w"> </span>testpypi<span class="w"> </span>dist/*

<span class="c1"># Test installation from TestPyPI</span>
$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--index-url<span class="w"> </span>https://test.pypi.org/simple/<span class="w"> </span>pytest-slow-detector
</code></pre></div>

<h4 id="step-5-publish-to-pypi">Step 5: Publish to PyPI</h4>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>twine<span class="w"> </span>upload<span class="w"> </span>dist/*
</code></pre></div>

<p><strong>Output</strong>:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Uploading</span><span class="w"> </span><span class="n">distributions</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">upload</span><span class="o">.</span><span class="n">pypi</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">legacy</span><span class="o">/</span>
<span class="n">Enter</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">username</span><span class="p">:</span><span class="w"> </span><span class="n">yourusername</span>
<span class="n">Enter</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">password</span><span class="p">:</span><span class="w"> </span>
<span class="n">Uploading</span><span class="w"> </span><span class="n">pytest_slow_detector</span><span class="o">-</span><span class="mf">0.1</span><span class="o">.</span><span class="mi">0</span><span class="o">-</span><span class="n">py3</span><span class="o">-</span><span class="n">none</span><span class="o">-</span><span class="n">any</span><span class="o">.</span><span class="n">whl</span>
<span class="mi">100</span><span class="o">%</span><span class="w"> </span><span class="err">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span><span class="w"> </span><span class="mf">12.3</span><span class="o">/</span><span class="mf">12.3</span><span class="w"> </span><span class="n">kB</span><span class="w"> </span><span class="err">‚Ä¢</span><span class="w"> </span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="w"> </span><span class="err">‚Ä¢</span><span class="w"> </span><span class="err">?</span>
<span class="n">Uploading</span><span class="w"> </span><span class="n">pytest</span><span class="o">-</span><span class="n">slow</span><span class="o">-</span><span class="n">detector</span><span class="o">-</span><span class="mf">0.1</span><span class="o">.</span><span class="mf">0.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>
<span class="mi">100</span><span class="o">%</span><span class="w"> </span><span class="err">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span><span class="w"> </span><span class="mf">8.9</span><span class="o">/</span><span class="mf">8.9</span><span class="w"> </span><span class="n">kB</span><span class="w"> </span><span class="err">‚Ä¢</span><span class="w"> </span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="w"> </span><span class="err">‚Ä¢</span><span class="w"> </span><span class="err">?</span>

<span class="n">View</span><span class="w"> </span><span class="n">at</span><span class="p">:</span>
<span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">pypi</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">project</span><span class="o">/</span><span class="n">pytest</span><span class="o">-</span><span class="n">slow</span><span class="o">-</span><span class="n">detector</span><span class="o">/</span><span class="mf">0.1</span><span class="o">.</span><span class="mi">0</span><span class="o">/</span>
</code></pre></div>

<p>Your plugin is now publicly available!</p>
<h3 id="phase-6-maintenance-and-updates">Phase 6: Maintenance and Updates</h3>
<h4 id="versioning-strategy">Versioning Strategy</h4>
<p>Follow semantic versioning (MAJOR.MINOR.PATCH):</p>
<ul>
<li><strong>MAJOR</strong>: Breaking changes (e.g., 1.0.0 ‚Üí 2.0.0)</li>
<li><strong>MINOR</strong>: New features, backward compatible (e.g., 1.0.0 ‚Üí 1.1.0)</li>
<li><strong>PATCH</strong>: Bug fixes (e.g., 1.0.0 ‚Üí 1.0.1)</li>
</ul>
<h4 id="releasing-updates">Releasing Updates</h4>
<div class="codehilite"><pre><span></span><code><span class="c1"># 1. Update version in setup.py and plugin.py</span>
<span class="c1"># 2. Update CHANGELOG.md</span>
<span class="c1"># 3. Commit changes</span>
$<span class="w"> </span>git<span class="w"> </span>add<span class="w"> </span>.
$<span class="w"> </span>git<span class="w"> </span>commit<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;Release version 0.2.0&quot;</span>
$<span class="w"> </span>git<span class="w"> </span>tag<span class="w"> </span>v0.2.0
$<span class="w"> </span>git<span class="w"> </span>push<span class="w"> </span>origin<span class="w"> </span>main<span class="w"> </span>--tags

<span class="c1"># 4. Build and upload</span>
$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>build
$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>twine<span class="w"> </span>upload<span class="w"> </span>dist/*
</code></pre></div>

<h3 id="modern-alternative-pyprojecttoml">Modern Alternative: pyproject.toml</h3>
<p>For modern Python projects, use <code>pyproject.toml</code> instead of <code>setup.py</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># pyproject.toml</span>
<span class="k">[build-system]</span>
<span class="n">requires</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;setuptools&gt;=61.0&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;wheel&quot;</span><span class="p">]</span>
<span class="n">build-backend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;setuptools.build_meta&quot;</span>

<span class="k">[project]</span>
<span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;pytest-slow-detector&quot;</span>
<span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;0.1.0&quot;</span>
<span class="n">description</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;Pytest plugin for automatic slow test detection&quot;</span>
<span class="n">readme</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;README.md&quot;</span>
<span class="n">authors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="p">{</span><span class="n">name</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s2">&quot;Your Name&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">email</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s2">&quot;your.email@example.com&quot;</span><span class="p">}</span>
<span class="p">]</span>
<span class="n">license</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="n">text</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s2">&quot;MIT&quot;</span><span class="p">}</span>
<span class="n">classifiers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;Development Status :: 3 - Alpha&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Framework :: Pytest&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Intended Audience :: Developers&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;License :: OSI Approved :: MIT License&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Operating System :: OS Independent&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Programming Language :: Python :: 3&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Programming Language :: Python :: 3.8&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Programming Language :: Python :: 3.9&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Programming Language :: Python :: 3.10&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Programming Language :: Python :: 3.11&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Topic :: Software Development :: Testing&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">requires-python</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&gt;=3.8&quot;</span>
<span class="n">dependencies</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;pytest&gt;=7.0.0&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">[project.urls]</span>
<span class="n">Homepage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;https://github.com/yourusername/pytest-slow-detector&quot;</span>
<span class="n">Documentation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;https://github.com/yourusername/pytest-slow-detector#readme&quot;</span>
<span class="n">Repository</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;https://github.com/yourusername/pytest-slow-detector&quot;</span>
<span class="n">Issues</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;https://github.com/yourusername/pytest-slow-detector/issues&quot;</span>

<span class="k">[project.entry-points.pytest11]</span>
<span class="n">slow_detector</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;pytest_slow_detector.plugin&quot;</span>

<span class="k">[tool.setuptools.packages.find]</span>
<span class="n">where</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span>
<span class="n">include</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;pytest_slow_detector*&quot;</span><span class="p">]</span>
</code></pre></div>

<h3 id="plugin-distribution-checklist">Plugin Distribution Checklist</h3>
<p>Before publishing your plugin, ensure:</p>
<ul>
<li>[ ] <strong>Code quality</strong></li>
<li>[ ] All tests pass</li>
<li>[ ] Code is well-documented</li>
<li>[ ] No hardcoded values</li>
<li>
<p>[ ] Error handling is robust</p>
</li>
<li>
<p>[ ] <strong>Documentation</strong></p>
</li>
<li>[ ] README.md with examples</li>
<li>[ ] Installation instructions</li>
<li>[ ] Configuration options documented</li>
<li>
<p>[ ] Changelog maintained</p>
</li>
<li>
<p>[ ] <strong>Package metadata</strong></p>
</li>
<li>[ ] Correct version number</li>
<li>[ ] Accurate dependencies</li>
<li>[ ] Proper classifiers</li>
<li>
<p>[ ] License file included</p>
</li>
<li>
<p>[ ] <strong>Testing</strong></p>
</li>
<li>[ ] Plugin tests pass</li>
<li>[ ] Tested in real projects</li>
<li>[ ] Tested on TestPyPI</li>
<li>
<p>[ ] Multiple Python versions tested</p>
</li>
<li>
<p>[ ] <strong>Repository</strong></p>
</li>
<li>[ ] Code on GitHub/GitLab</li>
<li>[ ] Issues enabled</li>
<li>[ ] Contributing guidelines</li>
<li>[ ] CI/CD configured</li>
</ul>
<h2 id="decision-framework-distribution-strategy">Decision Framework: Distribution Strategy</h2>
<table>
<thead>
<tr>
<th>Scenario</th>
<th>Strategy</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Internal company use</strong></td>
<td>Private PyPI or Git</td>
<td>Security, control</td>
</tr>
<tr>
<td><strong>Open source</strong></td>
<td>Public PyPI</td>
<td>Community access</td>
</tr>
<tr>
<td><strong>Experimental</strong></td>
<td>TestPyPI only</td>
<td>Safe testing</td>
</tr>
<tr>
<td><strong>Single project</strong></td>
<td>Keep in conftest.py</td>
<td>Simplicity</td>
</tr>
<tr>
<td><strong>Multiple projects</strong></td>
<td>Private package</td>
<td>Reusability</td>
</tr>
<tr>
<td><strong>Community value</strong></td>
<td>Public PyPI + GitHub</td>
<td>Maximum impact</td>
</tr>
</tbody>
</table>
<h2 id="the-journey-from-local-to-public">The Journey: From Local to Public</h2>
<table>
<thead>
<tr>
<th>Stage</th>
<th>Scope</th>
<th>Distribution</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Single test file</td>
<td>Inline code</td>
</tr>
<tr>
<td>1</td>
<td>Single project</td>
<td>conftest.py</td>
</tr>
<tr>
<td>2</td>
<td>Multiple projects (internal)</td>
<td>Private package</td>
</tr>
<tr>
<td>3</td>
<td>Company-wide</td>
<td>Private PyPI</td>
</tr>
<tr>
<td>4</td>
<td>Open source</td>
<td>Public PyPI</td>
</tr>
<tr>
<td>5</td>
<td>Popular plugin</td>
<td>Featured on pytest.org</td>
</tr>
</tbody>
</table>
<h3 id="lessons-learned_5">Lessons Learned</h3>
<ol>
<li><strong>Start local</strong>: Develop in conftest.py first, extract when proven useful</li>
<li><strong>Test thoroughly</strong>: Plugin bugs affect all users, not just one project</li>
<li><strong>Document well</strong>: Good documentation is as important as good code</li>
<li><strong>Version carefully</strong>: Follow semantic versioning strictly</li>
<li><strong>Test on TestPyPI</strong>: Always test distribution before publishing to PyPI</li>
<li><strong>Maintain actively</strong>: Respond to issues, update for new pytest versions</li>
<li><strong>Community matters</strong>: Engage with users, accept contributions</li>
<li><strong>Keep it simple</strong>: Plugins should solve one problem well</li>
</ol>
        </div>
        <div class="footer">
            Generated on 2025-12-03 10:37:13 | Made with ‚ù§Ô∏è by GitHub Pages Generator
        </div>
    </div>
    <script>
        // Syntax highlighting for code blocks
        document.addEventListener('DOMContentLoaded', (event) => {
            // Highlight code blocks
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightElement(block);
            });
            
            // Add copy buttons to code blocks
            document.querySelectorAll('pre').forEach((pre) => {
                const button = document.createElement('button');
                button.className = 'copy-btn';
                button.textContent = 'Copy';
                
                button.addEventListener('click', () => {
                    const code = pre.querySelector('code').textContent;
                    navigator.clipboard.writeText(code).then(() => {
                        button.textContent = 'Copied!';
                        button.classList.add('copied');
                        setTimeout(() => {
                            button.textContent = 'Copy';
                            button.classList.remove('copied');
                        }, 2000);
                    }).catch(err => {
                        console.error('Failed to copy:', err);
                        button.textContent = 'Error';
                        setTimeout(() => {
                            button.textContent = 'Copy';
                        }, 2000);
                    });
                });
                
                pre.appendChild(button);
            });
        });
    </script>
</body>
</html>