<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>19 Plugin Ecosystem and Extensibility</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <style>
        :root {
            --primary: #6366f1;
            --primary-dark: #4f46e5;
            --secondary: #8b5cf6;
            --bg-main: #0f172a;
            --bg-card: #1e293b;
            --bg-code: #0f172a;
            --text-primary: #f1f5f9;
            --text-secondary: #94a3b8;
            --border: #334155;
            --accent: #06b6d4;
            --success: #10b981;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body { 
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.7; 
            color: var(--text-primary); 
            background: var(--bg-main);
        }
        
        .container { 
            max-width: 1200px; 
            margin: 0 auto; 
            padding: 20px; 
        }
        
        .home-btn { 
            position: fixed; 
            top: 20px; 
            right: 20px; 
            background: var(--accent);
            color: white; 
            width: 50px;
            height: 50px;
            border-radius: 50%;
            text-decoration: none; 
            z-index: 1000;
            box-shadow: 0 4px 20px rgba(6, 182, 212, 0.4);
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            font-size: 24px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .home-btn:hover { 
            background: #0891b2;
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(6, 182, 212, 0.5);
        }
        
        .breadcrumb { 
            background: var(--bg-card);
            padding: 12px 0; 
            margin-bottom: 24px; 
            font-size: 14px;
            border-radius: 8px;
            border: 1px solid var(--border);
        }
        
        .breadcrumb a { 
            color: var(--accent);
            text-decoration: none;
            transition: color 0.2s;
        }
        
        .breadcrumb a:hover { 
            color: var(--primary);
            text-decoration: underline;
        }
        
        .content { 
            background: var(--bg-card);
            padding: 3rem; 
            border-radius: 16px; 
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
            border: 1px solid var(--border);
        }
        
        .file-list { 
            display: grid; 
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr)); 
            gap: 1.5rem; 
            margin: 2rem 0; 
        }
        
        .file-item { 
            padding: 1.5rem; 
            border: 1px solid var(--border);
            border-radius: 12px; 
            background: var(--bg-main);
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            overflow: hidden;
        }
        
        .file-item::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 3px;
            background: linear-gradient(90deg, var(--primary), var(--secondary));
            opacity: 0;
            transition: opacity 0.3s;
        }
        
        .file-item:hover { 
            transform: translateY(-4px); 
            box-shadow: 0 8px 30px rgba(99, 102, 241, 0.3);
            border-color: var(--primary);
        }
        
        .file-item:hover::before {
            opacity: 1;
        }
        
        .file-item a { 
            color: var(--text-primary);
            text-decoration: none; 
            font-weight: 600; 
            display: block;
            font-size: 1.1rem;
        }
        
        .file-item a:hover { 
            color: var(--primary);
        }
        
        .file-type { 
            font-size: 13px; 
            color: var(--text-secondary);
            margin-top: 8px; 
            font-weight: 500;
        }
        
        /* Code Blocks */
        pre { 
            background: var(--bg-code);
            padding: 1.5rem; 
            border-radius: 12px; 
            overflow-x: auto;
            border: 1px solid var(--border);
            margin: 1.5rem 0;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3);
            position: relative;
        }
        
        pre code { 
            background: none;
            padding: 0;
            font-family: 'Fira Code', 'Consolas', monospace;
            font-size: 0.9em;
            line-height: 1.6;
        }
        
        /* Copy Button */
        .copy-btn {
            position: absolute;
            top: 8px;
            right: 8px;
            background: var(--primary);
            color: white;
            border: none;
            padding: 6px 12px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 12px;
            font-weight: 600;
            transition: all 0.2s;
            opacity: 0.7;
            z-index: 10;
        }
        
        .copy-btn:hover {
            opacity: 1;
            background: var(--primary-dark);
            transform: translateY(-1px);
        }
        
        .copy-btn.copied {
            background: var(--success);
        }
        
        pre:hover .copy-btn {
            opacity: 1;
        }
        
        /* Inline Code */
        code { 
            background: var(--bg-code);
            color: #8b5cf6;
            padding: 3px 8px; 
            border-radius: 6px; 
            font-size: 1.1em;
            font-family: 'Fira Code', 'Consolas', monospace;
            border: 1px solid var(--border);
        }
        
        /* Headings */
        h1, h2, h3, h4, h5, h6 { 
            color: var(--text-primary);
            margin: 2rem 0 1rem 0;
            font-weight: 700;
            letter-spacing: -0.5px;
        }
        
        h1 { 
            font-size: 2.5rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            border-bottom: 3px solid var(--primary);
            padding-bottom: 12px;
            margin-bottom: 1.5rem;
        }
        
        h2 { 
            font-size: 2rem;
            color: var(--primary);
            border-bottom: 2px solid var(--border);
            padding-bottom: 8px;
        }
        
        h3 { 
            font-size: 1.5rem;
            color: var(--accent);
        }
        
        /* Links */
        a { 
            color: var(--accent);
            transition: color 0.2s;
        }
        
        a:hover { 
            color: var(--primary);
        }
        
        /* Paragraphs */
        p {
            margin: 1rem 0;
            color: var(--text-secondary);
        }
        
        /* Lists */
        ul, ol {
            margin: 1rem 0;
            padding-left: 2rem;
            color: var(--text-secondary);
        }
        
        li {
            margin: 0.5rem 0;
        }
        
        /* Tables */
        table { 
            border-collapse: collapse;
            width: 100%;
            margin: 1.5rem 0;
            background: var(--bg-main);
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
        }
        
        th, td { 
            border: 1px solid var(--border);
            padding: 12px 16px;
            text-align: left;
        }
        
        th { 
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            font-weight: 600;
        }
        
        tr:hover {
            background: var(--bg-card);
        }
        
        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--primary);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            background: var(--bg-main);
            border-radius: 0 8px 8px 0;
            color: var(--text-secondary);
            font-style: italic;
        }
        
        /* Horizontal Rule */
        hr {
            border: none;
            border-top: 2px solid var(--border);
            margin: 2rem 0;
        }
        
        .footer { 
            text-align: center; 
            padding: 2rem; 
            color: var(--text-secondary);
            border-top: 1px solid var(--border);
            margin-top: 3rem; 
            font-size: 14px;
        }
        
        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 12px;
            height: 12px;
        }
        
        ::-webkit-scrollbar-track {
            background: var(--bg-main);
        }
        
        ::-webkit-scrollbar-thumb {
            background: var(--border);
            border-radius: 6px;
        }
        
        ::-webkit-scrollbar-thumb:hover {
            background: var(--primary);
        }
        
        @media (max-width: 768px) {
            .container { padding: 10px; }
            .file-list { grid-template-columns: 1fr; }
            .content { padding: 1.5rem; }
            h1 { font-size: 2rem; }
            h2 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>
    <a href="../index.html" class="home-btn">üè†</a>
    <div class="container">
        <div class="breadcrumb">
            <div style="padding: 0 20px;">
                <a href="../index.html">üè† Home</a> <span style="color: #64748b;">/</span> <a href="index.html">06 Mastery and Beyond</a>
            </div>
        </div>
        <div class="content">
            <h1 id="chapter-19-plugin-ecosystem-and-extensibility">Chapter 19: Plugin Ecosystem and Extensibility</h1>
<h2 id="popular-pytest-plugins-pytest-html-pytest-xdist-pytest-sugar">Popular Pytest Plugins (pytest-html, pytest-xdist, pytest-sugar)</h2>
<h2 id="the-power-of-extensibility">The Power of Extensibility</h2>
<p>Pytest's core is powerful, but its true strength lies in its extensibility. The plugin system allows developers to alter or extend nearly every aspect of pytest's behavior, from test collection and execution to reporting. This has fostered a rich ecosystem of third-party plugins that can solve common testing problems with a simple <code>pip install</code>.</p>
<p>In this chapter, we'll explore this ecosystem. We'll start by using some of the most popular plugins to solve real-world problems, then we'll dive into the mechanics of how plugins work, and finally, we'll build and package our own custom plugin.</p>
<h3 id="phase-1-establish-the-reference-implementation">Phase 1: Establish the Reference Implementation</h3>
<p>To see the value of plugins, we need a test suite with some tangible challenges. Let's create a simple data validation utility. Its job is to check data files for common issues. To simulate a real-world scenario, our tests will be intentionally a bit slow, making them a perfect candidate for optimization.</p>
<p>Our anchor example will be a <code>DataValidator</code> that performs checks on CSV-like data.</p>
<p>Here is the code for our utility:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># project/validator.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DataValidationError</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">pass</span>

<span class="k">class</span><span class="w"> </span><span class="nc">DataValidator</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_source</span><span class="p">):</span>
        <span class="c1"># In a real app, this would read from a file or database</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data_source</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DataValidationError</span><span class="p">(</span><span class="s2">&quot;Data source is empty&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">headers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rows</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;,&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_row_length</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simulates a check that all rows have the same number of columns as the header.&quot;&quot;&quot;</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span> <span class="c1"># Simulate I/O and processing</span>
        <span class="n">header_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">headers</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rows</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="o">!=</span> <span class="n">header_len</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">DataValidationError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Row </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> has incorrect length&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_unique_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column_name</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simulates a check for unique values in a given column.&quot;&quot;&quot;</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.15</span><span class="p">)</span> <span class="c1"># Simulate a more expensive check</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">column_name</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DataValidationError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Column &#39;</span><span class="si">{</span><span class="n">column_name</span><span class="si">}</span><span class="s2">&#39; not found&quot;</span><span class="p">)</span>

        <span class="n">seen_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rows</span><span class="p">):</span>
            <span class="n">row_id</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">row_id</span> <span class="ow">in</span> <span class="n">seen_ids</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">DataValidationError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Duplicate ID &#39;</span><span class="si">{</span><span class="n">row_id</span><span class="si">}</span><span class="s2">&#39; found in column &#39;</span><span class="si">{</span><span class="n">column_name</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
            <span class="n">seen_ids</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">row_id</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">check_value_range</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column_name</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simulates checking that values in a column are within a numeric range.&quot;&quot;&quot;</span>
        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.12</span><span class="p">)</span> <span class="c1"># Simulate another expensive check</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">idx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">column_name</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DataValidationError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Column &#39;</span><span class="si">{</span><span class="n">column_name</span><span class="si">}</span><span class="s2">&#39; not found&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rows</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">val</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">min_val</span> <span class="o">&lt;=</span> <span class="n">val</span> <span class="o">&lt;=</span> <span class="n">max_val</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="n">DataValidationError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Value </span><span class="si">{</span><span class="n">val</span><span class="si">}</span><span class="s2"> in row </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> is out of range (</span><span class="si">{</span><span class="n">min_val</span><span class="si">}</span><span class="s2">-</span><span class="si">{</span><span class="n">max_val</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
            <span class="k">except</span> <span class="p">(</span><span class="ne">ValueError</span><span class="p">,</span> <span class="ne">IndexError</span><span class="p">):</span>
                <span class="k">raise</span> <span class="n">DataValidationError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid numeric value in row </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">, column &#39;</span><span class="si">{</span><span class="n">column_name</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>
</code></pre></div>

<p>And here is our initial test suite. We'll create a fixture to provide valid sample data.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_validator.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">project.validator</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataValidator</span><span class="p">,</span> <span class="n">DataValidationError</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">valid_data</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Provides a string of valid CSV data.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="s2">&quot;id,name,value</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;1,alpha,10</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;2,beta,25</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="s2">&quot;3,gamma,99</span><span class="se">\n</span><span class="s2">&quot;</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_valid_data_passes_all_checks</span><span class="p">(</span><span class="n">valid_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A baseline test for valid data.&quot;&quot;&quot;</span>
    <span class="n">validator</span> <span class="o">=</span> <span class="n">DataValidator</span><span class="p">(</span><span class="n">valid_data</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_row_length</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_unique_id</span><span class="p">(</span><span class="n">column_name</span><span class="o">=</span><span class="s2">&quot;id&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">validator</span><span class="o">.</span><span class="n">check_value_range</span><span class="p">(</span><span class="n">column_name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_incorrect_row_length_fails</span><span class="p">(</span><span class="n">valid_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests that a row with a missing column raises an error.&quot;&quot;&quot;</span>
    <span class="n">invalid_data</span> <span class="o">=</span> <span class="n">valid_data</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;25&quot;</span><span class="p">,</span> <span class="s2">&quot;25,extra_col&quot;</span><span class="p">)</span>
    <span class="n">validator</span> <span class="o">=</span> <span class="n">DataValidator</span><span class="p">(</span><span class="n">invalid_data</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="n">DataValidationError</span><span class="p">,</span> <span class="n">match</span><span class="o">=</span><span class="s2">&quot;incorrect length&quot;</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_row_length</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_duplicate_id_fails</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests that duplicate IDs are caught.&quot;&quot;&quot;</span>
    <span class="c1"># This test creates its own data to be independent</span>
    <span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;id,name</span><span class="se">\n</span><span class="s2">1,alpha</span><span class="se">\n</span><span class="s2">1,beta&quot;</span>
    <span class="n">validator</span> <span class="o">=</span> <span class="n">DataValidator</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="n">DataValidationError</span><span class="p">,</span> <span class="n">match</span><span class="o">=</span><span class="s2">&quot;Duplicate ID &#39;1&#39;&quot;</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_unique_id</span><span class="p">(</span><span class="n">column_name</span><span class="o">=</span><span class="s2">&quot;id&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_value_out_of_range_fails</span><span class="p">(</span><span class="n">valid_data</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests that a value outside the specified range fails.&quot;&quot;&quot;</span>
    <span class="n">invalid_data</span> <span class="o">=</span> <span class="n">valid_data</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;99&quot;</span><span class="p">,</span> <span class="s2">&quot;101&quot;</span><span class="p">)</span>
    <span class="n">validator</span> <span class="o">=</span> <span class="n">DataValidator</span><span class="p">(</span><span class="n">invalid_data</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="n">DataValidationError</span><span class="p">,</span> <span class="n">match</span><span class="o">=</span><span class="s2">&quot;out of range&quot;</span><span class="p">):</span>
        <span class="n">validator</span><span class="o">.</span><span class="n">check_value_range</span><span class="p">(</span><span class="n">column_name</span><span class="o">=</span><span class="s2">&quot;value&quot;</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div>

<p>Let's run this suite and see the standard pytest output.</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span><span class="nv">pytest</span>
<span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
platform<span class="w"> </span>linux<span class="w"> </span>--<span class="w"> </span>Python<span class="w"> </span><span class="m">3</span>.10.12,<span class="w"> </span>pytest-7.4.0,<span class="w"> </span>pluggy-1.2.0
rootdir:<span class="w"> </span>/path/to/project
collected<span class="w"> </span><span class="m">4</span><span class="w"> </span>items

tests/test_validator.py<span class="w"> </span>....<span class="w">                                             </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">==============================</span><span class="w"> </span><span class="m">4</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.51s<span class="w"> </span><span class="o">===============================</span>
</code></pre></div>

<p>Our test suite works, but it has three problems that plugins can solve:
1.  <strong>Readability</strong>: The output is functional, but for larger suites, the simple dots (<code>....</code>) aren't very engaging or informative.
2.  <strong>Reporting</strong>: If we need to share these results, terminal output isn't a professional or persistent format.
3.  <strong>Speed</strong>: The suite took over 1.5 seconds for just four simple tests. As we add hundreds more, this will become a major bottleneck.</p>
<p>Let's tackle these one by one using popular plugins.</p>
<h3 id="iteration-1-improving-readability-with-pytest-sugar">Iteration 1: Improving Readability with <code>pytest-sugar</code></h3>
<p><strong>Current Limitation</strong>: The default pytest output is minimal. It doesn't show test names as they run, and the progress indicator is just a series of dots.</p>
<p><code>pytest-sugar</code> is a plugin that provides a much nicer user interface in the terminal, including a progress bar and instant feedback on failing tests.</p>
<p>First, let's install it.</p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>pytest-sugar
</code></pre></div>

<p>Now, let's run our tests again with no other changes. <code>pytest-sugar</code> automatically activates itself upon installation.</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest

‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï<span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span>starts<span class="w"> </span>‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï
platform<span class="w"> </span>linux<span class="w"> </span>--<span class="w"> </span>Python<span class="w"> </span><span class="m">3</span>.10.12,<span class="w"> </span>pytest-7.4.0,<span class="w"> </span>pluggy-1.2.0
rootdir:<span class="w"> </span>/path/to/project
plugins:<span class="w"> </span>sugar-1.0.0
collected<span class="w"> </span><span class="m">4</span><span class="w"> </span>items

<span class="w"> </span>tests/test_validator.py<span class="w"> </span>‚úì‚úì‚úì‚úì<span class="w">                                           </span><span class="m">100</span>%<span class="w"> </span>‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà

‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï<span class="w"> </span>summary<span class="w"> </span>‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï
‚úÖ<span class="w"> </span><span class="m">4</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.52s
</code></pre></div>

<p><strong>Expected vs. Actual Improvement</strong>: The output is immediately clearer. We get a progress bar, and the checkmarks (<code>‚úì</code>) give a more positive sense of progress than the dots. If a test were to fail, <code>pytest-sugar</code> would print the failure immediately instead of waiting for the entire run to finish, providing faster feedback. This simple plugin enhances the developer experience with zero configuration.</p>
<h3 id="iteration-2-generating-reports-with-pytest-html">Iteration 2: Generating Reports with <code>pytest-html</code></h3>
<p><strong>Current Limitation</strong>: Our test results vanish as soon as we close the terminal. We have no artifact to archive, email, or post on a dashboard.</p>
<p><code>pytest-html</code> solves this by generating a self-contained HTML report with the results of the test run.</p>
<p>First, install it.</p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>pytest-html
</code></pre></div>

<p>To use it, we run pytest with an extra command-line option, <code>--html</code>, specifying the output file.</p>
<div class="codehilite"><pre><span></span><code>pytest<span class="w"> </span>--html<span class="o">=</span>report.html<span class="w"> </span>--self-contained-html
</code></pre></div>

<p>The run looks the same in the terminal (you'll still see the <code>pytest-sugar</code> output), but now you'll have a new file, <code>report.html</code>, in your directory. Opening it in a browser shows a detailed, professional report with information about your environment, a summary of results, and a detailed breakdown of each test, including its duration.</p>
<p>This artifact is invaluable for continuous integration (CI) systems, quality assurance teams, and project managers.</p>
<h3 id="iteration-3-speeding-up-the-suite-with-pytest-xdist">Iteration 3: Speeding Up the Suite with <code>pytest-xdist</code></h3>
<p><strong>Current Limitation</strong>: Our tests run sequentially, one after another. The total time is the sum of all individual test times. Our four tests took ~1.5 seconds. A suite with 400 such tests would take over 2 minutes.</p>
<p><code>pytest-xdist</code> allows you to run tests in parallel, distributing them across multiple CPU cores. Since our tests are independent of each other, they are perfect candidates for parallelization.</p>
<p>First, install it.</p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>pytest-xdist
</code></pre></div>

<p>To activate it, we use the <code>-n</code> flag to specify the number of parallel processes. A common choice is <code>auto</code>, which tells <code>pytest-xdist</code> to use the number of available CPU cores.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># First, let&#39;s get a baseline time without xdist</span>
$<span class="w"> </span><span class="nb">time</span><span class="w"> </span>pytest<span class="w"> </span>-q
....
<span class="m">4</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.51s

real<span class="w">    </span>0m1.623s
user<span class="w">    </span>0m0.315s
sys<span class="w">     </span>0m0.048s

<span class="c1"># Now, let&#39;s run with xdist</span>
$<span class="w"> </span><span class="nb">time</span><span class="w"> </span>pytest<span class="w"> </span>-q<span class="w"> </span>-n<span class="w"> </span>auto
....
<span class="m">4</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.55s

real<span class="w">    </span>0m0.897s
user<span class="w">    </span>0m0.641s
sys<span class="w">     </span>0m0.102s
</code></pre></div>

<p><strong>Expected vs. Actual Improvement</strong>: The results are dramatic. The test session duration reported by pytest dropped from <strong>1.51s</strong> to <strong>0.55s</strong>‚Äînearly a 3x speedup on a machine with 4 cores. The <code>real</code> time reported by the <code>time</code> command shows a similar improvement.</p>
<p><code>pytest-xdist</code> intelligently sends each test file to a different worker process. Because our tests are I/O-bound (due to <code>time.sleep</code>), parallelization allows the CPU to switch between them effectively, finishing the entire suite much faster than a sequential run. For large, slow test suites (e.g., those involving database access or network requests), <code>pytest-xdist</code> is an essential tool.</p>
<h2 id="installing-and-configuring-plugins">Installing and Configuring Plugins</h2>
<h2 id="managing-your-plugins">Managing Your Plugins</h2>
<p>As we saw in the previous section, using a plugin is typically a two-step process:
1.  Install the package using <code>pip</code>.
2.  Activate its functionality, either automatically or via a command-line flag.</p>
<h3 id="installation">Installation</h3>
<p>Plugins are standard Python packages, so they are installed with <code>pip</code>:</p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>pytest-xdist
</code></pre></div>

<p>It's crucial to manage these dependencies just like any other project dependency. You should add them to your <code>requirements.txt</code> or <code>pyproject.toml</code> file to ensure that your test environment is reproducible.</p>
<div class="codehilite"><pre><span></span><code># requirements.txt
pytest
pytest-sugar
pytest-html
pytest-xdist
</code></pre></div>

<h3 id="discovering-installed-plugins">Discovering Installed Plugins</h3>
<p>Pytest automatically discovers any installed packages that register as plugins. You can see which plugins are active in your environment by looking at the header of any pytest run:</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>--version
pytest<span class="w"> </span><span class="m">7</span>.4.0,<span class="w"> </span>pluggy<span class="w"> </span><span class="m">1</span>.2.0
plugins:<span class="w"> </span>sugar-1.0.0,<span class="w"> </span>html-4.0.1,<span class="w"> </span>xdist-3.3.1
</code></pre></div>

<p>For even more detail, you can use the <code>--trace-config</code> flag, which shows where each plugin was loaded from. This is extremely useful for debugging issues with plugin loading.</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>--trace-config
<span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
<span class="c1"># ... (lots of output)</span>
PLUGIN<span class="w"> </span>registered:<span class="w"> </span>&lt;module<span class="w"> </span><span class="s1">&#39;sugar.plugin&#39;</span><span class="w"> </span>from<span class="w"> </span><span class="s1">&#39;/.../lib/python3.10/site-packages/sugar/plugin.py&#39;</span>&gt;
PLUGIN<span class="w"> </span>registered:<span class="w"> </span>&lt;module<span class="w"> </span><span class="s1">&#39;pytest_html.plugin&#39;</span><span class="w"> </span>from<span class="w"> </span><span class="s1">&#39;/.../lib/python3.10/site-packages/pytest_html/plugin.py&#39;</span>&gt;
PLUGIN<span class="w"> </span>registered:<span class="w"> </span>&lt;module<span class="w"> </span><span class="s1">&#39;pytest_xdist.plugin&#39;</span><span class="w"> </span>from<span class="w"> </span><span class="s1">&#39;/.../lib/python3.10/site-packages/pytest_xdist/plugin.py&#39;</span>&gt;
<span class="c1"># ... (more output)</span>
</code></pre></div>

<h3 id="configuration">Configuration</h3>
<p>Many plugins offer configuration options to customize their behavior. These are typically managed in your <code>pytest.ini</code>, <code>pyproject.toml</code>, or <code>setup.cfg</code> file, under a section named <code>[pytest]</code>.</p>
<p>For example, you can set command-line options by default using <code>addopts</code>. Instead of typing <code>pytest -n auto --html=report.html</code> every time, you can configure it like this:</p>
<p><strong><code>pytest.ini</code></strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">[pytest]</span>
<span class="na">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">-n auto --html=report.html --self-contained-html</span>
</code></pre></div>

<p>Now, simply running <code>pytest</code> will execute with those options automatically.</p>
<p>Plugins may also define their own custom configuration keys. For example, <code>pytest-html</code> allows you to customize the report title.</p>
<p><strong><code>pytest.ini</code></strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">[pytest]</span>
<span class="na">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">-n auto --html=report.html --self-contained-html</span>

<span class="k">[pytest-html]</span>
<span class="na">report_title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">My Project Test Report</span>
</code></pre></div>

<p>Always consult the documentation for the specific plugin you are using to see the available configuration options.</p>
<h2 id="creating-custom-plugins">Creating Custom Plugins</h2>
<h2 id="extending-pytest-yourself">Extending Pytest Yourself</h2>
<p>Using third-party plugins is powerful, but the real magic happens when you realize you can write your own. This allows you to create project-specific helpers, enforce custom conventions, or integrate with internal tools.</p>
<p>The simplest way to create a plugin is to place code in a <code>conftest.py</code> file at your project's root. Pytest automatically discovers and loads this file, making any hooks or fixtures defined within it available to your entire test suite.</p>
<h3 id="iteration-4-creating-a-custom-test-timer-plugin">Iteration 4: Creating a Custom Test Timer Plugin</h3>
<p><strong>Current Limitation</strong>: We know the total test suite duration, but we don't have an easy way to see the duration of each individual test or to flag tests that are unusually slow. While <code>pytest --durations</code> exists, building our own simple version is the best way to learn the plugin mechanism.</p>
<p><strong>Goal</strong>: We want to create a plugin that:
1.  Times each test individually.
2.  At the end of the run, prints a report of the 5 slowest tests.</p>
<p>To do this, we need to tap into pytest's execution process. We'll use <strong>hooks</strong>, which are special functions that pytest calls at specific points during its lifecycle. We'll place our hook implementations in <code>conftest.py</code>.</p>
<p>Here is the implementation for our simple timing plugin:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_setup</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook called before a test item is run.&quot;&quot;&quot;</span>
    <span class="n">item</span><span class="o">.</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_teardown</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">nextitem</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook called after a test item is run.&quot;&quot;&quot;</span>
    <span class="n">item</span><span class="o">.</span><span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">item</span><span class="o">.</span><span class="n">duration</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">item</span><span class="o">.</span><span class="n">start_time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_sessionfinish</span><span class="p">(</span><span class="n">session</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook called after the entire test session finishes.&quot;&quot;&quot;</span>
    <span class="n">reporter</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pluginmanager</span><span class="o">.</span><span class="n">get_plugin</span><span class="p">(</span><span class="s1">&#39;terminalreporter&#39;</span><span class="p">)</span>
    <span class="n">reporter</span><span class="o">.</span><span class="n">write_sep</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="s2">&quot;CUSTOM TEST DURATIONS REPORT&quot;</span><span class="p">)</span>

    <span class="c1"># Collect all items and their durations</span>
    <span class="n">all_items</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">session</span><span class="o">.</span><span class="n">items</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;duration&#39;</span><span class="p">)]</span>

    <span class="c1"># Sort by duration, descending</span>
    <span class="n">slowest_tests</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">all_items</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">duration</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Report the top 5 slowest</span>
    <span class="n">reporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Top 5 slowest tests:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">slowest_tests</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
        <span class="n">reporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">duration</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="banish-magic-with-mechanics-how-this-works">Banish Magic with Mechanics: How This Works</h3>
<p>Let's break down what's happening here. We've defined three functions with special names that pytest recognizes as hooks:</p>
<ol>
<li>
<p><strong><code>pytest_runtest_setup(item)</code></strong>: Pytest calls this hook just before it executes a test function. The <code>item</code> object represents the test being run (e.g., <code>test_valid_data_passes_all_checks</code>). We attach a <code>start_time</code> attribute directly to this object to store the current time.</p>
</li>
<li>
<p><strong><code>pytest_runtest_teardown(item, nextitem)</code></strong>: This hook is called after the test has finished (and after any teardown fixtures). We record the <code>end_time</code> and calculate the <code>duration</code>, again storing it on the <code>item</code> object.</p>
</li>
<li>
<p><strong><code>pytest_sessionfinish(session)</code></strong>: Pytest calls this hook once at the very end of the entire test session. This is the perfect place to generate our summary report.</p>
<ul>
<li>We get the <code>terminalreporter</code> object from the session, which allows us to print nicely formatted output to the console.</li>
<li>We collect all test items from the session.</li>
<li>We sort them by the <code>duration</code> attribute we added.</li>
<li>We print the top 5 slowest tests.</li>
</ul>
</li>
</ol>
<p>Now, let's run pytest. We don't need any special flags; the plugin in <code>conftest.py</code> is loaded automatically.</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest
<span class="c1"># ... (normal test output from pytest-sugar) ...</span>

‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï<span class="w"> </span>summary<span class="w"> </span>‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï
‚úÖ<span class="w"> </span><span class="m">4</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.53s

<span class="o">=======================</span><span class="w"> </span>CUSTOM<span class="w"> </span>TEST<span class="w"> </span>DURATIONS<span class="w"> </span><span class="nv">REPORT</span><span class="w"> </span><span class="o">=======================</span>

Top<span class="w"> </span><span class="m">5</span><span class="w"> </span>slowest<span class="w"> </span>tests:
tests/test_validator.py::test_valid_data_passes_all_checks:<span class="w"> </span><span class="m">0</span>.3789s
tests/test_validator.py::test_duplicate_id_fails:<span class="w"> </span><span class="m">0</span>.1534s
tests/test_validator.py::test_value_out_of_range_fails:<span class="w"> </span><span class="m">0</span>.1227s
tests/test_validator.py::test_incorrect_row_length_fails:<span class="w"> </span><span class="m">0</span>.1019s
</code></pre></div>

<p><strong>Expected vs. Actual Improvement</strong>: It worked perfectly! After the standard summary, our custom report is printed, showing the exact duration of each test, sorted from slowest to fastest. We have successfully extended pytest's functionality to meet our specific needs. This demonstrates the core principle of plugins: you can attach your own logic to almost any part of the testing process.</p>
<h2 id="hooks-how-pytest-plugins-work-under-the-hood">Hooks: How Pytest Plugins Work Under the Hood</h2>
<h2 id="the-pytest-hook-system">The Pytest Hook System</h2>
<p>The functions we wrote in <code>conftest.py</code> (<code>pytest_runtest_setup</code>, etc.) are the heart of the plugin system. Pytest defines hundreds of these <strong>hook specifications</strong> that act as well-defined entry points for plugins to inject custom logic.</p>
<p>When pytest is about to perform an action (like collecting tests, running a test, or finishing the session), it checks if any loaded plugins (including <code>conftest.py</code>) have implemented the corresponding hook function. If so, it calls that function, passing in relevant context as arguments (like the <code>item</code> or <code>session</code> object).</p>
<p>This is a powerful design because it allows plugins to be modular and self-contained. A plugin only needs to implement the hooks it cares about.</p>
<h3 id="key-hook-categories-and-examples">Key Hook Categories and Examples</h3>
<p>While there are many hooks, they generally fall into a few main categories. Understanding these categories helps you know where to look when you want to build a plugin.</p>
<h4 id="1-initialization-and-configuration-hooks">1. Initialization and Configuration Hooks</h4>
<p>These hooks run at the very beginning of a pytest session and are used for adding command-line options or modifying configuration.</p>
<ul>
<li><strong><code>pytest_addoption(parser)</code></strong>: Allows you to add custom command-line options. The <code>parser</code> object is used to define the option, its help text, and default value.</li>
<li><strong><code>pytest_configure(config)</code></strong>: Called after command-line options have been parsed. This is where you can read your custom options and set up any global state for your plugin.</li>
</ul>
<h4 id="2-collection-hooks">2. Collection Hooks</h4>
<p>These hooks are called when pytest is discovering test files and functions. They allow you to customize the collection process.</p>
<ul>
<li><strong><code>pytest_collection_modifyitems(session, config, items)</code></strong>: One of the most powerful hooks. It's called after all test items have been collected. The <code>items</code> argument is a list of all test functions pytest is about to run. You can reorder this list, remove items (deselect tests), or add custom markers. For example, you could write a plugin to automatically add a <code>@pytest.mark.slow</code> marker to any test whose name contains <code>_integration_</code>.</li>
</ul>
<h4 id="3-test-execution-runtest-hooks">3. Test Execution (Runtest) Hooks</h4>
<p>This is the family of hooks we used for our timer plugin. They wrap the actual execution of a single test item.</p>
<ul>
<li><strong><code>pytest_runtest_protocol(item, nextitem)</code></strong>: This is the main hook that manages the execution of a single test. It's responsible for calling the setup, the test function itself, and the teardown.</li>
<li><strong><code>pytest_runtest_setup(item)</code></strong>: Called before the test and its fixtures are set up.</li>
<li><strong><code>pytest_runtest_call(item)</code></strong>: Called to execute the test function itself.</li>
<li><strong><code>pytest_runtest_teardown(item, nextitem)</code></strong>: Called after the test has run and fixtures have been torn down.</li>
<li><strong><code>pytest_runtest_makereport(item, call)</code></strong>: A crucial hook called to create the test report object. The <code>call</code> object contains information about whether the setup, call, or teardown phase failed. You can inspect the outcome (passed, failed, skipped) and add extra information to the report, which can then be used by other hooks or plugins (like <code>pytest-html</code>).</li>
</ul>
<h4 id="4-reporting-hooks">4. Reporting Hooks</h4>
<p>These hooks are used to customize the output of the test run.</p>
<ul>
<li><strong><code>pytest_report_header(config)</code></strong>: Allows you to add extra lines to the header of the test report.</li>
<li><strong><code>pytest_terminal_summary(terminalreporter, exitstatus, config)</code></strong>: Called just before pytest exits. This is an alternative to <code>pytest_sessionfinish</code> that is specifically for adding content to the terminal report. We could have used this for our timer plugin.</li>
</ul>
<p>By combining these hooks, you can create incredibly sophisticated plugins that integrate deeply with pytest's lifecycle. The official pytest documentation contains a complete reference of all available hooks. When building a plugin, your first step should be to browse this list to find the correct entry points for the logic you want to implement.</p>
<h2 id="common-plugin-use-cases">Common Plugin Use Cases</h2>
<h2 id="what-can-you-build-with-plugins">What Can You Build with Plugins?</h2>
<p>Now that we understand the mechanics of hooks, let's explore some common and powerful use cases for custom plugins.</p>
<h3 id="adding-custom-command-line-options">Adding Custom Command-Line Options</h3>
<p>Imagine our data validator needs to run against different environments (e.g., <code>staging</code>, <code>production</code>), and the data source changes for each. We can add a custom <code>--env</code> option to control this.</p>
<p><strong><code>tests/conftest.py</code></strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/conftest.py</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_addoption</span><span class="p">(</span><span class="n">parser</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Adds a custom --env command-line option.&quot;&quot;&quot;</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">addoption</span><span class="p">(</span>
        <span class="s2">&quot;--env&quot;</span><span class="p">,</span> <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;staging&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;environment to run tests against&quot;</span>
    <span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">api_endpoint</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A fixture that provides the correct API endpoint based on --env.&quot;&quot;&quot;</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">getoption</span><span class="p">(</span><span class="s2">&quot;--env&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">env</span> <span class="o">==</span> <span class="s2">&quot;staging&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;https://staging-api.example.com&quot;</span>
    <span class="k">elif</span> <span class="n">env</span> <span class="o">==</span> <span class="s2">&quot;production&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;https://api.example.com&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;https://dev-api.example.com&quot;</span>

<span class="c1"># In a test:</span>
<span class="c1"># def test_api_connection(api_endpoint):</span>
<span class="c1">#     assert requests.get(api_endpoint).status_code == 200</span>
</code></pre></div>

<p>Now you can run <code>pytest --env=production</code> to target the production API. This pattern is fundamental for building flexible test suites.</p>
<h3 id="dynamically-selecting-or-reordering-tests">Dynamically Selecting or Reordering Tests</h3>
<p>The <code>pytest_collection_modifyitems</code> hook gives you ultimate control over which tests run and in what order.</p>
<p><strong>Use Case</strong>: You want to run all tests marked as <code>slow</code> at the very end of the test suite to get faster feedback on quick unit tests.</p>
<p><strong><code>tests/conftest.py</code></strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/conftest.py</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_collection_modifyitems</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">items</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reorders tests to run @pytest.mark.slow tests last.&quot;&quot;&quot;</span>
    <span class="n">slow_tests</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">other_tests</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">item</span><span class="o">.</span><span class="n">get_closest_marker</span><span class="p">(</span><span class="s2">&quot;slow&quot;</span><span class="p">):</span>
            <span class="n">slow_tests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">other_tests</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

    <span class="c1"># Put the other tests first, then the slow ones</span>
    <span class="n">items</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">other_tests</span> <span class="o">+</span> <span class="n">slow_tests</span>
</code></pre></div>

<p>This simple plugin automatically prioritizes your faster tests, improving the developer feedback loop without any manual intervention.</p>
<h3 id="integrating-with-external-systems">Integrating with External Systems</h3>
<p>Plugins are the perfect way to integrate your test suite with external tools like test case management systems (e.g., TestRail, Zephyr) or notification services (e.g., Slack).</p>
<p><strong>Use Case</strong>: Post a summary of test failures to a Slack channel after a CI run.</p>
<p><strong><code>tests/conftest.py</code> (conceptual)</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/conftest.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_sessionfinish</span><span class="p">(</span><span class="n">session</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Posts a summary to Slack on failure.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">session</span><span class="o">.</span><span class="n">testsfailed</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;CI&quot;</span><span class="p">):</span>
        <span class="n">slack_webhook_url</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;SLACK_WEBHOOK_URL&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">slack_webhook_url</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">summary</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Test run finished with </span><span class="si">{</span><span class="n">session</span><span class="o">.</span><span class="n">testsfailed</span><span class="si">}</span><span class="s2"> failures.&quot;</span>
        <span class="c1"># In a real implementation, you would list the failed tests.</span>

        <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">summary</span><span class="p">}</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">slack_webhook_url</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">RequestException</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to send Slack notification: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>This hook runs at the end of the session, checks for failures, and (if running in a CI environment) sends a notification. This kind of automation is a hallmark of a mature testing setup.</p>
<h2 id="distributing-your-own-plugin">Distributing Your Own Plugin</h2>
<h2 id="from-conftestpy-to-installable-package">From <code>conftest.py</code> to Installable Package</h2>
<p>Our custom timer plugin is useful, and we might want to use it in other projects. Copying the <code>conftest.py</code> file everywhere is not a scalable or maintainable solution. The professional approach is to package it as a proper, installable Python package.</p>
<h3 id="iteration-5-packaging-the-custom-timer-plugin">Iteration 5: Packaging the Custom Timer Plugin</h3>
<p><strong>Current Limitation</strong>: The plugin code lives inside our project's <code>tests/conftest.py</code>, making it impossible to share with other projects without copy-pasting.</p>
<p><strong>Goal</strong>: Create a new, separate Python package for our <code>pytest-timer</code> plugin that can be installed with <code>pip</code>.</p>
<h4 id="step-1-create-the-package-structure">Step 1: Create the Package Structure</h4>
<p>First, we'll create a new directory for our plugin project.</p>
<div class="codehilite"><pre><span></span><code>pytest-timer/
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ pytest_timer/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ plugin.py
</code></pre></div>

<ul>
<li><code>pyproject.toml</code>: The modern standard for defining Python package metadata and dependencies.</li>
<li><code>src/pytest_timer/plugin.py</code>: We will move our plugin code here from <code>conftest.py</code>.</li>
</ul>
<h4 id="step-2-move-the-plugin-code">Step 2: Move the Plugin Code</h4>
<p>Copy the hook implementations from <code>conftest.py</code> into <code>src/pytest_timer/plugin.py</code>. The code remains identical.</p>
<p><strong><code>src/pytest_timer/plugin.py</code></strong></p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_setup</span><span class="p">(</span><span class="n">item</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook called before a test item is run.&quot;&quot;&quot;</span>
    <span class="n">item</span><span class="o">.</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_runtest_teardown</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="n">nextitem</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook called after a test item is run.&quot;&quot;&quot;</span>
    <span class="n">item</span><span class="o">.</span><span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">item</span><span class="o">.</span><span class="n">duration</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">item</span><span class="o">.</span><span class="n">start_time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">pytest_sessionfinish</span><span class="p">(</span><span class="n">session</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hook called after the entire test session finishes.&quot;&quot;&quot;</span>
    <span class="n">reporter</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pluginmanager</span><span class="o">.</span><span class="n">get_plugin</span><span class="p">(</span><span class="s1">&#39;terminalreporter&#39;</span><span class="p">)</span>
    <span class="n">reporter</span><span class="o">.</span><span class="n">write_sep</span><span class="p">(</span><span class="s2">&quot;=&quot;</span><span class="p">,</span> <span class="s2">&quot;CUSTOM TEST DURATIONS REPORT&quot;</span><span class="p">)</span>

    <span class="n">all_items</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">session</span><span class="o">.</span><span class="n">items</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="s1">&#39;duration&#39;</span><span class="p">)]</span>
    <span class="n">slowest_tests</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">all_items</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">duration</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">reporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Top 5 slowest tests:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">slowest_tests</span><span class="p">[:</span><span class="mi">5</span><span class="p">]:</span>
        <span class="n">reporter</span><span class="o">.</span><span class="n">write_line</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">nodeid</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">item</span><span class="o">.</span><span class="n">duration</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">s&quot;</span><span class="p">)</span>
</code></pre></div>

<h4 id="step-3-define-the-package-metadata-and-entry-point">Step 3: Define the Package Metadata and Entry Point</h4>
<p>This is the most critical step. We need to tell the Python packaging tools about our project and, most importantly, tell pytest that this package contains a plugin. This is done using an <strong>entry point</strong>.</p>
<p>The entry point is a special piece of metadata that tells a host application (like pytest) where to find plugins. For pytest, the entry point group is called <code>pytest11</code>.</p>
<p><strong><code>pyproject.toml</code></strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">[build-system]</span>
<span class="n">requires</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s2">&quot;setuptools&gt;=61.0&quot;</span><span class="p">]</span>
<span class="n">build-backend</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;setuptools.build_meta&quot;</span>

<span class="k">[project]</span>
<span class="n">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;pytest-timer&quot;</span>
<span class="n">version</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;0.1.0&quot;</span>
<span class="n">authors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">  </span><span class="p">{</span><span class="w"> </span><span class="n">name</span><span class="p">=</span><span class="s2">&quot;Your Name&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">email</span><span class="p">=</span><span class="s2">&quot;you@example.com&quot;</span><span class="w"> </span><span class="p">},</span>
<span class="p">]</span>
<span class="n">description</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;A simple pytest plugin to report slow tests&quot;</span>
<span class="n">requires-python</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;&gt;=3.8&quot;</span>
<span class="n">classifiers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;Programming Language :: Python :: 3&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;License :: OSI Approved :: MIT License&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Operating System :: OS Independent&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;Framework :: Pytest&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">dependencies</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;pytest&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="k">[project.entry-points.</span><span class="s2">&quot;pytest11&quot;</span><span class="k">]</span>
<span class="n">timer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;pytest_timer.plugin&quot;</span>
</code></pre></div>

<p>Let's break down the <code>[project.entry-points."pytest11"]</code> section:
-   <code>"pytest11"</code>: This is the specific entry point group that pytest looks for. The name is historical.
-   <code>timer</code>: This is a name we give our plugin. It's not strictly required but is good practice.
-   <code>"pytest_timer.plugin"</code>: This is the crucial part. It tells pytest to load the module <code>pytest_timer.plugin</code> as a plugin. Pytest will then inspect this module for any functions named like <code>pytest_*</code> and register them as hooks.</p>
<h4 id="step-4-install-and-verify">Step 4: Install and Verify</h4>
<p>Now, we can install our plugin in editable mode from the root of the <code>pytest-timer</code> directory. This links the package into our Python environment so we can use it immediately.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># From within the pytest-timer/ directory</span>
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.
</code></pre></div>

<p>Go back to our original data validator project. <strong>Crucially, remove the plugin code from <code>tests/conftest.py</code></strong> so we can be sure the packaged version is being used.</p>
<p>Now, run pytest.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># In the original project directory</span>
$<span class="w"> </span>pytest
<span class="c1"># ... (normal test output) ...</span>

‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï<span class="w"> </span>summary<span class="w"> </span>‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï
‚úÖ<span class="w"> </span><span class="m">4</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.55s

<span class="o">=======================</span><span class="w"> </span>CUSTOM<span class="w"> </span>TEST<span class="w"> </span>DURATIONS<span class="w"> </span><span class="nv">REPORT</span><span class="w"> </span><span class="o">=======================</span>

Top<span class="w"> </span><span class="m">5</span><span class="w"> </span>slowest<span class="w"> </span>tests:
tests/test_validator.py::test_valid_data_passes_all_checks:<span class="w"> </span><span class="m">0</span>.3812s
tests/test_validator.py::test_duplicate_id_fails:<span class="w"> </span><span class="m">0</span>.1540s
tests/test_validator.py::test_value_out_of_range_fails:<span class="w"> </span><span class="m">0</span>.1233s
tests/test_validator.py::test_incorrect_row_length_fails:<span class="w"> </span><span class="m">0</span>.1025s
</code></pre></div>

<p>It works! Pytest discovered our installed <code>pytest-timer</code> package via the entry point, loaded <code>pytest_timer.plugin</code>, and registered our hooks. Our project's test suite is now clean, and our plugin is a reusable, distributable tool that can be shared, versioned, and even published to the Python Package Index (PyPI) for anyone to use.</p>
<h3 id="the-journey-from-problem-to-solution">The Journey: From Problem to Solution</h3>
<table>
<thead>
<tr>
<th>Iteration</th>
<th>Failure Mode / Problem</th>
<th>Technique Applied</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Default output is verbose and slow</td>
<td>None</td>
<td>Baseline performance and readability</td>
</tr>
<tr>
<td>1</td>
<td>Output is hard to scan quickly</td>
<td><code>pytest-sugar</code> plugin</td>
<td>Improved terminal UI with progress bar</td>
</tr>
<tr>
<td>2</td>
<td>No shareable report for stakeholders</td>
<td><code>pytest-html</code> plugin</td>
<td>Generated self-contained HTML report</td>
</tr>
<tr>
<td>3</td>
<td>Test suite execution is slow</td>
<td><code>pytest-xdist</code> plugin</td>
<td>Parallel execution, significantly faster suite</td>
</tr>
<tr>
<td>4</td>
<td>Need custom per-test timing data</td>
<td>Local plugin in <code>conftest.py</code> using hooks</td>
<td>Custom summary report with slow test warnings</td>
</tr>
<tr>
<td>5</td>
<td>Custom plugin is not reusable</td>
<td>Packaging plugin as an installable distribution</td>
<td>Plugin can be installed and used in any project</td>
</tr>
</tbody>
</table>
<p>This journey encapsulates the power of the pytest ecosystem. You can start by leveraging the work of others to quickly solve common problems. As your needs become more specific, you can tap into the same hook system the community uses to build your own powerful, project-specific tooling. And finally, you can contribute back to that ecosystem by packaging and sharing your solutions.</p>
        </div>
        <div class="footer">
            Generated on 2025-11-24 14:31:16 | Made with ‚ù§Ô∏è by GitHub Pages Generator
        </div>
    </div>
    <script>
        // Syntax highlighting for code blocks
        document.addEventListener('DOMContentLoaded', (event) => {
            // Highlight code blocks
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightElement(block);
            });
            
            // Add copy buttons to code blocks
            document.querySelectorAll('pre').forEach((pre) => {
                const button = document.createElement('button');
                button.className = 'copy-btn';
                button.textContent = 'Copy';
                
                button.addEventListener('click', () => {
                    const code = pre.querySelector('code').textContent;
                    navigator.clipboard.writeText(code).then(() => {
                        button.textContent = 'Copied!';
                        button.classList.add('copied');
                        setTimeout(() => {
                            button.textContent = 'Copy';
                            button.classList.remove('copied');
                        }, 2000);
                    }).catch(err => {
                        console.error('Failed to copy:', err);
                        button.textContent = 'Error';
                        setTimeout(() => {
                            button.textContent = 'Copy';
                        }, 2000);
                    });
                });
                
                pre.appendChild(button);
            });
        });
    </script>
</body>
</html>