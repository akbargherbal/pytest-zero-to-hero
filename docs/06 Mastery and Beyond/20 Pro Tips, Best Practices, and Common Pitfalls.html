<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>20 Pro Tips, Best Practices, and Common Pitfalls</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <style>
        :root {
            --primary: #6366f1;
            --primary-dark: #4f46e5;
            --secondary: #8b5cf6;
            --bg-main: #0f172a;
            --bg-card: #1e293b;
            --bg-code: #0f172a;
            --text-primary: #f1f5f9;
            --text-secondary: #94a3b8;
            --border: #334155;
            --accent: #06b6d4;
            --success: #10b981;
        }
        
        * { margin: 0; padding: 0; box-sizing: border-box; }
        
        body { 
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.7; 
            color: var(--text-primary); 
            background: var(--bg-main);
        }
        
        .container { 
            max-width: 1200px; 
            margin: 0 auto; 
            padding: 20px; 
        }
        
        .home-btn { 
            position: fixed; 
            top: 20px; 
            right: 20px; 
            background: var(--accent);
            color: white; 
            width: 50px;
            height: 50px;
            border-radius: 50%;
            text-decoration: none; 
            z-index: 1000;
            box-shadow: 0 4px 20px rgba(6, 182, 212, 0.4);
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            font-size: 24px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .home-btn:hover { 
            background: #0891b2;
            transform: translateY(-2px);
            box-shadow: 0 8px 30px rgba(6, 182, 212, 0.5);
        }
        
        .breadcrumb { 
            background: var(--bg-card);
            padding: 12px 0; 
            margin-bottom: 24px; 
            font-size: 14px;
            border-radius: 8px;
            border: 1px solid var(--border);
        }
        
        .breadcrumb a { 
            color: var(--accent);
            text-decoration: none;
            transition: color 0.2s;
        }
        
        .breadcrumb a:hover { 
            color: var(--primary);
            text-decoration: underline;
        }
        
        .content { 
            background: var(--bg-card);
            padding: 3rem; 
            border-radius: 16px; 
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
            border: 1px solid var(--border);
        }
        
        .file-list { 
            display: grid; 
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr)); 
            gap: 1.5rem; 
            margin: 2rem 0; 
        }
        
        .file-item { 
            padding: 1.5rem; 
            border: 1px solid var(--border);
            border-radius: 12px; 
            background: var(--bg-main);
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            overflow: hidden;
        }
        
        .file-item::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 3px;
            background: linear-gradient(90deg, var(--primary), var(--secondary));
            opacity: 0;
            transition: opacity 0.3s;
        }
        
        .file-item:hover { 
            transform: translateY(-4px); 
            box-shadow: 0 8px 30px rgba(99, 102, 241, 0.3);
            border-color: var(--primary);
        }
        
        .file-item:hover::before {
            opacity: 1;
        }
        
        .file-item a { 
            color: var(--text-primary);
            text-decoration: none; 
            font-weight: 600; 
            display: block;
            font-size: 1.1rem;
        }
        
        .file-item a:hover { 
            color: var(--primary);
        }
        
        .file-type { 
            font-size: 13px; 
            color: var(--text-secondary);
            margin-top: 8px; 
            font-weight: 500;
        }
        
        /* Code Blocks */
        pre { 
            background: var(--bg-code);
            padding: 1.5rem; 
            border-radius: 12px; 
            overflow-x: auto;
            border: 1px solid var(--border);
            margin: 1.5rem 0;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3);
            position: relative;
        }
        
        pre code { 
            background: none;
            padding: 0;
            font-family: 'Fira Code', 'Consolas', monospace;
            font-size: 0.9em;
            line-height: 1.6;
        }
        
        /* Copy Button */
        .copy-btn {
            position: absolute;
            top: 8px;
            right: 8px;
            background: var(--primary);
            color: white;
            border: none;
            padding: 6px 12px;
            border-radius: 6px;
            cursor: pointer;
            font-size: 12px;
            font-weight: 600;
            transition: all 0.2s;
            opacity: 0.7;
            z-index: 10;
        }
        
        .copy-btn:hover {
            opacity: 1;
            background: var(--primary-dark);
            transform: translateY(-1px);
        }
        
        .copy-btn.copied {
            background: var(--success);
        }
        
        pre:hover .copy-btn {
            opacity: 1;
        }
        
        /* Inline Code */
        code { 
            background: var(--bg-code);
            color: #8b5cf6;
            padding: 3px 8px; 
            border-radius: 6px; 
            font-size: 1.1em;
            font-family: 'Fira Code', 'Consolas', monospace;
            border: 1px solid var(--border);
        }
        
        /* Headings */
        h1, h2, h3, h4, h5, h6 { 
            color: var(--text-primary);
            margin: 2rem 0 1rem 0;
            font-weight: 700;
            letter-spacing: -0.5px;
        }
        
        h1 { 
            font-size: 2.5rem;
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            -webkit-background-clip: text;
            border-bottom: 3px solid var(--primary);
            padding-bottom: 12px;
            margin-bottom: 1.5rem;
        }
        
        h2 { 
            font-size: 2rem;
            color: var(--primary);
            border-bottom: 2px solid var(--border);
            padding-bottom: 8px;
        }
        
        h3 { 
            font-size: 1.5rem;
            color: var(--accent);
        }
        
        /* Links */
        a { 
            color: var(--accent);
            transition: color 0.2s;
        }
        
        a:hover { 
            color: var(--primary);
        }
        
        /* Paragraphs */
        p {
            margin: 1rem 0;
            color: var(--text-secondary);
        }
        
        /* Lists */
        ul, ol {
            margin: 1rem 0;
            padding-left: 2rem;
            color: var(--text-secondary);
        }
        
        li {
            margin: 0.5rem 0;
        }
        
        /* Tables */
        table { 
            border-collapse: collapse;
            width: 100%;
            margin: 1.5rem 0;
            background: var(--bg-main);
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
        }
        
        th, td { 
            border: 1px solid var(--border);
            padding: 12px 16px;
            text-align: left;
        }
        
        th { 
            background: linear-gradient(135deg, var(--primary), var(--secondary));
            color: white;
            font-weight: 600;
        }
        
        tr:hover {
            background: var(--bg-card);
        }
        
        /* Blockquotes */
        blockquote {
            border-left: 4px solid var(--primary);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            background: var(--bg-main);
            border-radius: 0 8px 8px 0;
            color: var(--text-secondary);
            font-style: italic;
        }
        
        /* Horizontal Rule */
        hr {
            border: none;
            border-top: 2px solid var(--border);
            margin: 2rem 0;
        }
        
        .footer { 
            text-align: center; 
            padding: 2rem; 
            color: var(--text-secondary);
            border-top: 1px solid var(--border);
            margin-top: 3rem; 
            font-size: 14px;
        }
        
        /* Scrollbar */
        ::-webkit-scrollbar {
            width: 12px;
            height: 12px;
        }
        
        ::-webkit-scrollbar-track {
            background: var(--bg-main);
        }
        
        ::-webkit-scrollbar-thumb {
            background: var(--border);
            border-radius: 6px;
        }
        
        ::-webkit-scrollbar-thumb:hover {
            background: var(--primary);
        }
        
        @media (max-width: 768px) {
            .container { padding: 10px; }
            .file-list { grid-template-columns: 1fr; }
            .content { padding: 1.5rem; }
            h1 { font-size: 2rem; }
            h2 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>
    <a href="../index.html" class="home-btn">üè†</a>
    <div class="container">
        <div class="breadcrumb">
            <div style="padding: 0 20px;">
                <a href="../index.html">üè† Home</a> <span style="color: #64748b;">/</span> <a href="index.html">06 Mastery and Beyond</a>
            </div>
        </div>
        <div class="content">
            <h1 id="chapter-20-pro-tips-best-practices-and-common-pitfalls">Chapter 20: Pro Tips, Best Practices, and Common Pitfalls</h1>
<h2 id="time-saving-tips">Time-Saving Tips</h2>
<h2 id="time-saving-tips_1">Time-Saving Tips</h2>
<p>As you move from writing a few tests to managing large test suites, efficiency becomes paramount. The time spent waiting for tests to run is time you're not developing. This section covers powerful tools and techniques to tighten your feedback loop, run tests faster, and select only the tests you need, turning your test suite from a slow gatekeeper into a rapid development partner.</p>
<h3 id="2011-watching-tests-with-pytest-watch">20.1.1 Watching Tests with pytest-watch</h3>
<p>The core cycle of Test-Driven Development (TDD) is Red-Green-Refactor. You write a failing test (Red), write the code to make it pass (Green), and then clean up your code (Refactor). This cycle is most effective when it's fast. Manually re-running <code>pytest</code> after every small code change is tedious and breaks your flow.</p>
<p>The <code>pytest-watch</code> plugin automates this process. It monitors your project files for changes and automatically re-runs your tests whenever you save a file.</p>
<p><strong>Installation</strong></p>
<p>First, install the plugin.</p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>pytest-watch
</code></pre></div>

<p><strong>Usage</strong></p>
<p>Imagine you have a simple function and a test for it.</p>
<p><code>src/utils.py</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># src/utils.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A simple function to be tested.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div>

<p><code>tests/test_utils.py</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_utils.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">add</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_add_positive_numbers</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">add</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_add_negative_numbers</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">add</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">)</span> <span class="o">==</span> <span class="o">-</span><span class="mi">5</span>
</code></pre></div>

<p>Instead of running <code>pytest</code> manually, start <code>pytest-watch</code> (often abbreviated as <code>ptw</code>) in your terminal:</p>
<div class="codehilite"><pre><span></span><code>ptw
</code></pre></div>

<p>It will run the tests once, then wait. Now, go into <code>src/utils.py</code> and introduce a bug. For example, change the return line to <code>return a - b</code>. As soon as you save the file, <code>pytest-watch</code> will detect the change and instantly re-run the tests, showing you the failures.</p>
<div class="codehilite"><pre><span></span><code>...
=========================== short test summary info ===========================
FAILED tests/test_utils.py::test_add_positive_numbers - assert 2 + 3 == 5
FAILED tests/test_utils.py::test_add_negative_numbers - assert -2 + -3 == -5
...
&gt;&gt;&gt; Waiting for changes...
</code></pre></div>

<p>Fix the bug, save the file, and the tests will run again, this time passing. This instant feedback loop is invaluable for maintaining focus and productivity. You can pass any standard pytest arguments to <code>ptw</code>, for example <code>ptw -k "positive"</code>.</p>
<h2 id="parallel-test-execution-with-pytest-xdist">Parallel Test Execution with pytest-xdist</h2>
<h2 id="parallel-test-execution-with-pytest-xdist_1">Parallel Test Execution with pytest-xdist</h2>
<p>As a project grows, its test suite can take minutes‚Äîor even hours‚Äîto run. <code>pytest-xdist</code> is a crucial plugin that dramatically speeds up test execution by running tests in parallel across multiple CPU cores.</p>
<p><strong>The Problem: Sequential Execution</strong></p>
<p>By default, pytest runs tests one by one. If you have 100 tests that each take 0.1 seconds, your total run time is 10 seconds. If you have 4 CPU cores, three of them are sitting idle.</p>
<p><strong>The Solution: Parallel Execution</strong></p>
<p><code>pytest-xdist</code> distributes your tests across multiple worker processes, utilizing all available CPU power.</p>
<p><strong>Installation</strong></p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>pytest-xdist
</code></pre></div>

<p><strong>Usage</strong></p>
<p>The primary flag added by <code>pytest-xdist</code> is <code>-n</code> (or <code>--numprocesses</code>). You can specify a number of workers, or let it auto-detect the number of available CPU cores.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Run tests in parallel using all available CPU cores</span>
pytest<span class="w"> </span>-n<span class="w"> </span>auto

<span class="c1"># Run tests using exactly 4 worker processes</span>
pytest<span class="w"> </span>-n<span class="w"> </span><span class="m">4</span>
</code></pre></div>

<p>Let's see it in action. Consider these tests, which simulate some I/O-bound work.</p>
<p><code>tests/test_slow_operations.py</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_slow_operations.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s2">&quot;i&quot;</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_slow_operation</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span>
</code></pre></div>

<p>Running this normally would take at least 4 seconds (8 tests * 0.5s).</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>--durations<span class="o">=</span><span class="m">3</span>
<span class="c1"># ...</span>
<span class="c1"># ========================= slowest 3 durations =========================</span>
<span class="c1"># 0.51s call     tests/test_slow_operations.py::test_slow_operation[1]</span>
<span class="c1"># 0.51s call     tests/test_slow_operations.py::test_slow_operation[0]</span>
<span class="c1"># 0.51s call     tests/test_slow_operations.py::test_slow_operation[2]</span>
<span class="c1"># ========================= 8 passed in 4.08s ===========================</span>
</code></pre></div>

<p>Now, let's run it with <code>pytest-xdist</code> on a machine with 4 cores.</p>
<div class="codehilite"><pre><span></span><code>$<span class="w"> </span>pytest<span class="w"> </span>-n<span class="w"> </span>auto<span class="w"> </span>--durations<span class="o">=</span><span class="m">0</span>
<span class="c1"># ...</span>
<span class="c1"># ========================= 8 passed in 1.15s ===========================</span>
</code></pre></div>

<p>The total time is dramatically reduced. The 8 tests were distributed among the available workers, so instead of running 8 tasks sequentially, we ran 2 sets of 4 parallel tasks.</p>
<p><strong>Important Caveat: Test Interdependency</strong></p>
<p><code>pytest-xdist</code> is a powerful tool, but it exposes a common anti-pattern: test interdependency. If <code>test_b</code> relies on some state created by <code>test_a</code>, your suite will fail unpredictably when run in parallel, because there's no guarantee <code>test_a</code> will run before <code>test_b</code> on the same worker. This forces you to write better, isolated tests, which is a best practice you should follow anyway (see Section 20.3.4).</p>
<h2 id="test-selection-shortcuts">Test Selection Shortcuts</h2>
<h2 id="test-selection-shortcuts_1">Test Selection Shortcuts</h2>
<p>Running the entire test suite is often unnecessary during development. You're typically working on a single feature or fixing a specific bug. Pytest's powerful test selection mechanisms, covered in Chapter 2, are your best friends for saving time. Here's a summary of the most useful shortcuts.</p>
<h3 id="selecting-by-keyword-k">Selecting by Keyword (<code>-k</code>)</h3>
<p>The <code>-k</code> flag allows you to run tests whose names match a given expression.</p>
<p>Let's assume this test file:
<code>tests/test_user_auth.py</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_user_auth.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_user_can_login_with_valid_credentials</span><span class="p">():</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_user_cannot_login_with_invalid_password</span><span class="p">():</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_admin_can_access_dashboard</span><span class="p">():</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_user_cannot_access_admin_dashboard</span><span class="p">():</span>
    <span class="k">assert</span> <span class="kc">True</span>
</code></pre></div>

<p>Here are some examples of using <code>-k</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Run only tests related to login</span>
pytest<span class="w"> </span>-k<span class="w"> </span><span class="s2">&quot;login&quot;</span>

<span class="c1"># Run tests for invalid scenarios</span>
pytest<span class="w"> </span>-k<span class="w"> </span><span class="s2">&quot;cannot&quot;</span>

<span class="c1"># Run tests for admin users</span>
pytest<span class="w"> </span>-k<span class="w"> </span><span class="s2">&quot;admin&quot;</span>

<span class="c1"># Use boolean operators for more complex queries</span>
pytest<span class="w"> </span>-k<span class="w"> </span><span class="s2">&quot;user and not admin&quot;</span>
</code></pre></div>

<h3 id="selecting-by-marker-m">Selecting by Marker (<code>-m</code>)</h3>
<p>Markers (Chapter 6) are tags you can apply to tests to categorize them. This is more robust than keyword matching.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_user_auth.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">smoke</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_user_can_login_with_valid_credentials</span><span class="p">():</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">regression</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_user_cannot_login_with_invalid_password</span><span class="p">():</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">smoke</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">admin</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_admin_can_access_dashboard</span><span class="p">():</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">regression</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_user_cannot_access_admin_dashboard</span><span class="p">():</span>
    <span class="k">assert</span> <span class="kc">True</span>
</code></pre></div>

<p>Now you can select tests by their category:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Run only the quick &quot;smoke&quot; tests</span>
pytest<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;smoke&quot;</span>

<span class="c1"># Run only tests for the admin role</span>
pytest<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;admin&quot;</span>

<span class="c1"># Run smoke tests that are NOT admin tests</span>
pytest<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;smoke and not admin&quot;</span>
</code></pre></div>

<h3 id="selecting-by-file-path-or-node-id">Selecting by File Path or Node ID</h3>
<p>This is the most direct way to run a specific test or group of tests.</p>
<ul>
<li><strong>Run all tests in a directory</strong>: <code>pytest tests/api/</code></li>
<li><strong>Run all tests in a file</strong>: <code>pytest tests/test_user_auth.py</code></li>
<li><strong>Run a specific test function</strong>: <code>pytest tests/test_user_auth.py::test_admin_can_access_dashboard</code></li>
</ul>
<p>Combining these shortcuts allows for surgical precision, saving you immense amounts of time by focusing the test runner only on the code you're actively changing.</p>
<h2 id="using-test-templates">Using Test Templates</h2>
<h2 id="using-test-templates_1">Using Test Templates</h2>
<p>Parametrization (Chapter 5) is the standard way to run the same test logic with different data. However, sometimes you have a set of tests that share a complex structure and behavior but aren't just simple data variations. In these cases, a "test template" pattern using class inheritance can be very effective.</p>
<p>The key is to define a base class with common tests but prevent pytest from discovering it as a test class directly. Then, you create concrete subclasses that provide the specific setup or data.</p>
<p><strong>The Problem: Repetitive Test Logic for Different Implementations</strong></p>
<p>Imagine you have two different database clients that are supposed to follow the same interface. You want to run the exact same suite of tests against both.</p>
<p><strong>The Solution: A Templated Base Class</strong></p>
<p>First, create a base class for your tests. We'll name it <code>BaseTestDatabaseClient</code> so pytest's default discovery (<code>Test*</code>) won't pick it up. This class will contain fixtures and test methods that rely on a <code>client</code> object, which is not yet defined.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_db_clients.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="k">class</span><span class="w"> </span><span class="nc">BaseTestDatabaseClient</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A test template. Pytest will not collect this class because its name</span>
<span class="sd">    does not start with &#39;Test&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">client</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This fixture must be overridden by subclasses.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Subclass must implement this fixture&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">test_connect_disconnect</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">client</span><span class="p">):</span>
        <span class="n">client</span><span class="o">.</span><span class="n">connect</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">client</span><span class="o">.</span><span class="n">is_connected</span><span class="p">()</span>
        <span class="n">client</span><span class="o">.</span><span class="n">disconnect</span><span class="p">()</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">client</span><span class="o">.</span><span class="n">is_connected</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">test_query_returns_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">client</span><span class="p">):</span>
        <span class="n">client</span><span class="o">.</span><span class="n">connect</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;SELECT * FROM users&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
        <span class="n">client</span><span class="o">.</span><span class="n">disconnect</span><span class="p">()</span>

<span class="c1"># Dummy client implementations for the example</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PostgresClient</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_connected</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">connect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">_connected</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">disconnect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">_connected</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_connected</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_connected</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">query</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span> <span class="k">return</span> <span class="p">[{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Alice&quot;</span><span class="p">}]</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SQLiteClient</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_connected</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">connect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">_connected</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">disconnect</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="bp">self</span><span class="o">.</span><span class="n">_connected</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_connected</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_connected</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">query</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span> <span class="k">return</span> <span class="p">[{</span><span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Bob&quot;</span><span class="p">}]</span>
</code></pre></div>

<p>Now, create concrete test classes that inherit from the base template. Each subclass only needs to provide the specific implementation of the <code>client</code> fixture.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_db_clients.py (continued)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TestPostgresClient</span><span class="p">(</span><span class="n">BaseTestDatabaseClient</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tests the PostgresClient against the standard interface.</span>
<span class="sd">    Pytest will collect this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">client</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Provide the concrete implementation for this test suite</span>
        <span class="k">return</span> <span class="n">PostgresClient</span><span class="p">()</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TestSQLiteClient</span><span class="p">(</span><span class="n">BaseTestDatabaseClient</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Tests the SQLiteClient against the standard interface.</span>
<span class="sd">    Pytest will collect this class.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">client</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Provide the concrete implementation for this test suite</span>
        <span class="k">return</span> <span class="n">SQLiteClient</span><span class="p">()</span>
</code></pre></div>

<p>When you run <code>pytest</code>, it will discover <code>TestPostgresClient</code> and <code>TestSQLiteClient</code>. It will run <code>test_connect_disconnect</code> and <code>test_query_returns_data</code> for each of them, using the appropriate client implementation provided by the overridden fixture.</p>
<p>This pattern keeps your test logic DRY while allowing you to test multiple components that adhere to a common interface, a common scenario in plugin architectures or systems with multiple drivers.</p>
<h2 id="industry-hacks-and-patterns">Industry Hacks and Patterns</h2>
<h2 id="industry-hacks-and-patterns_1">Industry Hacks and Patterns</h2>
<p>Beyond the core features of pytest, several powerful patterns have emerged from real-world practice. These "hacks" address common, complex challenges like dealing with legacy code, managing enormous test suites, and ensuring distributed systems work together correctly.</p>
<h3 id="2021-testing-legacy-code-without-refactoring">20.2.1 Testing Legacy Code Without Refactoring</h3>
<p><strong>The Problem:</strong> You're tasked with modifying a complex piece of legacy code that has zero tests. You're afraid to change anything because you don't know what you might break. You can't refactor it to make it testable without tests to ensure your refactoring is safe. It's a classic catch-22.</p>
<p><strong>The Pattern: Characterization Tests</strong></p>
<p>A characterization test (also known as a "golden master" test) is not about verifying <em>correct</em> behavior. It's about verifying the <em>current</em> behavior, bugs and all. The goal is to create a safety net that locks down the system's existing output, allowing you to refactor with confidence.</p>
<p><strong>How to Write One</strong></p>
<ol>
<li><strong>Identify a pure function or a system boundary.</strong> Find a piece of code that takes an input and produces a deterministic output (a value, a file, a log message).</li>
<li><strong>Run the code with a representative input.</strong></li>
<li><strong>Capture the output and save it.</strong> This output is your "golden master." Store it in a file.</li>
<li><strong>Write a test that runs the code again with the same input and asserts that the new output is identical to the saved golden master.</strong></li>
</ol>
<p><strong>Example</strong></p>
<p>Imagine this convoluted legacy function you need to refactor:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># src/legacy_report.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_report</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># A complex, hard-to-read function with weird formatting and logic</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;--- REPORT ---&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">item</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">50</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="s2">&quot;HIGH&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="s2">&quot;low&quot;</span> <span class="c1"># Inconsistent case</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">. </span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> [</span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;--- END ---&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
</code></pre></div>

<p>You don't want to analyze its logic; you just want to preserve its behavior.</p>
<p>First, create a test file and a place to store the golden master.
<code>tests/test_legacy_report.py</code>:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_legacy_report.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.legacy_report</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate_report</span>

<span class="c1"># Define the path to our &quot;golden master&quot; file</span>
<span class="n">GOLDEN_MASTER_PATH</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span> <span class="o">/</span> <span class="s2">&quot;golden_master_report.txt&quot;</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_generate_report_characterization</span><span class="p">():</span>
    <span class="n">sample_data</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Sensor A&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="mi">75</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Sensor B&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">},</span>
    <span class="p">]</span>

    <span class="c1"># Generate the current output</span>
    <span class="n">actual_output</span> <span class="o">=</span> <span class="n">generate_report</span><span class="p">(</span><span class="n">sample_data</span><span class="p">)</span>

    <span class="c1"># If the golden master doesn&#39;t exist, create it.</span>
    <span class="c1"># This is a one-time setup step.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">GOLDEN_MASTER_PATH</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="n">GOLDEN_MASTER_PATH</span><span class="o">.</span><span class="n">write_text</span><span class="p">(</span><span class="n">actual_output</span><span class="p">)</span>
        <span class="n">pytest</span><span class="o">.</span><span class="n">fail</span><span class="p">(</span><span class="s2">&quot;Golden master file created. Re-run the test.&quot;</span><span class="p">)</span>

    <span class="c1"># Compare the current output to the golden master</span>
    <span class="n">expected_output</span> <span class="o">=</span> <span class="n">GOLDEN_MASTER_PATH</span><span class="o">.</span><span class="n">read_text</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">actual_output</span> <span class="o">==</span> <span class="n">expected_output</span>
</code></pre></div>

<p>The first time you run this test, it will fail, but it will create the <code>golden_master_report.txt</code> file:</p>
<p><code>tests/golden_master_report.txt</code>:</p>
<div class="codehilite"><pre><span></span><code>--- REPORT ---
1. Sensor A: 75 [HIGH]
2. Sensor B: 30 [low]
--- END ---
</code></pre></div>

<p>The second time you run the test, it will pass. Now, the behavior is "characterized." You can refactor <code>generate_report</code> with confidence. If you accidentally change the output (e.g., by fixing the capitalization of "low"), the test will fail, alerting you to the change. If the change was intentional, you simply delete the golden master file and re-run the test to generate a new one.</p>
<h2 id="incremental-testing-for-large-projects">Incremental Testing for Large Projects</h2>
<h2 id="incremental-testing-for-large-projects_1">Incremental Testing for Large Projects</h2>
<p>On massive projects, even with <code>pytest-xdist</code>, running the full test suite can be too slow for a tight feedback loop. Incremental testing strategies allow you to run only the tests most relevant to your recent changes.</p>
<h3 id="run-only-failed-tests-last-failed-or-lf">Run Only Failed Tests (<code>--last-failed</code> or <code>--lf</code>)</h3>
<p>This is the simplest and most effective incremental strategy. After a test run, pytest saves a cache of which tests failed. The <code>--lf</code> flag tells pytest to run <em>only</em> the tests that failed on the last run.</p>
<p><strong>Workflow:</strong>
1.  Run the full suite: <code>pytest</code>
2.  See 5 failures out of 2000 tests.
3.  Fix the code.
4.  Run <code>pytest --lf</code>. Pytest will now only execute those 5 tests.
5.  Once they pass, run <code>pytest --lf</code> again. Since there are no more known failures, pytest will run the full suite to ensure your fix didn't break anything else.</p>
<h3 id="run-failed-tests-first-failed-first-or-ff">Run Failed Tests First (<code>--failed-first</code> or <code>--ff</code>)</h3>
<p>This is a variation of <code>--lf</code>. It runs the previously failed tests first, and if they all pass, it proceeds to run the rest of the tests. This gives you the fastest possible feedback on your fix while still providing the safety of a full run.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Run last failed tests, then the rest if they pass</span>
pytest<span class="w"> </span>--ff
</code></pre></div>

<h3 id="testing-based-on-code-changes">Testing Based on Code Changes</h3>
<p>For even more advanced workflows, plugins can integrate with your version control system (like Git) to run only the tests that cover the code you've recently changed. A popular plugin for this is <code>pytest-testmon</code>. It monitors which tests execute which lines of code. When you change a file, it knows exactly which tests need to be re-run.</p>
<p><strong>Installation</strong></p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>pytest-testmon
</code></pre></div>

<p><strong>Usage</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Run pytest with testmon enabled</span>
pytest<span class="w"> </span>--testmon
</code></pre></div>

<p>The first run will be normal. After that, if you change a source file, <code>pytest --testmon</code> will only run the tests that were affected by that change, resulting in near-instantaneous feedback on large codebases.</p>
<h2 id="contract-testing-for-microservices">Contract Testing for Microservices</h2>
<h2 id="contract-testing-for-microservices_1">Contract Testing for Microservices</h2>
<p><strong>The Problem:</strong> In a microservices architecture, you have a <code>UserService</code> and a <code>BillingService</code>. The <code>BillingService</code> depends on data from the <code>UserService</code>'s API. How do you ensure they can communicate without running slow, brittle end-to-end tests that require deploying both services? If the <code>UserService</code> team changes an API endpoint field from <code>user_id</code> to <code>userId</code>, the <code>BillingService</code> will break in production.</p>
<p><strong>The Pattern: Contract Testing</strong></p>
<p>Contract testing is a technique to ensure that two separate systems (e.g., an API provider and a consumer) are compatible without testing them directly together.</p>
<p>The consumer (<code>BillingService</code>) defines a "contract"‚Äîa file specifying the requests it will make and the responses it expects to receive. The provider (<code>UserService</code>) then uses this contract in its own test suite to verify that it fulfills the consumer's expectations.</p>
<p>While tools like Pact are industry standards for this, you can implement a simplified version of this pattern in pytest using a shared data file.</p>
<p><strong>Example Workflow</strong></p>
<ol>
<li>
<p><strong>The Consumer Defines the Contract</strong></p>
<p>The <code>BillingService</code> team writes a test that mocks the <code>UserService</code> API. As part of this, they generate a contract file.</p>
<p><code>billing_service/tests/test_user_client.py</code>:</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># billing_service/tests/test_user_client.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_get_user_and_generate_contract</span><span class="p">():</span>
    <span class="c1"># This test defines what the BillingService expects from the UserService</span>
    <span class="n">expected_user_data</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;userId&quot;</span><span class="p">:</span> <span class="mi">123</span><span class="p">,</span>
        <span class="s2">&quot;email&quot;</span><span class="p">:</span> <span class="s2">&quot;test@example.com&quot;</span><span class="p">,</span>
        <span class="s2">&quot;subscription_level&quot;</span><span class="p">:</span> <span class="s2">&quot;premium&quot;</span>
    <span class="p">}</span>

    <span class="c1"># In a real test, you&#39;d use this data to mock the API call</span>
    <span class="c1"># and test your client.</span>
    <span class="c1"># ... client logic test ...</span>

    <span class="c1"># Generate the contract file</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;../contracts/billing_service_expects_user.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">expected_user_data</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">assert</span> <span class="kc">True</span> <span class="c1"># Test passes by generating the contract</span>
</code></pre></div>

<p>This contract is then committed to a shared repository or sent to the <code>UserService</code> team.</p>
<ol>
<li>
<p><strong>The Provider Verifies the Contract</strong></p>
<p>The <code>UserService</code> team adds a test that uses this contract to validate their actual API endpoint.</p>
<p><code>user_service/tests/test_api_contracts.py</code>:</p>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># user_service/tests/test_api_contracts.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">user_service.api</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_user_by_id</span> <span class="c1"># The actual API implementation</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_fulfills_billing_service_contract</span><span class="p">():</span>
    <span class="c1"># Load the contract defined by the consumer</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;../contracts/billing_service_expects_user.json&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">contract</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

    <span class="c1"># Call the actual API endpoint</span>
    <span class="n">user_id</span> <span class="o">=</span> <span class="n">contract</span><span class="p">[</span><span class="s2">&quot;userId&quot;</span><span class="p">]</span>
    <span class="n">actual_user_data</span> <span class="o">=</span> <span class="n">get_user_by_id</span><span class="p">(</span><span class="n">user_id</span><span class="p">)</span>

    <span class="c1"># Verify that our actual response contains all the fields</span>
    <span class="c1"># the consumer expects.</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">contract</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">assert</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">actual_user_data</span>
        <span class="c1"># You might want more specific type checks here too</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">actual_user_data</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="nb">type</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

    <span class="c1"># This test now ensures we don&#39;t break the BillingService.</span>
</code></pre></div>

<p>Now, if a developer on the <code>UserService</code> team renames <code>userId</code> to <code>user_id</code>, <code>test_fulfills_billing_service_contract</code> will fail in their CI pipeline, preventing them from deploying a breaking change long before it reaches production. This pattern allows teams to work independently while ensuring their services remain compatible.</p>
<h2 id="property-based-testing-with-hypothesis">Property-Based Testing with Hypothesis</h2>
<h2 id="property-based-testing-with-hypothesis_1">Property-Based Testing with Hypothesis</h2>
<p>So far, all our tests have been <em>example-based</em>. We, the developers, think of specific inputs (<code>add(2, 3)</code>) and assert a specific output (<code>== 5</code>). The weakness of this approach is that we might not think of the tricky edge cases.</p>
<p><em>Property-based testing</em> flips this around. Instead of testing for specific outcomes, you state general properties of your code that should hold true for <em>all</em> valid inputs. A library then generates hundreds of different, often surprising, inputs to try and prove your property wrong.</p>
<p>The premier library for this in Python is <code>Hypothesis</code>.</p>
<p><strong>Installation</strong></p>
<div class="codehilite"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>hypothesis
</code></pre></div>

<p><strong>Example: Testing an Encoding Function</strong></p>
<p>Let's say we have a function that encodes and decodes a string.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># src/encoding.py</span>
<span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span><span class="n">input_string</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bytes</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">input_string</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="n">input_bytes</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">input_bytes</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span>
</code></pre></div>

<p>An example-based test might look like this:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_encoding_example.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.encoding</span><span class="w"> </span><span class="kn">import</span> <span class="n">encode</span><span class="p">,</span> <span class="n">decode</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_simple_string_roundtrip</span><span class="p">():</span>
    <span class="n">original</span> <span class="o">=</span> <span class="s2">&quot;hello world&quot;</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">encode</span><span class="p">(</span><span class="n">original</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="n">decode</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">decoded</span> <span class="o">==</span> <span class="n">original</span>
</code></pre></div>

<p>This is good, but what about empty strings? Strings with emoji? Strings with null bytes? We'd have to write a separate test for each case.</p>
<p>With Hypothesis, we describe the <em>property</em> we want to test: "for any valid text string, decoding the encoded version of it should result in the original string."</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># tests/test_encoding_property.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">hypothesis</span><span class="w"> </span><span class="kn">import</span> <span class="n">given</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">hypothesis</span><span class="w"> </span><span class="kn">import</span> <span class="n">strategies</span> <span class="k">as</span> <span class="n">st</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.encoding</span><span class="w"> </span><span class="kn">import</span> <span class="n">encode</span><span class="p">,</span> <span class="n">decode</span>

<span class="c1"># The `given` decorator tells Hypothesis to run this test many times</span>
<span class="c1"># with different arguments.</span>
<span class="c1"># `st.text()` is a &quot;strategy&quot; that generates arbitrary text strings.</span>
<span class="nd">@given</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">text</span><span class="p">())</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_string_roundtrip_property</span><span class="p">(</span><span class="n">original_string</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Property: Encoding and then decoding a string should return the original.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">encode</span><span class="p">(</span><span class="n">original_string</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="n">decode</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">decoded</span> <span class="o">==</span> <span class="n">original_string</span>
</code></pre></div>

<p>When you run this test, Hypothesis gets to work. It will generate simple strings (<code>""</code>, <code>"a"</code>), complex strings (<code>"‰Ω†Â•Ω"</code>, <code>"\x00"</code>), and long, convoluted strings. It intelligently searches for inputs that are likely to cause problems. If it finds an input that makes your assertion fail, it will report the <em>simplest possible failing example</em>.</p>
<p>For instance, if our <code>decode</code> function had a bug with non-ASCII characters, Hypothesis would run hundreds of examples and might report a failure like this:</p>
<div class="codehilite"><pre><span></span><code>Falsifying example: test_string_roundtrip_property(original_string=&#39;‚Ç¨&#39;)
</code></pre></div>

<p>Property-based testing doesn't replace example-based testing‚Äîit complements it. Use examples for clear, simple business cases and properties for catching a wide range of edge cases in your algorithms and data processing logic.</p>
<h2 id="best-practices-summary">Best Practices Summary</h2>
<h2 id="best-practices-summary_1">Best Practices Summary</h2>
<p>Throughout this book, we've explored not just the "how" but also the "why" of testing. This section distills the most important philosophical principles and best practices into a concise summary. Following these guidelines will lead to a test suite that is not only effective but also readable, maintainable, and trustworthy.</p>
<h3 id="2031-keep-tests-simple-and-readable">20.3.1 Keep Tests Simple and Readable</h3>
<p>A common debate in software is DRY (Don't Repeat Yourself) vs. DAMP (Descriptive and Meaningful Phrases). While DRY is a virtue in application code, in test code, DAMP is often more important.</p>
<p>A test should read like a story, explaining what is being set up, what action is being taken, and what outcome is expected. It's often better to repeat a few lines of setup code in two different tests than to hide them behind a complex fixture that makes the tests harder to understand in isolation.</p>
<p><strong>Bad: Overly DRY test</strong></p>
<div class="codehilite"><pre><span></span><code><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">complex_user_setup</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
    <span class="c1"># ... 15 lines of complex setup logic based on request.param ...</span>
    <span class="k">return</span> <span class="n">user</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s2">&quot;complex_user_setup&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;admin&quot;</span><span class="p">,</span> <span class="s2">&quot;guest&quot;</span><span class="p">],</span> <span class="n">indirect</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_user_behavior</span><span class="p">(</span><span class="n">complex_user_setup</span><span class="p">):</span>
    <span class="c1"># What is this test actually verifying? It&#39;s hard to tell.</span>
    <span class="k">assert</span> <span class="n">complex_user_setup</span><span class="o">.</span><span class="n">can_do_thing</span><span class="p">()</span>
</code></pre></div>

<p><strong>Good: Readable and explicit tests</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">test_admin_user_can_do_thing</span><span class="p">():</span>
    <span class="n">user</span> <span class="o">=</span> <span class="n">create_user</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;admin&quot;</span><span class="p">)</span>
    <span class="c1"># ... 2 lines of specific setup for this test ...</span>
    <span class="k">assert</span> <span class="n">user</span><span class="o">.</span><span class="n">can_do_thing</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_guest_user_cannot_do_thing</span><span class="p">():</span>
    <span class="n">user</span> <span class="o">=</span> <span class="n">create_user</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;guest&quot;</span><span class="p">)</span>
    <span class="c1"># ... 2 lines of specific setup for this test ...</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">user</span><span class="o">.</span><span class="n">can_do_thing</span><span class="p">()</span>
</code></pre></div>

<p>Clarity in tests is a feature. When a test fails six months from now, you should be able to understand what it's testing without having to debug the test itself.</p>
<h3 id="2032-one-assertion-per-test-usually">20.3.2 One Assertion Per Test (Usually)</h3>
<p>The ideal test focuses on a single concept or behavior. This makes test failures easier to diagnose. If a test with five assertions fails, you have to figure out which of the five behaviors is broken. If five separate tests fail, the test names themselves tell you exactly what's broken.</p>
<p><strong>Guideline:</strong> A test should verify one logical concept.</p>
<p>This doesn't strictly mean one <code>assert</code> statement. It's perfectly fine to assert multiple properties of a single object if they form a coherent whole.</p>
<p><strong>Good: Multiple assertions for one concept</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">test_create_user_returns_user_object_with_defaults</span><span class="p">():</span>
    <span class="n">user</span> <span class="o">=</span> <span class="n">User</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">email</span><span class="o">=</span><span class="s2">&quot;test@example.com&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">user</span><span class="o">.</span><span class="n">email</span> <span class="o">==</span> <span class="s2">&quot;test@example.com&quot;</span>
    <span class="k">assert</span> <span class="n">user</span><span class="o">.</span><span class="n">is_active</span> <span class="ow">is</span> <span class="kc">True</span>
    <span class="k">assert</span> <span class="n">user</span><span class="o">.</span><span class="n">role</span> <span class="o">==</span> <span class="s2">&quot;viewer&quot;</span>
</code></pre></div>

<p><strong>Bad: Multiple concepts in one test</strong></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">test_user_creation_and_login</span><span class="p">():</span>
    <span class="c1"># Concept 1: User Creation</span>
    <span class="n">user</span> <span class="o">=</span> <span class="n">User</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">email</span><span class="o">=</span><span class="s2">&quot;test@example.com&quot;</span><span class="p">,</span> <span class="n">password</span><span class="o">=</span><span class="s2">&quot;pw&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">user</span><span class="o">.</span><span class="n">id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="c1"># Concept 2: User Login</span>
    <span class="n">session_token</span> <span class="o">=</span> <span class="n">login</span><span class="p">(</span><span class="n">user</span><span class="o">.</span><span class="n">email</span><span class="p">,</span> <span class="s2">&quot;pw&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">session_token</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
</code></pre></div>

<p>If this test fails, is the problem with creation or login? Splitting it into <code>test_user_creation</code> and <code>test_user_login</code> makes the suite more precise.</p>
<h3 id="2033-name-tests-to-describe-what-they-test">20.3.3 Name Tests to Describe What They Test</h3>
<p>Test names are documentation. A well-named test tells you what the code is supposed to do. When it fails, the name tells you what the code is <em>not</em> doing. A popular and effective naming convention is:</p>
<p><code>test_unitOfWork_stateUnderTest_expectedBehavior</code></p>
<p><strong>Bad:</strong> <code>test_auth()</code>
<strong>Slightly Better:</strong> <code>test_login_fails()</code>
<strong>Good:</strong> <code>test_login_with_invalid_password_returns_error()</code></p>
<p>Let's apply this to a real example:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Good, descriptive test names</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_payment_processor_with_valid_card_succeeds</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_payment_processor_with_expired_card_raises_exception</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_payment_processor_when_fraud_service_is_down_uses_fallback</span><span class="p">():</span>
    <span class="k">pass</span>
</code></pre></div>

<p>When you see <code>FAILED: test_payment_processor_with_expired_card_raises_exception</code>, you know exactly what feature is broken without even looking at the code.</p>
<h3 id="2034-avoid-test-interdependency">20.3.4 Avoid Test Interdependency</h3>
<p>Each test should be a self-contained universe. It should be able to run independently and in any order relative to other tests. Relying on side effects from other tests is a recipe for a flaky and unreliable test suite.</p>
<p><strong>The Anti-Pattern:</strong></p>
<div class="codehilite"><pre><span></span><code><span class="c1"># ANTI-PATTERN: DO NOT DO THIS</span>
<span class="n">db</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_add_item</span><span class="p">():</span>
    <span class="n">db</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;item1&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">db</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_remove_item</span><span class="p">():</span>
    <span class="c1"># This test depends on test_add_item running first!</span>
    <span class="n">db</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">db</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div>

<p>If you run these tests with <code>pytest-xdist</code> or in a different order (<code>pytest tests.py::test_remove_item tests.py::test_add_item</code>), <code>test_remove_item</code> will fail.</p>
<p><strong>The Solution:</strong> Use fixtures to guarantee a clean state for every test.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># GOOD: Each test gets a clean state</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">db</span><span class="p">():</span>
    <span class="c1"># The fixture provides a fresh database for each test</span>
    <span class="k">return</span> <span class="p">[]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_add_item</span><span class="p">(</span><span class="n">db</span><span class="p">):</span>
    <span class="n">db</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;item1&quot;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">db</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_list_is_initially_empty</span><span class="p">(</span><span class="n">db</span><span class="p">):</span>
    <span class="c1"># This test runs with its own empty `db` fixture instance</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">db</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div>

<p>This is one of the most critical principles for a scalable and reliable test suite.</p>
<h3 id="2035-use-fixtures-for-setup-not-test-data">20.3.5 Use Fixtures for Setup, Not Test Data</h3>
<p>This is a subtle but important distinction.</p>
<ul>
<li><strong>Fixtures</strong> are for creating the <em>context</em> or <em>environment</em> your test runs in. Think database connections, authenticated user objects, temporary directories, or a running web server instance. They provide the "stage" for your test.</li>
<li><strong><code>@pytest.mark.parametrize</code></strong> is for providing the <em>data</em> or <em>inputs</em> your test will act upon. Think different usernames, various numbers to be calculated, or different search queries.</li>
</ul>
<p><strong>Blurring the lines (less ideal):</strong></p>
<div class="codehilite"><pre><span></span><code><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;admin&quot;</span><span class="p">,</span> <span class="s2">&quot;editor&quot;</span><span class="p">,</span> <span class="s2">&quot;viewer&quot;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">user</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">create_user</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="n">request</span><span class="o">.</span><span class="n">param</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_user_permissions</span><span class="p">(</span><span class="n">user</span><span class="p">):</span>
    <span class="c1"># This test is now doing three different things depending on the user role.</span>
    <span class="c1"># It&#39;s less clear what the specific intent is for each role.</span>
    <span class="k">if</span> <span class="n">user</span><span class="o">.</span><span class="n">role</span> <span class="o">==</span> <span class="s2">&quot;admin&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">user</span><span class="o">.</span><span class="n">can_delete</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">user</span><span class="o">.</span><span class="n">can_delete</span><span class="p">()</span>
</code></pre></div>

<p><strong>Clear separation (better):</strong></p>
<div class="codehilite"><pre><span></span><code><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">admin_user</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">create_user</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;admin&quot;</span><span class="p">)</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span><span class="w"> </span><span class="nf">viewer_user</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">create_user</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;viewer&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_admin_can_delete</span><span class="p">(</span><span class="n">admin_user</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">admin_user</span><span class="o">.</span><span class="n">can_delete</span><span class="p">()</span>

<span class="k">def</span><span class="w"> </span><span class="nf">test_viewer_cannot_delete</span><span class="p">(</span><span class="n">viewer_user</span><span class="p">):</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">viewer_user</span><span class="o">.</span><span class="n">can_delete</span><span class="p">()</span>

<span class="c1"># Use parametrize for variations on the same logical test</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s2">&quot;username&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;valid_user&quot;</span><span class="p">,</span> <span class="s2">&quot;user-with-dashes&quot;</span><span class="p">,</span> <span class="s2">&quot;u&quot;</span><span class="p">])</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_valid_usernames</span><span class="p">(</span><span class="n">username</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">is_valid_username</span><span class="p">(</span><span class="n">username</span><span class="p">)</span>
</code></pre></div>

<p>Using fixtures for context and parametrization for data leads to tests that are more explicit, readable, and easier to maintain.</p>
<h2 id="common-pitfalls-to-avoid">Common Pitfalls to Avoid</h2>
<h2 id="common-pitfalls-to-avoid_1">Common Pitfalls to Avoid</h2>
<p>Knowing what <em>not</em> to do is as important as knowing what to do. Many test suites become brittle, slow, and untrustworthy because of a few common anti-patterns. This section highlights these pitfalls and shows you how to steer clear of them.</p>
<h3 id="2041-over-mocking-your-code">20.4.1 Over-Mocking Your Code</h3>
<p>Mocking (Chapter 8) is a powerful tool for isolating code, but it's easy to overuse. Over-mocking leads to "brittle" tests‚Äîtests that are so tightly coupled to the implementation details of your code that they break every time you refactor, even if the public behavior remains correct.</p>
<p><strong>The Pitfall:</strong> A test that mocks every collaborator of the function under test.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># src/service.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">db</span><span class="p">,</span> <span class="n">api</span><span class="p">,</span> <span class="n">logger</span>

<span class="k">def</span><span class="w"> </span><span class="nf">process_user_data</span><span class="p">(</span><span class="n">user_id</span><span class="p">):</span>
    <span class="n">user</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">get_user</span><span class="p">(</span><span class="n">user_id</span><span class="p">)</span>
    <span class="n">api_data</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">fetch_extra_data</span><span class="p">(</span><span class="n">user</span><span class="o">.</span><span class="n">email</span><span class="p">)</span>
    <span class="n">user</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">api_data</span><span class="p">)</span>
    <span class="n">db</span><span class="o">.</span><span class="n">save_user</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Processed user </span><span class="si">{</span><span class="n">user_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="s2">&quot;OK&quot;</span>

<span class="c1"># tests/test_service.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">unittest.mock</span><span class="w"> </span><span class="kn">import</span> <span class="n">patch</span>

<span class="c1"># BAD: This test knows too much about the implementation</span>
<span class="nd">@patch</span><span class="p">(</span><span class="s2">&quot;src.service.logger&quot;</span><span class="p">)</span>
<span class="nd">@patch</span><span class="p">(</span><span class="s2">&quot;src.service.api&quot;</span><span class="p">)</span>
<span class="nd">@patch</span><span class="p">(</span><span class="s2">&quot;src.service.db&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_process_user_data_brittle</span><span class="p">(</span><span class="n">mock_db</span><span class="p">,</span> <span class="n">mock_api</span><span class="p">,</span> <span class="n">mock_logger</span><span class="p">):</span>
    <span class="c1"># This test is just re-implementing the function logic in the test itself.</span>
    <span class="c1"># It doesn&#39;t actually test the integration of the components.</span>
    <span class="n">mock_user</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;email&quot;</span><span class="p">:</span> <span class="s2">&quot;test@example.com&quot;</span><span class="p">}</span>
    <span class="n">mock_db</span><span class="o">.</span><span class="n">get_user</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="n">mock_user</span>
    <span class="n">mock_api</span><span class="o">.</span><span class="n">fetch_extra_data</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;extra&quot;</span><span class="p">:</span> <span class="s2">&quot;data&quot;</span><span class="p">}</span>

    <span class="n">process_user_data</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

    <span class="n">mock_db</span><span class="o">.</span><span class="n">get_user</span><span class="o">.</span><span class="n">assert_called_with</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">mock_api</span><span class="o">.</span><span class="n">fetch_extra_data</span><span class="o">.</span><span class="n">assert_called_with</span><span class="p">(</span><span class="s2">&quot;test@example.com&quot;</span><span class="p">)</span>
    <span class="n">mock_db</span><span class="o">.</span><span class="n">save_user</span><span class="o">.</span><span class="n">assert_called_with</span><span class="p">({</span><span class="s2">&quot;email&quot;</span><span class="p">:</span> <span class="s2">&quot;test@example.com&quot;</span><span class="p">,</span> <span class="s2">&quot;extra&quot;</span><span class="p">:</span> <span class="s2">&quot;data&quot;</span><span class="p">})</span>
    <span class="n">mock_logger</span><span class="o">.</span><span class="n">log</span><span class="o">.</span><span class="n">assert_called_with</span><span class="p">(</span><span class="s2">&quot;Processed user 123&quot;</span><span class="p">)</span>
</code></pre></div>

<p>This test is fragile. If you decide to rename a variable or call the logger before saving the user, the test breaks, even though the end result is the same. The test is coupled to the <em>implementation</em>, not the <em>behavior</em>.</p>
<p><strong>The Solution:</strong> Mock at the boundaries of your system. For an integration test like this, it's often better to use a real (but test-specific) database and only mock external services like the third-party API. This provides much more confidence that your components work together correctly.</p>
<h3 id="2042-testing-implementation-instead-of-behavior">20.4.2 Testing Implementation Instead of Behavior</h3>
<p>This is a close cousin of over-mocking. A good test verifies the public contract or observable behavior of a unit of code. A bad test verifies the internal workings.</p>
<p><strong>The Pitfall:</strong> Testing a private method.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># src/calculator.py</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PriceCalculator</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tax_rate</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tax_rate</span> <span class="o">=</span> <span class="n">tax_rate</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_calculate_tax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">price</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">price</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tax_rate</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_total_price</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">price</span><span class="p">):</span>
        <span class="n">tax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_tax</span><span class="p">(</span><span class="n">price</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">price</span> <span class="o">+</span> <span class="n">tax</span>

<span class="c1"># tests/test_calculator.py</span>
<span class="c1"># BAD: Testing a private method directly</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_calculate_tax_private_method</span><span class="p">():</span>
    <span class="n">calc</span> <span class="o">=</span> <span class="n">PriceCalculator</span><span class="p">(</span><span class="n">tax_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="c1"># Python lets you do this, but you shouldn&#39;t.</span>
    <span class="k">assert</span> <span class="n">calc</span><span class="o">.</span><span class="n">_calculate_tax</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">==</span> <span class="mi">10</span>
</code></pre></div>

<p>What's wrong with this? Imagine you refactor <code>PriceCalculator</code> to be more efficient, perhaps by inlining the calculation:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># src/calculator.py (refactored)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PriceCalculator</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tax_rate</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_tax_rate</span> <span class="o">=</span> <span class="n">tax_rate</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_total_price</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">price</span><span class="p">):</span>
        <span class="c1"># The private method is gone!</span>
        <span class="k">return</span> <span class="n">price</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_tax_rate</span><span class="p">)</span>
</code></pre></div>

<p>The public behavior of <code>get_total_price</code> is identical, but <code>test_calculate_tax_private_method</code> now fails with an <code>AttributeError</code>. Your tests are hindering refactoring instead of enabling it.</p>
<p><strong>The Solution:</strong> Test through the public interface.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># GOOD: Testing the public behavior</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_get_total_price_includes_tax</span><span class="p">():</span>
    <span class="n">calc</span> <span class="o">=</span> <span class="n">PriceCalculator</span><span class="p">(</span><span class="n">tax_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">calc</span><span class="o">.</span><span class="n">get_total_price</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">==</span> <span class="mi">110</span>
</code></pre></div>

<p>This test verifies the correct outcome regardless of how the tax is calculated internally. It will continue to pass after the refactoring, giving you confidence that you haven't broken anything.</p>
<h3 id="2043-flaky-tests-and-timing-issues">20.4.3 Flaky Tests and Timing Issues</h3>
<p>A flaky test is a test that sometimes passes and sometimes fails without any code changes. These are insidious because they destroy trust in your test suite. If developers see a test failing intermittently, they'll start to ignore it, and soon real failures will be missed.</p>
<p><strong>Common Causes and Solutions:</strong></p>
<ol>
<li><strong>Time Dependency:</strong> Tests that use <code>datetime.now()</code> or <code>time.time()</code> can fail if they happen to run across a boundary (e.g., midnight).<ul>
<li><strong>Solution:</strong> Use a library like <code>freezegun</code> to control the current time.</li>
</ul>
</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="c1"># pip install freezegun</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">freezegun</span><span class="w"> </span><span class="kn">import</span> <span class="n">freeze_time</span>

<span class="nd">@freeze_time</span><span class="p">(</span><span class="s2">&quot;2023-01-01 12:00:00&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_something_time_sensitive</span><span class="p">():</span>
    <span class="c1"># datetime.now() will always return the frozen time inside this test</span>
    <span class="o">...</span>
</code></pre></div>

<ol>
<li>
<p><strong>Race Conditions:</strong> In tests involving threading or asynchronous operations, you might assert a result before the operation has had time to complete.</p>
<ul>
<li><strong>Solution:</strong> Use explicit synchronization (locks, events) or polling with a timeout. Avoid <code>time.sleep()</code> as it's unreliable and slow. For async code, properly <code>await</code> all operations.</li>
</ul>
</li>
<li>
<p><strong>Random Data:</strong> Tests that rely on <code>random</code> can fail on an unlucky run.</p>
<ul>
<li><strong>Solution:</strong> Set a fixed seed (<code>random.seed(0)</code>) at the beginning of the test to make the "random" sequence deterministic.</li>
</ul>
</li>
<li>
<p><strong>External Service Unavailability:</strong> Tests that make real network calls to a third-party service will fail if that service is down or slow.</p>
<ul>
<li><strong>Solution:</strong> Mock the external service (as discussed in Chapter 12). Your test suite should not depend on the internet.</li>
</ul>
</li>
</ol>
<h3 id="2044-ignoring-test-maintenance">20.4.4 Ignoring Test Maintenance</h3>
<p>Tests are not write-only code. They are a living part of your codebase and require the same care and maintenance as your application code.</p>
<p><strong>The Pitfall:</strong> A test suite full of commented-out tests, ignored failures (<code>@pytest.mark.xfail(reason="TODO: fix this")</code> that's been there for a year), and convoluted, unreadable test code. This is "test debt."</p>
<p><strong>The Solution:</strong> Treat your tests as first-class citizens.
-   <strong>Refactor tests:</strong> When you refactor application code, refactor the corresponding tests to keep them clean and readable.
-   <strong>Delete obsolete tests:</strong> If you remove a feature, delete its tests. A large, slow test suite is a liability.
-   <strong>Address failures immediately:</strong> A failing test on the main branch should be treated as a critical bug. A clean test suite is a useful test suite.</p>
<h3 id="2045-coverage-theater">20.4.5 Coverage Theater</h3>
<p>As we saw in Chapter 13, code coverage is a useful metric, but it's not a measure of test quality. "Coverage theater" is the practice of chasing a 100% coverage number at the expense of writing meaningful tests.</p>
<p><strong>The Pitfall:</strong> Writing tests that execute lines of code without actually asserting anything about their behavior.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">process_data</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;safe&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">data</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot process empty data&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;fast&quot;</span><span class="p">:</span>
        <span class="c1"># ... complex logic ...</span>
        <span class="k">return</span> <span class="s2">&quot;fast_processed&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># ... other complex logic ...</span>
        <span class="k">return</span> <span class="s2">&quot;safe_processed&quot;</span>

<span class="c1"># BAD: This test achieves 100% coverage but is useless</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_process_data_for_coverage</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">process_data</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">process_data</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;fast&quot;</span><span class="p">)</span>
    <span class="n">process_data</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;safe&quot;</span><span class="p">)</span>
</code></pre></div>

<p>This test will give you 100% line coverage for <code>process_data</code>, but it asserts nothing about the return values. A bug could be introduced in the "fast" path, and this test would still pass.</p>
<p><strong>The Solution:</strong> Focus on testing behaviors and properties. Write assertions that matter. It's better to have 80% coverage with strong assertions than 100% coverage with weak or missing assertions.</p>
<h3 id="2046-tests-that-pass-when-they-shouldnt">20.4.6 Tests That Pass When They Shouldn't</h3>
<p>This is the most dangerous pitfall: a test that gives you a green checkmark but isn't actually testing what you think it is.</p>
<p><strong>The Pitfall:</strong> A <code>pytest.raises</code> block that never raises.</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">test_divide_by_zero_should_raise_error</span><span class="p">():</span>
    <span class="c1"># Imagine we have a bug and `1 / 0` doesn&#39;t raise an error</span>
    <span class="c1"># or we accidentally test `1 / 1`.</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">ZeroDivisionError</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">1</span> <span class="c1"># Oops, wrong input!</span>

    <span class="c1"># This test will PASS because no exception was raised *inside* the block,</span>
    <span class="c1"># and the code after the block is never reached. Pytest can&#39;t know</span>
    <span class="c1"># that the exception was *supposed* to happen.</span>
</code></pre></div>

<p>The test passes silently, giving you a false sense of security.</p>
<p><strong>The Solution: The Red-Green-Refactor Cycle</strong></p>
<p>The best way to avoid this is to follow the TDD discipline: <strong>always see the test fail first</strong>.</p>
<ol>
<li><strong>Red:</strong> Write the test for the feature that doesn't exist yet. Run it. It should fail (e.g., <code>NameError</code>, <code>AssertionError</code>). This proves your test is correctly wired up and capable of failing.</li>
<li><strong>Green:</strong> Write the minimum amount of application code to make the test pass.</li>
<li><strong>Refactor:</strong> Clean up both the application code and the test code.</li>
</ol>
<p>If you write a test and it passes immediately, be suspicious. You may have written a test that can never fail.</p>
<h2 id="where-to-go-from-here">Where to Go From Here</h2>
<h2 id="where-to-go-from-here_1">Where to Go From Here</h2>
<p>Congratulations! You have journeyed from writing your first simple assertion to understanding the advanced patterns and philosophies that underpin professional software testing. You've built a solid foundation in pytest, but the journey of mastery is ongoing.</p>
<p>Here are some paths to continue your learning:</p>
<ol>
<li>
<p><strong>Explore the Plugin Ecosystem:</strong> We've touched on essential plugins like <code>pytest-cov</code>, <code>pytest-xdist</code>, and <code>pytest-asyncio</code>. There are hundreds more. Browse the <a href="https://docs.pytest.org/en/latest/reference/plugin_list.html">official plugin list</a> to find tools that can help with your specific domain, whether it's web development (e.g., <code>pytest-django</code>, <code>pytest-flask</code>), data science, or something else entirely.</p>
</li>
<li>
<p><strong>Read the Official Documentation:</strong> The official pytest documentation is an excellent, comprehensive resource. Now that you understand the core concepts, you'll be able to dive into the finer details of hooks, configuration, and internal mechanics.</p>
</li>
<li>
<p><strong>Write Your Own Plugins:</strong> The ultimate test of understanding is to extend the tool yourself. As we saw in Chapter 19, creating a simple plugin by implementing pytest hooks in <code>conftest.py</code> is surprisingly straightforward. Try writing a plugin that adds a custom command-line option or automatically adds a marker to certain tests.</p>
</li>
<li>
<p><strong>Contribute to Open Source:</strong> Find a project you use and look at their test suite. Can you improve it? Can you add tests for an un-tested part of the code? Reading and contributing to high-quality test suites written by experienced developers is one of the best ways to learn.</p>
</li>
<li>
<p><strong>Teach Others:</strong> The act of explaining a concept to someone else solidifies your own understanding. Mentor a junior developer, write a blog post about a clever testing trick you discovered, or give a presentation at a local meetup.</p>
</li>
</ol>
<p>Testing is not a separate, secondary activity; it is an integral part of the craft of software development. The skills you've learned in this book will make you a more confident, effective, and professional developer. Go forth and build robust, reliable, and well-tested software.</p>
<h2 id="cheat-sheet-common-pytest-commands-and-patterns">Cheat Sheet: Common Pytest Commands and Patterns</h2>
<h2 id="cheat-sheet-common-pytest-commands-and-patterns_1">Cheat Sheet: Common Pytest Commands and Patterns</h2>
<p>A quick reference for the commands, markers, and patterns you'll use most frequently.</p>
<h3 id="command-line-flags">Command-Line Flags</h3>
<table>
<thead>
<tr>
<th>Flag</th>
<th>Shorthand</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--verbose</code></td>
<td><code>-v</code></td>
<td>Increase verbosity; show one test per line with status.</td>
</tr>
<tr>
<td><code>--quiet</code></td>
<td><code>-q</code></td>
<td>Decrease verbosity.</td>
</tr>
<tr>
<td><code>--showlocals</code></td>
<td><code>-l</code></td>
<td>Show local variables in tracebacks.</td>
</tr>
<tr>
<td><code>--exitfirst</code></td>
<td><code>-x</code></td>
<td>Stop the test session after the first failure.</td>
</tr>
<tr>
<td><code>--maxfail=NUM</code></td>
<td></td>
<td>Stop after <code>NUM</code> failures.</td>
</tr>
<tr>
<td><code>--keyword=EXPR</code></td>
<td><code>-k EXPR</code></td>
<td>Run tests that match the given keyword expression.</td>
</tr>
<tr>
<td><code>--mark=EXPR</code></td>
<td><code>-m EXPR</code></td>
<td>Run tests that match the given marker expression.</td>
</tr>
<tr>
<td><code>--collect-only</code></td>
<td></td>
<td>Show which tests would be run, without executing them.</td>
</tr>
<tr>
<td><code>--pdb</code></td>
<td></td>
<td>Drop into the Python debugger on failure.</td>
</tr>
<tr>
<td><code>--last-failed</code></td>
<td><code>--lf</code></td>
<td>Run only the tests that failed in the last run.</td>
</tr>
<tr>
<td><code>--failed-first</code></td>
<td><code>--ff</code></td>
<td>Run last failed tests first, then the rest.</td>
</tr>
<tr>
<td><code>--numprocesses=N</code></td>
<td><code>-n N</code></td>
<td>(pytest-xdist) Run tests in parallel. Use <code>-n auto</code>.</td>
</tr>
<tr>
<td><code>--cov=PATH</code></td>
<td></td>
<td>(pytest-cov) Generate a coverage report for the specified path.</td>
</tr>
<tr>
<td><code>--durations=N</code></td>
<td></td>
<td>Show the <code>N</code> slowest tests.</td>
</tr>
</tbody>
</table>
<h3 id="built-in-markers">Built-in Markers</h3>
<table>
<thead>
<tr>
<th>Marker</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>@pytest.mark.skip(reason=...)</code></td>
<td>Always skip a test.</td>
</tr>
<tr>
<td><code>@pytest.mark.skipif(condition, reason=...)</code></td>
<td>Skip a test if the condition is true.</td>
</tr>
<tr>
<td><code>@pytest.mark.xfail(reason=...)</code></td>
<td>Expect a test to fail. It will be reported as <code>XFAIL</code> or <code>XPASS</code>.</td>
</tr>
<tr>
<td><code>@pytest.mark.parametrize(argnames, argvalues)</code></td>
<td>Perform parametrized testing.</td>
</tr>
<tr>
<td><code>@pytest.mark.usefixtures("fixture_name")</code></td>
<td>Explicitly use a fixture for a test, even if not an argument.</td>
</tr>
<tr>
<td><code>@pytest.mark.filterwarnings("action:message")</code></td>
<td>Add a warning filter for a specific test.</td>
</tr>
</tbody>
</table>
<h3 id="core-assertions-and-helpers">Core Assertions and Helpers</h3>
<table>
<thead>
<tr>
<th>Function</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>assert expression</code></td>
<td>The primary way to make assertions. Pytest provides rich introspection.</td>
</tr>
<tr>
<td><code>pytest.raises(ExpectedException)</code></td>
<td>A context manager to assert that a block of code raises an exception.</td>
</tr>
<tr>
<td><code>pytest.warns(ExpectedWarning)</code></td>
<td>A context manager to assert that a block of code issues a warning.</td>
</tr>
<tr>
<td><code>pytest.approx(expected, rel=..., abs=...)</code></td>
<td>Assert that a floating-point number is approximately equal to another.</td>
</tr>
</tbody>
</table>
<h3 id="fixture-scopes">Fixture Scopes</h3>
<table>
<thead>
<tr>
<th>Scope</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>function</code></td>
<td>(Default) The fixture is created once per test function.</td>
</tr>
<tr>
<td><code>class</code></td>
<td>The fixture is created once per test class.</td>
</tr>
<tr>
<td><code>module</code></td>
<td>The fixture is created once per module.</td>
</tr>
<tr>
<td><code>session</code></td>
<td>The fixture is created once for the entire test session.</td>
</tr>
</tbody>
</table>
<h3 id="key-conftestpy-hooks">Key <code>conftest.py</code> Hooks</h3>
<table>
<thead>
<tr>
<th>Hook</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>pytest_addoption(parser)</code></td>
<td>Add custom command-line options.</td>
</tr>
<tr>
<td><code>pytest_configure(config)</code></td>
<td>Called after command-line options are parsed. Good for global setup.</td>
</tr>
<tr>
<td><code>pytest_sessionstart(session)</code></td>
<td>Called at the beginning of a test session.</td>
</tr>
<tr>
<td><code>pytest_sessionfinish(session)</code></td>
<td>Called at the end of a test session.</td>
</tr>
<tr>
<td><code>pytest_generate_tests(metafunc)</code></td>
<td>Allows for dynamic parametrization of tests.</td>
</tr>
</tbody>
</table>
<h3 id="essential-plugins">Essential Plugins</h3>
<table>
<thead>
<tr>
<th>Plugin</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>pytest-cov</code></td>
<td>Code coverage reporting.</td>
</tr>
<tr>
<td><code>pytest-xdist</code></td>
<td>Parallel test execution and other distribution features.</td>
</tr>
<tr>
<td><code>pytest-watch</code></td>
<td>Automatically re-run tests when files are modified.</td>
</tr>
<tr>
<td><code>pytest-asyncio</code></td>
<td>Support for testing <code>asyncio</code> code.</td>
</tr>
<tr>
<td><code>pytest-benchmark</code></td>
<td>Performance testing and benchmarking.</td>
</tr>
<tr>
<td><code>hypothesis</code></td>
<td>Advanced property-based testing.</td>
</tr>
</tbody>
</table>
        </div>
        <div class="footer">
            Generated on 2025-11-22 17:25:05 | Made with ‚ù§Ô∏è by GitHub Pages Generator
        </div>
    </div>
    <script>
        // Syntax highlighting for code blocks
        document.addEventListener('DOMContentLoaded', (event) => {
            // Highlight code blocks
            document.querySelectorAll('pre code').forEach((block) => {
                hljs.highlightElement(block);
            });
            
            // Add copy buttons to code blocks
            document.querySelectorAll('pre').forEach((pre) => {
                const button = document.createElement('button');
                button.className = 'copy-btn';
                button.textContent = 'Copy';
                
                button.addEventListener('click', () => {
                    const code = pre.querySelector('code').textContent;
                    navigator.clipboard.writeText(code).then(() => {
                        button.textContent = 'Copied!';
                        button.classList.add('copied');
                        setTimeout(() => {
                            button.textContent = 'Copy';
                            button.classList.remove('copied');
                        }, 2000);
                    }).catch(err => {
                        console.error('Failed to copy:', err);
                        button.textContent = 'Error';
                        setTimeout(() => {
                            button.textContent = 'Copy';
                        }, 2000);
                    });
                });
                
                pre.appendChild(button);
            });
        });
    </script>
</body>
</html>